[
  {
    "name": "user-story-proposal-template",
    "text": "# 📝 User Story Proposal: [User Story]\n\n---\n\n## 1. 👤 User Story\n_Provide a clear and concise description of the user story, its purpose, and the value it brings._\n\n---\n\n## 2. 🔍 High-Level Solution Approach\n_Outline the proposed technical approach to implement this user story. Mention key technologies, components, or strategies involved._\n\n---\n\n## 3. ⏱️ Effort Breakdown & Estimates\n\n*   **Design:** [X] hours\n    *   _Reasoning: [Explain why this amount of design effort is needed, or 0 if none]_\n*   **Refinement:** [X] hours\n    *   _Reasoning: [Explain the effort needed for planning, detailing requirements, and refining the approach for this user story]_\n*   **Front-end:** [X] hours\n    *   _Reasoning: [Explain the front-end development tasks involved (UI implementation, state management, etc.)]_\n*   **Backend:** [X] hours\n    *   _Reasoning: [Explain the backend development tasks involved (API endpoints, database changes, logic, etc.)]_\n*   **General Work:** [X] hours\n    *   _Reasoning: [Explain any other tasks not covered above (e.g., documentation, specific integrations)]_\n\n---\n\n## 4. 🧪 QA, Testing & Delay Margin\n\n*   **QA:** [X] hours ([Y]%)\n    *   _Reasoning: [Based on complexity, explain the QA effort needed (manual testing, exploratory testing)]_\n*   **Testing:** [X] hours ([Y]%)\n    *   _Reasoning: [Based on complexity, explain the testing effort needed (unit tests, integration tests, e2e tests)]_\n*   **Delay Margin:** [X] hours ([Y]%)\n    *   _Reasoning: [Based on complexity and potential risks/unknowns, explain the buffer needed]_\n\n---\n\n## 5. 📊 Total Estimated Effort\n*   **Total Effort:** [Sum of all estimates above] hours\n\n---\n\n## 6. 📦 Deliverables / Artifacts\n_List the tangible outputs or artifacts that will be produced upon completion of this user story._\n*   [Deliverable 1: e.g., New UI component in Storybook]\n*   [Deliverable 2: e.g., API endpoint documentation]\n*   [Deliverable 3: e.g., Updated user guide section]\n*   [Deliverable 4: e.g., Set of passing automated tests]\n\n---\n\n## 7. ✅ Acceptance Criteria\n_Define the specific, measurable criteria that must be met for this user story to be considered complete and accepted._\n*   [Criterion 1: e.g., User can successfully perform X action via the new UI]\n*   [Criterion 2: e.g., Backend endpoint Y returns the expected data structure]\n*   [Criterion 3: e.g., All related unit tests pass]\n*   [Criterion 4: e.g., Feature meets the design specifications]\n\n---\n\n## 8. 📝 Assumptions & Notes\n*   _List any assumptions made during the estimation process (e.g., availability of specific APIs, existing infrastructure)._\n*   _Include any important notes, exclusions, dependencies, or potential risks related to this specific user story._\n",
    "keyword": "user-story-proposal-template"
  },
  {
    "name": "research-results-template",
    "text": "   ## Summary\n   Provide a brief overview of your key findings and main points.\n   \n   ## Detailed Analysis\n   Present your in-depth analysis, organized by relevant subtopics or themes.\n   \n   ## Conclusions\n   Summarize the main conclusions drawn from your research.\n   \n   ## Limitations\n   If applicable, discuss any limitations or gaps in the available information.\n   \n   ## Recommendations\n   If appropriate, provide recommendations for further research or action based on your findings.",
    "keyword": "research-results-template"
  },
  {
    "name": "repurposing-content-system",
    "text": "# ​​🔖 Description\nA structured approach to efficiently transform existing content into multiple formats across different platforms, maximizing reach while minimizing additional creation effort.\n\n# ​🎯 Goals\nMaximize content value by transforming single pieces into multiple formats, extend audience reach across platforms, save time on content creation, and maintain consistent messaging throughout all repurposed materials.\n\n# 🧪 Example\nOriginal blog post on \"AI Tools for Developers\" repurposed into: LinkedIn carousel highlighting each tool, Twitter tweets covering each tool, YouTube short video going over the article, TikTok same YouTube video.\n\n# 🪜 Steps\n> 💡 *The heart of every system. All activities and results in chronological order.*\n\n## 📋 Pre-requisites\n\n- [ ] [[initial-article]]\n\n## 👣 Activity Flow\n\n- [ ] Repost [[initial-article]] to [[all-long-text-channels]]\n- [ ] Create variations \n- [ ] Create initial type of content.\n- [ ] Determine other types of content.\n- [ ] Create todo items in preferred project management tool with reference to initial content.\n- [ ] Create other types of content.\n- [ ] Schedule all content in a staggered release pattern across different platforms.\n",
    "keyword": "repurposing-content-system"
  },
  {
    "name": "publish-markdown-cursor-article-system",
    "text": "1.  Post to Medium using https://medium.com/new-story based on relevant channels in @marketeer/resources/dev-channels/all-dev-long-text-channels.md.\n2.  Post to Dev.to using https://dev.to/new based on relevant channels in @marketeer/resources/dev-channels/all-dev-long-text-channels.md.\n3.  Post to LinkedIn Articles using https://www.linkedin.com/post/new/article based on relevant channels in @marketeer/resources/dev-channels/all-dev-long-text-channels.md.\n4.  Post to HackerNoon using https://app.hackernoon.com/new based on relevant channels in @marketeer/resources/dev-channels/all-dev-long-text-channels.md.\n5.  Post to Cursor Forum using https://forum.cursor.com/new-topic based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n6.  Post to Hacker News using https://news.ycombinator.com/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n7.  Post to r/opensource using https://www.reddit.com/r/opensource/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n8.  Post to r/programming using https://www.reddit.com/r/programming/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n9.  Post to r/softwaredevelopment using https://www.reddit.com/r/softwaredevelopment/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n10. Post to r/learnprogramming using https://www.reddit.com/r/learnprogramming/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n11. Post to r/coding using https://www.reddit.com/r/coding/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n12. Post to r/cursor using https://www.reddit.com/r/cursor/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n13. Post to r/ArtificialIntelligence using https://www.reddit.com/r/ArtificialIntelligence/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n14. Post to r/vscode using https://www.reddit.com/r/vscode/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.\n15. Post to r/IDE using https://www.reddit.com/r/IDE/submit based on relevant channels in @marketeer/resources/dev-channels/all-dev-community-channels.md.",
    "keyword": "publish-markdown-cursor-article-system"
  },
  {
    "name": "posting-articles-system",
    "text": "# 🔖 Description\n> 💡 *A brief description and (optional) instructions on how to get started.*\n\nA systematic approach for creating, publishing, and repurposing article content across different platforms.\n\n# 🪜 Steps\n> 💡 *The heart of every system. All activities and results in chronological order.*\n\n- [ ] Choose / create article content.\n    - [ ] Artifact: [[the-article-content]]\n- [ ] Post content on preferred blog site.\n- [ ] Create header image with 'a-creature' prompt.\n- [ ] Create an excerpt with 'plx-create-excerpt' prompt.\n    - [ ] Artifact: [[the-website-excerpt]]\n- [ ] Add excerpt to blog site.\n- [ ] (Optional) Create gist for prompt/code. Use excerpt as description.\n    - [ ] Artifact: [[the-github-gist]]\n- [ ] (Optional) Find link to file in ultra wide tubo workspace.\n- [ ] (Optional) Add gist at start of article with caption 'copy friendly format'.\n- [ ] (Optional) Add workspace link at end of article with caption 'copy friendly format'.\n- [ ] Schedule article.\n- [ ] Create repurpose ticket for different platforms in project management tool.\n",
    "keyword": "posting-articles-system"
  },
  {
    "name": "wow-for-bmad-agile-workflow",
    "text": "# Cursor Agile Workflow Documentation\n\nThis document provides comprehensive documentation for the Agile workflow system integrated with Cursor's AI capabilities. The workflow is designed to maintain project focus and memory and ensure consistent progress through a structured approach to development.\n\n## Overview\n\nThe Agile-Cursor workflow combines traditional Agile methodologies with AI-assisted development to create a powerful, efficient development process. It can be utilized in two primary ways:\n\n1. **Rule-Based Implementation** (Automatic)\n\n   - Uses `.cursor/rules/workflows/workflow-agile-manual` and `.cursor/templates`\n   - Automatically applies standards to matching files\n   - Provides consistent structure enforcement\n\n## Work Item Hierarchy\n\n```mermaid\ngraph TD\n    E[Epic] --> S[Story]\n    S --> T[Task]\n    T --> ST[Subtask]\n\n    style E fill:#f9f,stroke:#333,stroke-width:2px\n    style S fill:#dfd,stroke:#333,stroke-width:2px\n    style T fill:#bbf,stroke:#333,stroke-width:2px\n    style ST fill:#ffd,stroke:#333,stroke-width:2px\n```\n\n1. **Epics**\n\n   - Large, self-contained features\n   - Only one active at a time\n   - Example: \"Online Matchmaking System\"\n\n2. **Stories**\n\n   - Smaller, implementable work units\n   - Must belong to an Epic\n   - Example: \"User Profile Creation\"\n\n3. **Tasks**\n\n   - Technical implementation steps\n   - Clear completion criteria\n   - Example: \"Implement Database Schema\"\n\n4. **Subtasks**\n   - Granular work items\n   - Often includes test requirements\n   - Example: \"Write Unit Tests\"\n\n## AI Project Plan and Memory Structure the Workflow will result in\n\n```\nai/\n├── prd.md                 # Product Requirements Document\n├── arch.md               # Architecture Decision Record\n├── epic-1/              # Current Epic directory\n│   ├── story-1.story.md  # Story files for Epic 1\n│   ├── story-2.story.md\n│   └── story-3.story.md\n├── epic-2/              # Future Epic directory\n│   └── ...\n└── epic-3/              # Future Epic directory\n    └── ...\n```\n\n## Workflow Phases\n\n### 1. Initial Planning\n\n- Focus on documentation and planning\n- Only modify `.ai/`, docs, readme, and rules\n- Required approvals for PRD and then the Architecture\n\n### 2. Development Phase\n\n- Generates the first or next story and waits on approval\n- Implementation of approved in progress story\n- Task-by-task story execution\n- Continuous testing and validation\n\n```mermaid\ngraph LR\n    subgraph PLAN Phase\n        A[Project Idea] --> B[PRD Creation]\n        B --> C[Architecture Design]\n        C --> D[Epic Planning]\n        D --> E[Story Definition]\n    end\n\n    subgraph ACT Phase\n        E --> F[Story Implementation]\n        F --> G[Testing & Validation]\n        G --> H[Story Completion]\n    end\n\n    subgraph Iteration\n        H --> |Next Story|E\n        H --> |Epic Complete|I[Epic Completion]\n        I --> |Next Epic|D\n        I --> |Project Complete|J[Release]\n    end\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style B fill:#dfd,stroke:#333,stroke-width:2px\n    style C fill:#dfd,stroke:#333,stroke-width:2px\n    style D fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#bbf,stroke:#333,stroke-width:2px\n    style F fill:#bbf,stroke:#333,stroke-width:2px\n    style G fill:#bbf,stroke:#333,stroke-width:2px\n    style H fill:#bbf,stroke:#333,stroke-width:2px\n    style I fill:#f9f,stroke:#333,stroke-width:2px\n    style J fill:#f9f,stroke:#333,stroke-width:2px\n```\n\n## Implementation Guidelines\n\n### Story Implementation Process\n\n1. **Initialization**\n\n   - Verify `.ai` directory exists\n   - Locate approved architecture and current story\n   - Ensure story is properly marked as in-progress\n\n2. **Development Flow**\n\n   - Follow Test-Driven Development (TDD)\n   - Update task/subtask status regularly\n   - Document all implementation notes\n   - Record significant commands used\n\n3. **Completion Requirements**\n   - All tests must pass\n   - Documentation must be updated\n   - User must approve completion\n\n### Critical Rules\n\n> 🚨 **Critical Rules:**\n>\n> - Never creates first story without PRD and Architecture approval\n> - Only one Epic can be in-progress at a time\n> - Only one Story can be in-progress at a time\n> - Stories must be implemented in PRD-specified order\n> - Never implement without story approval from user (marked as in progress on the story file)\n\n## Using the Workflow\n\nThe best way post 0.47.x+ of cursor is to use the rules based approach, with either manual, agent selection or always on rules. I prefer manual selection type rule for the workflows, so that they will not be in a context if I do not need it (explanation to follow).\n\nIf I am starting a brand new project (with our without an existing code template) I have a few options:\n\n- Use an external tool to generate the PRD (Such as ChatGPT Canvas or o3 mini Web UI or Google AI Studio)\n- Use the workflow and agent in cursor to generate the PRD\n  (This comes down to personal preference and consideration of token burn within cursor)\n\nIf I am doing this in cursor, I will start a new Agent chat with Claude 3.7 Thinking (or choose a different model if concerned about credit burn) and type something like:\n\n`Lets follow the @workflow-agile-manual to create a PRD for a new project I want to create that will do XYZ, have the following features etc etc. Lets focus on just the MVP feature first will be to deliver X minimally, but lets also plan to have some epics for fast follows or future enhancements such as A B and C.`\n\nAs this can be quite lengthy, I will many times craft this prompt in the xnotes folder, and then paste it into the chat, ensuring that the @workflow is still properly added.\n\nNote: you can also modify the workflow-agile-manual to be Agent auto-selectable, this work reliably well also - you will just need to ensure the description you give it in the front matter will ensure its used when needed (PRD story and work implementation phases) - or potentially just make it an always rule. When starting out, its fine to make it an always rule, until your project grows to a very significant size, then I suggest turning it off manually, as at that point you might be just going in and making very targeted updates to specific files or features - and do not need the whole workflow as overhead - or you might want to instead select a different workflow (maybe a refactor workflow, a test workflow, an external MCP agent, etc...)\n\nThe agent should generate a draft prd.md file in a .ai folder.\n\nI suggest at this point, you do not approve and jump right in - either in cursor with the agent, or an external tool - engage further with the agent to refine the document, have the agent ask you questions on holes in the document that it might want to know the answer to, ask the agent if it needs any clarifications that will allow for a very jr agent developer to understand and implement the stories, ask the agent if the sequencing of the stories make sense etc...\n\nOnce you feel its in a good spot - you can mark the file as status: approved.\n\nAt this point, I would start another chat and with the workflow - the agent will first check for the prd, and then if its approved, will offer to create (if not already existing and approved) the architecture file - and similar a new chat window with the workflow will search for the new first or in progress story.\n\nOnce a story is in progress and approved by the user - the agent can be told to execute the story. Once a story or part of a story is completed and the story file is updated with progress by the agent, commit often (I use my manual gitpush.mdc manual rule macro). After this, I might start a new chat window with a fresh context and the workflow again loaded. Once a story is complete (status: complete) and tested and pushed, I always will start a new chat window with the workflow, and ask the agent to 'Create the next story draft' - or just ask it what it thinks it should do next, it should recognize what is next story to do from the prd and what story was last marked completed, and generate a draft for the next story, and then stop and ask for my approval before doing any further coding.\n\nA more detailed example, up to date repo and video coming soon, but this should give the main ideas...\n\nNOTE: Some models (Sonnet 3.7 thinking) have gotten a bit overly aggressive, so the rules might need to be tuned to further ensure the agent does not start updating code until the story is approved.\n\n## Best Practices\n\n1. **Documentation and tips**\n\n   - AI will keep PRD and Architecture documents updated - sometimes you will need to tell it to update the prd and arch files as needed.\n   - Document all significant decisions\n   - Maintain clear implementation notes\n   - Have the AI create readme.md files in each src subfolder to help give it direction\n\n2. **Testing**\n\n   - Have the AI Write tests before implementation - a fun excercise in TDD\n   - Maintain high test coverage\n   - Verify all tests pass before completion\n\n3. **Progress Tracking**\n\n   - Have the AI (or you) update story status regularly\n   - Record all implementation notes\n   - Document command history\n\n4. **Context Management**\n   - Start fresh composer instance per story or after significant recorded progress (recorded in task completion updates)\n   - Use appropriate context level\n   - Minimize context overhead\n   - Consider making a leaner workflow when you are in story execution mode - that does not need all of the templates and overhead of how to create a prd and a architecture. But you will need to consider what other files or parts of other files it might need reference to to retain the plot. This is why currently I still use the full workflow.\n\n## Status Progression\n\nStories follow a strict status progression:\n\n```\nDraft -> In Progress -> Complete\n```\n\nEpics follow a similar progression:\n\n```\nFuture -> Current -> Complete\n```\n\n## Integration with Cursor AI\n\nThe workflow is designed to work seamlessly with Cursor's AI capabilities:\n\n1. **AI-Assisted Planning**\n\n   - AI helps create and refine PRD\n   - AI suggests architecture improvements\n   - AI assists in story breakdown\n\n2. **AI-Assisted Implementation**\n\n   - AI implements story tasks\n   - AI maintains test coverage\n   - AI updates documentation\n\n3. **AI-Assisted Review**\n   - AI verifies completion criteria\n   - AI suggests improvements\n   - AI maintains consistency\n\n## Cost Savings\n\n- LLMs outside of Cursor, if you have them available, such as ChatGPT, Claude, Gemini, etc. are also great to generate the initial PRD and architecture, and really iterate on them.\n- Within Cursor, currently you can use DeepSeek R1 for example which seems to be free and also decent and udpating PRD and architecture - but I have found it to be a bit less reliable than using Claude to follow the format I want - but much cheaper, if trying to do it all in cursor planning.\n",
    "keyword": "wow-for-bmad-agile-workflow"
  },
  {
    "name": "how-to-create-milestones-and-user-stories",
    "text": "# 🎯 Creating Milestones and User Stories Using Expert Prompts\n\nThis tutorial demonstrates how to use the `Milestone Proposal Expert` and `User Story Expert` AI prompts along with their respective templates to generate structured project milestones and detailed user story proposals. Following these steps will help ensure consistency and completeness in your project planning artifacts.\n\n## 🎯 Part 1: Creating a Project Milestone\n\nThis section explains how to use the `Milestone Proposal Expert` prompt and the `milestone-template.md` to define a project milestone.\n\n### 📥 Step 1: Understand the Inputs\n\nThe `Milestone Proposal Expert` requires specific inputs to function correctly:\n\n1.  **User Instructions:** Your specific request detailing the goal and scope of the milestone.\n2.  **File Map (Optional but helpful):** An overview of project files for context.\n3.  **File Contents:** The actual content of relevant files, most importantly the `milestone-template.md` and the `you-are-a-milestone-proposal-expert.md` prompt itself.\n\n### 🛠️ Step 2: Prepare the Prompt and Template\n\nEnsure you provide the AI with the full content of the `Milestone Proposal Expert` prompt and the `milestone-template.md`.\n\n**Milestone Proposal Expert Prompt (`you-are-a-milestone-proposal-expert.md`):**```markdown\n# Role: Milestone Proposal Expert\n\n## Goal:\nYour primary goal is to create well-defined project milestones based on user requirements and a provided template (`milestone-template.md`). You will analyze the user's request, leverage the structure of the template, and generate a complete milestone definition ready for use in project planning.\n\n## Input Context:\nYou will receive the following information to perform your task:\n1.  **User Instructions:** Specific requirements or context for the milestone, enclosed in `<user_instructions>` tags.\n2.  **File Map:** An overview of relevant project files, enclosed in `<file_map>` tags (primarily for context, the template content is key).\n3.  **File Contents:** The content of relevant files, crucially including the `milestone-template.md`, enclosed in `<file_contents>` tags.\n\n## Process:\n1.  **Understand the Request:** Carefully read the `<user_instructions>` to grasp the purpose and scope of the requested milestone.\n2.  **Identify the Template:** Locate the `milestone-template.md` within the `<file_contents>`. Familiarize yourself with its sections (Goal, Tasks, Related User Stories, Deliverables, Acceptance Criteria, etc.).\n3.  **Structure the Milestone:** Use the exact structure and headings provided in the `milestone-template.md` for your output. Do not deviate from this structure.\n4.  **Populate Sections:** Fill in each section of the template based on the user's request and the nature of the milestone:\n    *   **Milestone Title:** Create a concise and descriptive title based on the user instructions.\n    *   **Goal / Objective:** Clearly articulate the primary goal this milestone aims to achieve, derived from the user instructions.\n    *   **Key Tasks / Activities:** List the specific, actionable tasks required to reach the milestone goal. Infer these from the user instructions or state that they need further definition if not provided. Use the checklist format `[ ] Task Description`.\n    *   **Related User Stories:** List the user stories that fall under this milestone's scope. These stories are intended to be refined later using the `user-story-proposal-template.md` (often by a `User Story Expert`). Use the format `[ ] User Story Title: [Brief Description or Link]`. Infer potential stories from the goal and tasks if not explicitly provided, noting that they need refinement.\n    *   **Deliverables / Artifacts:** List the tangible outputs expected upon completion (e.g., code, documents, features). Infer these based on the tasks and goal.\n    *   **Acceptance Criteria:** Define specific, measurable criteria that confirm the milestone goal has been met. These should directly relate to the goal and deliverables.\n    *   **Dependencies / Related Links (Optional):** Include if mentioned or clearly implied in the user instructions.\n    *   **Notes (Optional):** Add any relevant context, assumptions, or risks mentioned or inferred.\n5.  **Clarity and Completeness:** Ensure the goal is clear, tasks are actionable, deliverables are tangible, and acceptance criteria are specific. If information is missing in the user request to fill a mandatory section (like Goal or Tasks), state what is needed or make reasonable assumptions and note them. Ensure listed User Stories logically align with the milestone's scope.\n\n## Output Format:\nProduce the complete Milestone definition in Markdown format, strictly adhering to the structure and headings found in the `milestone-template.md`. Do not include any introductory or concluding remarks outside of the template structure. Start directly with the `# Milestone: [Generated Title]` heading.\n```\n\n**Milestone Template (`milestone-template.md`):**\n\n```markdown\n# Milestone: [Enter Milestone Title Here]\n\n---\n\n## 🎯 Goal / Objective\n*   _Clearly state the primary goal or objective this milestone aims to achieve. What does success look like for this specific milestone?_\n\n---\n\n## 🛠️ Key Tasks / Activities\n*   [ ] Task 1: _Describe the first key task required._\n*   [ ] Task 2: _Describe the second key task required._\n*   [ ] Task 3: _Add more tasks as needed._\n*   ...\n\n---\n\n## 👤 Related User Stories\n*   _List the specific User Stories associated with this milestone. These stories will be further detailed using the User Story Proposal template._\n    *   [ ] User Story 1 Title: [Link to proposal if available, or brief description]\n    *   [ ] User Story 2 Title: [Link to proposal if available, or brief description]\n    *   ...\n\n---\n\n## 📦 Deliverables / Artifacts\n*   _List the tangible outputs or artifacts that will be produced upon completion of this milestone._\n    *   Example: Updated `README.md`\n    *   Example: Functional prototype link\n    *   Example: Test results document\n\n---\n\n## ✅ Acceptance Criteria\n*   _Define the specific, measurable criteria that must be met for this milestone to be considered complete._\n    *   Criterion 1: _e.g., Code merged to main branch._\n    *   Criterion 2: _e.g., Feature X is demonstrable in the staging environment._\n    *   Criterion 3: _e.g., All tasks listed above are marked as complete._\n\n---\n\n## 🔗 Dependencies / Related Links (Optional)\n*   _List any dependencies (e.g., previous milestones, external factors) or relevant links (e.g., design docs, tickets)._\n    *   Dependency: [Link or Description]\n    *   Related: [Link or Description]\n\n---\n\n## 📝 Notes (Optional)\n*   _Add any additional context, notes, or risks relevant to this milestone._\n\n---\n```\n\n### ✍️ Step 3: Provide User Instructions\n\nCraft your specific request for the milestone. Enclose it in `<user_instructions>` tags.\n\n**Example User Instructions:**\n\n```xml\n<user_instructions>\nCreate a milestone for implementing the basic user authentication feature for our new mobile app. This should include user registration, login, and password reset functionality. The goal is to have a working authentication flow ready for internal testing.\n</user_instructions>\n```\n\n### 🚀 Step 4: Generate the Milestone\n\nProvide the prompt, template, and your user instructions to the AI. The AI, acting as the `Milestone Proposal Expert`, will generate the milestone definition based on the template.\n\n**Example Expected Output:**\n\n```markdown\n# Milestone: Implement Basic User Authentication\n\n---\n\n## 🎯 Goal / Objective\n*   To implement and internally test the core user authentication features (registration, login, password reset) for the mobile app, enabling secure user access.\n\n---\n\n## 🛠️ Key Tasks / Activities\n*   [ ] Design authentication UI screens (Registration, Login, Password Reset).\n*   [ ] Set up backend infrastructure for user accounts (database schema, API endpoints).\n*   [ ] Implement user registration logic (front-end and backend).\n*   [ ] Implement user login logic (front-end and backend, including session management).\n*   [ ] Implement password reset flow (request token, email sending, reset form).\n*   [ ] Write basic unit/integration tests for authentication endpoints.\n*   [ ] Conduct internal testing of the complete authentication flow.\n\n---\n\n## 👤 Related User Stories\n*   _These stories need further refinement using the User Story Proposal template._\n    *   [ ] User Registration: As a new user, I want to create an account so I can access the app's features.\n    *   [ ] User Login: As a registered user, I want to log in to my account securely.\n    *   [ ] Password Reset: As a user who forgot my password, I want to reset it so I can regain access to my account.\n\n---\n\n## 📦 Deliverables / Artifacts\n*   Functional authentication UI components (Registration, Login, Password Reset screens).\n*   Backend API endpoints for registration, login, and password reset.\n*   User account data storage mechanism.\n*   Automated tests for authentication logic.\n*   Internal test report confirming flow functionality.\n\n---\n\n## ✅ Acceptance Criteria\n*   New users can successfully register an account via the app UI.\n*   Registered users can successfully log in using their credentials.\n*   Users can successfully initiate and complete the password reset process via email.\n*   Authentication API endpoints function as expected and handle common error cases.\n*   The authentication flow is demonstrable in the internal testing environment.\n*   All key tasks listed above are marked as complete.\n\n---\n\n## 🔗 Dependencies / Related Links (Optional)\n*   Dependency: Base project setup and navigation structure.\n*   Related: UI/UX Design mockups for authentication screens (link if available).\n\n---\n\n## 📝 Notes (Optional)\n*   This milestone focuses on core functionality. Advanced features like social login or two-factor authentication are out of scope for this milestone.\n*   Assumes email service integration is available for password reset.\n\n---\n```\n\n## 👤 Part 2: Creating a User Story Proposal\n\nThis section explains how to use the `User Story Expert` prompt and the `user-story-proposal-template.md` to detail a specific user story, often one identified within a milestone.\n\n### Step 1: Understand the Inputs\n\nThe `User Story Expert` requires:\n\n1.  **User Instructions:** Your specific request detailing the user story to be elaborated.\n2.  **File Map (Optional):** Project file context.\n3.  **File Contents:** The content of the `user-story-proposal-template.md` and the `you-are-a-user-story-expert.md` prompt.\n\n### Step 2: Prepare the Prompt and Template\n\nProvide the AI with the full content of the `User Story Expert` prompt and the `user-story-proposal-template.md`.\n\n**User Story Expert Prompt (`you-are-a-user-story-expert.md`):**\n\n```markdown\n# Role: User Story Expert\n\n## Goal:\nYour primary goal is to create comprehensive and well-structured User Story Proposals based on user requirements and a provided template. You will analyze the user's request, leverage the structure of the `user-story-proposal-template.md`, and generate a detailed proposal ready for review and refinement.\n\n## Input Context:\nYou will receive the following information to perform your task:\n1.  **User Instructions:** Specific requirements for the user story, enclosed in `<user_instructions>` tags.\n2.  **File Map:** An overview of the relevant project files, enclosed in `<file_map>` tags.\n3.  **File Contents:** The content of relevant files, particularly the template (`user-story-proposal-template.md`), enclosed in `<file_contents>` tags.\n\n## Process:\n1.  **Understand the Request:** Carefully read the `<user_instructions>` to grasp the core user story, its objectives, and any specific details provided by the user.\n2.  **Identify the Template:** Locate the `user-story-proposal-template.md` within the `<file_contents>`. Familiarize yourself with its sections.\n3.  **Structure the Proposal:** Use the exact structure and headings provided in the `user-story-proposal-template.md` for your output.\n4.  **Populate Sections:** Fill in each section of the template based on the user's request:\n    *   **User Story:** Clearly articulate the user story based on the instructions.\n    *   **High-Level Solution Approach:** Briefly outline a potential technical approach. If not specified, make reasonable assumptions or state that it needs further definition.\n    *   **Effort Breakdown & Estimates:** Provide realistic hour estimates for Design, Refinement, Front-end, Backend, and General Work. **Crucially, include clear reasoning** for each estimate, referencing the complexity or tasks involved. If a category is not applicable, mark it as 0 hours and explain why.\n    *   **QA, Testing & Delay Margin:** Estimate hours and percentages for QA, Testing (unit, integration, e2e), and a Delay Margin. Provide reasoning based on the story's complexity and potential risks.\n    *   **Total Estimated Effort:** Calculate the sum of all estimates.\n    *   **Deliverables / Artifacts:** List the tangible outputs expected (e.g., code components, documentation, test results).\n    *   **Acceptance Criteria:** Define specific, measurable, achievable, relevant, and time-bound (SMART) criteria for accepting the story.\n    *   **Assumptions & Notes:** List any assumptions made during estimation and any relevant notes, dependencies, or risks.\n5.  **Clarity and Detail:** Ensure your reasoning for estimates is clear and justifies the numbers. Define deliverables and acceptance criteria precisely.\n6.  **Handle Missing Information:** If critical information is missing from the user instructions, state what is needed or make a reasonable assumption and clearly note it in the \"Assumptions & Notes\" section.\n\n## Output Format:\nProduce the complete User Story Proposal in Markdown format, strictly adhering to the structure and headings found in the `user-story-proposal-template.md`. Do not include any introductory or concluding remarks outside of the template structure.\n```\n\n**User Story Proposal Template (`user-story-proposal-template.md`):**\n\n```markdown\n# 📝 User Story Proposal: [User Story]\n\n---\n\n## 1. 👤 User Story\n_Provide a clear and concise description of the user story, its purpose, and the value it brings._\n\n---\n\n## 2. 🔍 High-Level Solution Approach\n_Outline the proposed technical approach to implement this user story. Mention key technologies, components, or strategies involved._\n\n---\n\n## 3. ⏱️ Effort Breakdown & Estimates\n\n*   **Design:** [X] hours\n    *   _Reasoning: [Explain why this amount of design effort is needed, or 0 if none]_\n*   **Refinement:** [X] hours\n    *   _Reasoning: [Explain the effort needed for planning, detailing requirements, and refining the approach for this user story]_\n*   **Front-end:** [X] hours\n    *   _Reasoning: [Explain the front-end development tasks involved (UI implementation, state management, etc.)]_\n*   **Backend:** [X] hours\n    *   _Reasoning: [Explain the backend development tasks involved (API endpoints, database changes, logic, etc.)]_\n*   **General Work:** [X] hours\n    *   _Reasoning: [Explain any other tasks not covered above (e.g., documentation, specific integrations)]_\n\n---\n\n## 4. 🧪 QA, Testing & Delay Margin\n\n*   **QA:** [X] hours ([Y]%)\n    *   _Reasoning: [Based on complexity, explain the QA effort needed (manual testing, exploratory testing)]_\n*   **Testing:** [X] hours ([Y]%)\n    *   _Reasoning: [Based on complexity, explain the testing effort needed (unit tests, integration tests, e2e tests)]_\n*   **Delay Margin:** [X] hours ([Y]%)\n    *   _Reasoning: [Based on complexity and potential risks/unknowns, explain the buffer needed]_\n\n---\n\n## 5. 📊 Total Estimated Effort\n*   **Total Effort:** [Sum of all estimates above] hours\n\n---\n\n## 6. 📦 Deliverables / Artifacts\n_List the tangible outputs or artifacts that will be produced upon completion of this user story._\n*   [Deliverable 1: e.g., New UI component in Storybook]\n*   [Deliverable 2: e.g., API endpoint documentation]\n*   [Deliverable 3: e.g., Updated user guide section]\n*   [Deliverable 4: e.g., Set of passing automated tests]\n\n---\n\n## 7. ✅ Acceptance Criteria\n_Define the specific, measurable criteria that must be met for this user story to be considered complete and accepted._\n*   [Criterion 1: e.g., User can successfully perform X action via the new UI]\n*   [Criterion 2: e.g., Backend endpoint Y returns the expected data structure]\n*   [Criterion 3: e.g., All related unit tests pass]\n*   [Criterion 4: e.g., Feature meets the design specifications]\n\n---\n\n## 8. 📝 Assumptions & Notes\n*   _List any assumptions made during the estimation process (e.g., availability of specific APIs, existing infrastructure)._\n*   _Include any important notes, exclusions, dependencies, or potential risks related to this specific user story._\n```\n\n### Step 3: Provide User Instructions\n\nCraft the request for the specific user story proposal.\n\n**Example User Instructions:**\n\n```xml\n<user_instructions>\nCreate a user story proposal for the \"User Login\" story identified in the Basic Authentication milestone. The story is: \"As a registered user, I want to log in to my account securely.\" Assume standard email/password login. We need estimates for design (basic screen layout), refinement, front-end (React Native), and backend (Node.js/Express API, PostgreSQL DB). Consider it medium complexity (3/5).\n</user_instructions>\n```\n\n### 🚀 Step 4: Generate the User Story Proposal\n\nProvide the prompt, template, and user instructions to the AI. The `User Story Expert` will generate the detailed proposal.\n\n**Example Expected Output:**\n\n```markdown\n# 📝 User Story Proposal: User Login\n\n---\n\n## 1. 👤 User Story\nAs a registered user, I want to log in to my account securely using my email and password so that I can access my personalized content and features within the app.\n\n---\n\n## 2. 🔍 High-Level Solution Approach\n*   **Front-end (React Native):** Create a Login screen with input fields for email and password, a submit button, and error message display. Implement state management for input values and loading/error states. On successful login, store the authentication token (e.g., JWT) securely and navigate the user to the main app screen.\n*   **Backend (Node.js/Express):** Create a `/login` API endpoint that accepts email and password. Validate inputs. Query the PostgreSQL database to find the user by email. Verify the provided password against the stored hash. If valid, generate a JWT containing user identification and return it to the client. Implement appropriate error handling for invalid credentials or server issues.\n\n---\n\n## 3. ⏱️ Effort Breakdown & Estimates\n\n*   **Design:** 2 hours\n    *   _Reasoning: Basic wireframing and layout definition for the login screen, including input fields, button placement, and error message styling._\n*   **Refinement:** 2 hours\n    *   _Reasoning: Finalizing requirements, defining API contract details between front-end and backend, clarifying error handling specifics._\n*   **Front-end:** 8 hours\n    *   _Reasoning: Implementing the React Native Login screen component, handling user input, integrating with state management, making the API call, handling responses/errors, secure token storage, and navigation logic._\n*   **Backend:** 6 hours\n    *   _Reasoning: Creating the Express route and controller for `/login`, writing database query logic (PostgreSQL), implementing password hashing verification (e.g., using bcrypt), generating JWT, and adding input validation and error handling._\n*   **General Work:** 0 hours\n    *   _Reasoning: No additional general tasks identified for this specific story beyond standard development practices covered in other categories._\n\n---\n\n## 4. 🧪 QA, Testing & Delay Margin\n\n*   **Subtotal (Design + Refine + FE + BE):** 2 + 2 + 8 + 6 = 18 hours\n*   **QA:** 4.5 hours (25%)\n    *   _Reasoning: Manual testing of the login flow on different devices/scenarios (valid credentials, invalid email, invalid password, empty fields), checking error messages, verifying successful navigation._\n*   **Testing:** 2.7 hours (15%)\n    *   _Reasoning: Writing unit tests for the backend login endpoint logic (validation, password check, token generation) and potentially basic front-end component tests for the login form._\n*   **Complexity Factor:** 3/5 (Medium)\n*   **Delay Margin:** 5.0 hours (20% of Subtotal + QA + Testing = 20% of 25.2 hours)\n    *   _Reasoning: Medium complexity involves integration between front-end, backend, and database, potential issues with token handling or environment setup warrant a 20% buffer._\n\n---\n\n## 5. 📊 Total Estimated Effort\n*   **Total Effort:** 18 (Dev) + 4.5 (QA) + 2.7 (Testing) + 5.0 (Delay) = **30.2 hours** (Rounded estimate: ~30 hours)\n\n---\n\n## 6. 📦 Deliverables / Artifacts\n*   Login screen UI component implemented in React Native.\n*   `/login` API endpoint implemented in Node.js/Express.\n*   Unit tests for backend login logic.\n*   Documentation for the `/login` API endpoint.\n*   QA test results confirming successful login functionality and error handling.\n\n---\n\n## 7. ✅ Acceptance Criteria\n*   Given a registered user provides their correct email and password on the Login screen, when they tap \"Login\", they are successfully authenticated, redirected to the main app screen, and an auth token is stored securely.\n*   Given a user provides an incorrect email or password, when they tap \"Login\", an appropriate error message is displayed on the Login screen, and they remain on the Login screen.\n*   Given a user leaves the email or password field blank, when they tap \"Login\", a validation error message is displayed.\n*   The backend `/login` endpoint correctly validates input data format.\n*   The backend `/login` endpoint returns a valid JWT upon successful authentication.\n*   The backend `/login` endpoint returns appropriate error codes/messages for failed login attempts (e.g., 401 Unauthorized).\n*   Unit tests for the backend login logic pass.\n\n---\n\n## 8. 📝 Assumptions & Notes\n*   Assumes the user registration feature (database schema, password hashing) is already implemented or will be available.\n*   Assumes a standard JWT library is available for token generation/verification.\n*   Assumes secure storage mechanism (e.g., AsyncStorage, Keychain) is set up on the front-end.\n*   Excludes \"Remember Me\" functionality for this story.\n*   Depends on the base API setup and database connection being functional.\n\n```\n\n## ✅ Conclusion\n\nBy using the `Milestone Proposal Expert` and `User Story Expert` prompts with their corresponding templates, you can systematically break down project work into manageable milestones and detailed user stories. This structured approach facilitates clearer planning, estimation, and execution. Remember to provide the full prompt and template content along with clear user instructions for the best results.\n",
    "keyword": "how-to-create-milestones-and-user-stories"
  },
  {
    "name": "wow-for-the-bmad-code-method",
    "text": "# The BMad Code Method\n\nThis method outlines how to create and pairing with Custom Agile Persona Agents to follow the **Breakthrough Method Agile-Ai Driven-Development (B.M.A.D. Method)**\n\n## Quick note about previous repo that this replaces\n\nThis method is a full replacement and enhancement to what was hinted at and partially described in the custom-agents-rules-generator [this repo](https://github.com/bmadcode/cursor-custom-agents-rules-generator). This is now more tailored to being generic and working with any IDE (not just cursor specific) and the custom rule used to generate rules is no longer needed in Cursor anyways as of 0.49x (And the other IDE's now support auto rule generation also) and with custom agents and agile artifacts, rules become less necessary. Rules that apply to general standards can be build into your developer agents. For example, you can expand the dev persona agent herein to be a typescript dev agent, or a python dev agent, or even a ui dev agent - all with the best practices you want it to follow baked in! By having multiple dev types, you can have specialized devs with the rules in their context primed for what they will be working on - instead of overall bloated rules that do not apply to every task at hand. This is all optional, but you can start to see why this replaces the detailed rules based workflows.\n\nWhere IDE rules will still apply, is for fine tuning quick one off rules as you are going if you find the agent making many mistakes in certain ways. In the future you can craft this adherance into your agile artifacts and stories, or the custom mode configurations!\n\nJoin in on the [Community Discussion Forum](https://github.com/bmadcode/BMAD-METHOD/discussions), help contribute, evolve, and advance the ideas laid out here. This is IDE Agnostic, works great with Cursor, Cline, RooCode, CoPilot etc...! If it has an intelligent agent, this will help you tame it and keep the good vibes flowing!\n\nAlso check out [Part 1 and 2 on the BMad Code YouTube channel](https://youtu.be/JbhiLUY_V2U) - feel free to comment, like, and subscribe also for future videos and updates.\n\n## Overview\n\nThe BMad Method is a (not so) revolutionary approach to software development that leverages AI-driven processes to accelerate and enhance the entire product development lifecycle from ideation and market fit, through agentic code implementation.\n\nThe method is meant to be tool agnostic including a workflow built into the role-prompts. It is a somewhat manual workflow that can be used in multiple ways.\n\nIt can easily be adapted to specifics of any agentic coding toolset, ide and platform.\n\n## What is the BMad Method?\n\nThe BMad Method is a comprehensive, step-by-step approach that transforms a product idea into a fully implemented application agile prompt chain by:\n\n1. Structuring the development process into distinct AI persona-based phases\n2. Generating detailed artifacts at each phase\n3. Using a sequential workflow to track progress\n4. Enabling AI to code the full application based on generated specifications that are granular and detailed, setting you up for maximal success.\n\n## Getting Started\n\n### Prerequisites\n\n- An AI assistant capable of using these prompts (Claude, GPT-4, Gemini, etc.)\n- Optional burt HIGHLY Recommended: Access to Deep Research AI\n- Basic understanding of Cursor / Cline / Roo / CoPilot Agent\n- A software product or project idea you want to build with AI\n\n### How to Use with your UI or IDE of choice\n\n#### Gemini (Google)\n\n- Configure a Custom Gem for each mode you want to use. For example, I recommend before even going into your IDE set up the ba, pm and ux Gems at a minimum, also potentially the architect. Especially if you intend to use deep research (which you might as well with it be so great) - you will want to make use of the custom modes in Gemini.\n\n#### Cursor\n\n- Ensure you have Custom Modes (Beta) turned on in your cursor options\n- Create the Custom Modes for each of your intended agents, going into the advanced options to give them custom prompts (copied and modified as needed from the ./custom-mode-prompts folder)\n\n#### RooCode\n\n- Follow this [guide](https://publish.obsidian.md/aixplore/AI+Systems+%26+Architecture/custom-modes-quick-start) along with the prompts (copied and modified as needed from the ./custom-mode-prompts folder)\n\n#### Other IDEs\n\nOther IDEs do not yet seem to have the exact same way of creating custom modes - but you can still use this methodology through rules, plan/act modes, and using the mode prompts as a prompt to start a new chat session.\n\n## Project Setup\n\nIf you are going to use the full workflow including the dev working on one story at a time and making story drafts - you will want to add to your project folder:\n/ai/\n/ai/stories/\n/ai/templates/story-template.md\n\n- The other templates are embedded in the custom mode prompts so are not necessary to copy over.\n\n### Workflow\n\nThe BMad Method follows a structured workflow:\n\n1. **BA:** If your idea is vague or very ambitious and you are not even sure what would or should be in an MVP - start with the BA. Use this as your brainstorming buddy, check the market conditions and competitor analysis, and let it help you elicit features or ideas you may have never considered. It can also help you craft a great prompt to trigger deep research mode to really get advice and analysis of your fleshed out idea. The output will be a **Project Brief** which you will feed to the PM.\n2. **PM:** Either give the PM the Project Brief, or describe manually your project if you understand it well enough. The PM will ask you clarifying questions until it feels comfortable drafting the PRD with enough detail to enable eventual agent development. This will include a high level story breakdown and sequence. The output will be a **PRD**. You can give some platform and technical ideas to the PM if you already know them - or wait to work with the architect. If you are already sure of the platform languages and libraries you are sure you want to use, best to specify them now, or even prior to this in the project brief.\n3. **UX Expert:** This is a special purpose agent that is good at one thing, taking the idea from the PRD and helping elicit and flesh out a prompt tuned to get great results from V0 or similar UI generators. But you can also use the UX Expert to just help flesh out more details for the PRD before we hit the architect.\n4. **Architect:** If your project is technically complex, or you did not know all of the technical details with the PM, pull in the architect to produce an architecture document, and also ensure that it and the PRD are both in alignment. You can also push the Architect into Deep Research mode - use it to research potential alternative technologies, find if others have done similar things already (don't always need to reinvent the wheel), and maybe even suggest a whole new approach. If you do deep research, its best to take the time to understand it and ensure anything you want to use is incorporated back into the architecture draft and PRD. IF its so drastically different, you may want to go all the way back to the project brief. This is where upfront planning really plays off before we start burning up LLM agent credits!\n5. **PO:** At this point, the PO may be unnecessary - but if you have produced a PRD, Architecture, and potentially some UX content - the PO is a good reviewer to ensure that our stories are high level but properly sequenced in the PRD - or can make updates as needed.\n6. **SM:** **(Not recommended for use at this time)** - the Technical Scrum Master can take all of the polished high level stories the PO just cleaned up and produce at once all of the stories in full detail in one large document. This is practice is not recommended, instead skip this and I suggest using the Dev Agent to draft their own story that they will work on. In the future - the SM will be an agent that can create and manage story workflows with integrations such as Jira or Trello or a local folder structure.\n7. **DEV:** Finally we are ready for development! The Dev agent is set to work on 1 story at a time, and will create a story in draft mode for your review before starting to work on it. The story will follow the template in the ai folder and create it at /ai/stories/ following a naming convention of story-{epic}.{story}.md.\n    1. Once you approve of the story (change status to `In Progress`), the dev will work on it and update its progress. It will use the PRD and Architecture documents as reference to draft ths stories and ensure the level of detail is in the story.\n    2. It is recommended to start a new chat with each story - and potentially even after transitioning a story to In-Progress (from draft) so its starts with a clean context overhead ready to execute. But see what works best for your workflow.\n    3. I always recommend having tests done with each story (ideally even follow TDD) and ensure all stories are passing in the whole project. Once they are and the story is complete - commit and push to the remote!!!\n\n## Why no prompts folder\n\nThe separate prompts folder was removed as it was redundant to maintain that along with the custom-mode-prompts. If you are using a tool without custom modes - the prompts still work as is, you will just use the idea and paste it into the chat to set up the LLMs operations, personality and behavior.\n\n## A note on Templates\n\nThe ai/templates folder contains a prd, architecture and story template. The prd and architecture templates themselves are embedded within the custom modes themselves and are not referenced by any custom models- so if using the modes for PM or Architect, you will not actually need those templates. The reason for not having it reference the external file (like the dev agent does) is that generally these modes can be used outside of cursor such as in Gemini or OpenAI - and it would be clunky to have a separate template file when its easier to just have it all in the external tool instruction set.\n\nThe story template is instead referenced from within the prompt so it will load the template when needed to draft an initial story. Having this as an external template makes it a bit easier to tweak the template - and the idea is that when the dev agent is working in your IDE it does not need to always have the content of the template in memory, and should always be able to reference it.\n\n## What about rules files?\n\nYou can still augment with rules files per your specific tool to put more guardrails in place. If you are going to use multiple tools and do not want to maintain a lot of different rule sets - you can instead add rules to non rules files such as docs, or contributing.md for example. And then just have a single rule that indicates the agent should reference these files when needed. YMMV with the approach - I have found it to work well enough - especially with the embedded agent modes rules.\n\n## Future Enhancements\n\n1. BMad Method MCP Tool\n2. Workflow Diagrams for different project types\n\n## Contributing\n\nInterested in improving the BMad Method? See our [contributing guidelines](CONTRIBUTING.md).\n\n## License\n\n[License](./LICENSE)\n",
    "keyword": "wow-for-the-bmad-code-method"
  },
  {
    "name": "requirements-template",
    "text": "# 👤 Actors & 🧩 Components (Who or what)\n> - Someone or something that can perform actions or be interacted with (examples include User, Button, Screen, Input Field, Message, System, API, Database, and they can be a person, service, visual or non-visual).\n\n> - What benefits from this? · Who maintains this? · What do users interact with? · What shows information? · What processes data? · What stores data? · What external systems are involved? · What needs to be monitored?\n\n> - GPT Instructions: Start by listing all nouns from your feature description - these are your potential actors and components. Then expand this list by asking: who uses it, what do they interact with, what shows information, what stores data, and what processes data? For each item, decide if it's an Actor (can perform actions) or Component (is acted upon). Finally, break down any complex components into smaller, more manageable pieces.\n\n> - Possible Parents: Itself\n> - Link actors and components to their (optional) parent by starting with the parent in [[double square brackets]] and the actor(s)/component(s) beneath it. Example:\n> \t- [[parent]]\n> \t\t- [[Actor]]\n> \t\t- [[Component]]\n---\n\n- [ ]\n\n# 🎬 Activities (Who or what does what?)\n> - Actions that an Actor or Component performs (examples include Create List, Delete Item, Sync Data, and they must always contain a verb + action).\n\n> - What can each actor do? · What should happen automatically? · What needs user input? · What happens periodically? · What triggers other activities? · What needs to be logged? · What needs to be measured? · What needs authorization?\n\n> - GPT Instructions: Take each Actor and Component and list everything they can do, must do, or should do automatically. Start each activity with a verb (create, update, delete, etc.) and make it specific. Think about: user interactions, system automations, periodic tasks, and data operations. Don't worry about the \"how\" yet - focus on what needs to happen.\n\n> - Possible Parents: Actor, Component\n> - Link activities to their parent by starting with the parent in [[double square brackets]] and the activitity beneath it. Example:\n> \t- [[parent]]\n> \t\t- [[Create item]]\n> \t\t- [[Delete item]]\n---\n\n- [ ]\n\n## 🌊 Activity Flows & Scenarios (What in which order?)\n> - Sequences of Atomic Actions (like \"Tap button\") that map out the steps to complete an Activity. May have optional paths for both successful completion (Happy Flow), errors (Error Flow), and scenarios like no connection, empty states, loading states, etc.\n\n> - What's the ideal path? · What could fail? · What needs validation? · What needs confirmation? · What's time sensitive? · What needs recovery steps? · What should be cached? · What should be retried? · What needs rollback?\n\n> - GPT Instructions: For each Activity think of the perfect scenario (Happy Flow) - what happens when everything works? Then optionally add Error Flows by asking \"what could go wrong?\" at each step. Finally, consider edge cases like no connection, empty states, or loading states. Break each flow into atomic (indivisible) actions that can be clearly implemented and tested. Prefix each atomic action with BDD Gherkin keywords: GIVEN, WHEN, THEN, AND, BUT.\n\n> - Possible Parents: Activities, Itself\n> - Link activity flows to their parent by starting with the parent in [[double square brackets]] and the activity flow(s) beneath it. Example:\n> \t- [[parent]]\n> \t\t- GIVEN [[User.pew]] is at [[Home Screen]]\n> \t\t- WHEN [[User.pew]] [[taps create item button]]\n> \t\t- THEN [[System.pew]] [[shows create item feedback]]\n> \t\t- AND [[System.pew]] [[creates database item]]\n> \t\t- BUT [[System.pew]] [[does not navigate]]\n---\n\n- [ ]\n\n# 📝 Properties (Which values?)\n> - Describes a value or configuration that belongs to an object (examples include width, color, id, name).\n\n> - What identifies it? · What describes it? · What configures it? · What measures it? · What styles it? · What formats it? · What tracks it? · What groups it? · What orders it?\n\n> - GPT Instructions: For each object in your system, think about its data needs in three categories: identity (what makes it unique), configuration (what can be changed), and state (what can vary). Consider what needs to be stored, displayed, measured, or tracked. Make sure each property has a clear type and purpose.\n\n> - Possible Parents: Actor, Component, Activity, Activity Flow, Scenario, Atomic Action, Scenario, Behaviour\n> - Link properties to their parent by starting with the parent in [[double square brackets]] and the property/properties beneath it. Example:\n> \t- [[parent]]\n> \t\t- [[name : string]]\n---\n\n- [ ]\n\n# 🛠️ Behaviours (How does it act when.. in terms of.. ?)\n> - Defines how something looks, works and performs Examples include ui/ux, rules & limits, data & analytics, security, performance and scalability.\n\n> - When should it change? · How should it respond? · What are the limits? · What needs validation? · What needs animation? · What needs protection? · What should be cached? · What should be optimized? · What should be monitored? · What needs fallback? · How should it scale? · What should be logged? · How should it fail? · What should be measured? · What needs authorization?\n\n> - GPT Instructions: Think about each object's rules and constraints in terms of: limits (max/min values, allowed inputs), timing (when, how often), security (who can access), and performance (what needs to be fast). Focus on behaviours that can be clearly tested - if you can't write a test for it, make it more specific.\n\n> - Possible Parents: Actor, Component, Activity, Activity Flow, Scenario, Atomic Action, Scenario, Property\n> - Link behaviours to their parent by starting with the parent in [[double square brackets]] and the behaviour(s) beneath it. Example:\n> \t- [ ] [[parent]]\n> \t\t- [ ] [[Should fail when length is 100+ characters]]\n> \t\t- [ ] [[Should not show when list is empty]]\n---\n\n- [ ]\n\n# 💡 Ideas & 🪵 Backlog\n> - Anything that could be added later, too complex now, needs more research, would be nice to have, or alternative approaches.\n\n> - What could be added later? · What's too complex now? · What needs more research? · What would be nice to have? · What are alternative approaches? · What could be automated?\n\n> - GPT Instructions: While working through the requirements, note down any ideas that come up but don't fit the current scope. Think about: future enhancements, alternative approaches, performance improvements, and nice-to-have features. Don't discard ideas just because they're complex - they might be valuable later.\n\n> - Possible Parents: Anything (optional)\n> - Link ideas and backlog items to their (optional) parent by starting with the parent in [[double square brackets]] and the idea(s)/backlog item(s) beneath it. Example:\n> \t- [ ] [[parent]]\n> \t\t- [ ] [[Do we need a limit?]]\n> \t\t- [ ] [[Is this safe?]]\n---\n\n- [ ]\n\n# ❓ Questions\n> - Questions that need to be answered to clarify requirements.\n\n> - What's unclear? · What needs decision? · What are the edge cases? · What could be improved? · What are we missing? · What assumptions are we making? · What risks exist? · What dependencies are there?\n\n> - GPT Instructions: Throughout the process, note any uncertainties or assumptions you make. Focus on questions that could impact implementation or user experience.\n\n> - Possible Parents: Everything (optional)\n> - Link questions to their (optional) parent by starting with the parent in [[double square brackets]] and the question(s) beneath it. Example:\n> \t- [ ] [[parent]]\n> \t\t- [ ] [[Do we need a limit?]]\n> \t\t- [ ] [[Is this safe?]]\n---\n\n- [ ]\n\n# 🎯 Roles, 📝 Tasks & 🎓 Suggested Approach\n> - Each behaviour, property, activity (flow), scenario, atomic action, actor, component must directly or indirectly (by parents) cascade down to a todo with assigned role. Creating a task for a parent and completing it automatically covers its children unless children have open tasks themselves.\n\n> - Who's responsible for what?\n\n> - GPT Instructions: Review all items in the requirements and create clear specific tasks for implementation. Every item should have at least one task. Group tasks by role (UI/UX, Frontend, Backend, etc.) and ensure they're specific enough to be actionable and testable.\n\n> - Possible Parents: Everything (optional)\n> - Link tasks to their parent by starting with the parent in [[double square brackets]] and the task(s) beneath it. Example:\n> \t- [ ] [[parent]]\n> \t\t- [ ] Make a cool design\n> \t\t- [ ] Get some feedback\n---\n\n- [ ] UI/UX Designer\n- [ ] Frontend Developer\n- [ ] Backend Developer\n- [ ] Data Engineer\n- [ ] DevOps Engineer\n- [ ] Project Manager\n- [ ] Marketeer",
    "keyword": "requirements-template"
  },
  {
    "name": "bug-tickets-template",
    "text": "# 🔖 Description\n> 💡 *A short and descriptive introduction of the problem we are going to solve.*\n\n# 🦋 Expected Result\n> 💡 *What should happen / what is expected in an ideal situation where the bug does not occur?*\n\n# 🐛 Actual Result\n> 💡 *What is actually happening in the current situation?*\n\n# 🔎 Steps to Reproduce\n> 💡 *Provide a step by step plan to mimic the bug. Attach any screenshots to the ticket if necessary.*\n\n# 💾 Data Model\n> 💡 *Old and new data models that will be created and/or altered when this feature is added.*\n\n# 🔒 Security Rules\n> 💡 *Old and new security rules with roles and access that should be created and/or altered. Include create, read, update and delete.*\n\n# 🐒 API\n> 💡 *Old and new API calls that should be created and/or altered.*\n\n# 📊 Analytics\n> 💡 *Old and new analytics that should be created and/or altered when this feature is added. Include a name, when it’s fired and optional properties.*\n\n# 🤝 Acceptance Test\n> 💡 *Which scenario’s should we test in the acceptance test? So that we can make sure that this feature does what it is supposed to do without any unexpected errors.*\n\n# ☎️ Impact Communication\n> 💡 *Who / which teams should we inform about the impact of releasing this ticket? Sales, marketing, data, CS, other?*\n\n# 🧪 Tests\n> 💡 *Components/flows/code that would benefit from tests and which scenario's should be tested.*\n\n# 🤝 Acceptance Test\n> 💡 *Which scenario’s should we test in the acceptance test? So that we can make sure that this ticket does what it is supposed to do without any unexpected errors.*\n\n# 📝 Suggested Approach\n> 💡 *With knowledge of the current codebase, try to define a best suggested approach. Think of current components used, flow of data and UI elements.*\n\n# 👉️ Final Remarks\n> 💡 *Anything to take note off that is not properly defined yet. Think of out of scope notes, dependencies, anything to be extra cautious about and/or information about related issues.*",
    "keyword": "bug-tickets-template"
  },
  {
    "name": "bmad-story-template",
    "text": "# {Epic-N} - {Story-#}\n\n{Story Title}\n\n**As a** {role}\n**I want** {action}\n**so that** {benefit}\n\n## Status\n\n{Draft|In Progress| Complete}\n\n## Context\n\n{\n\n- Background information\n- Current state\n- Story justification\n- Technical context\n- Business drivers\n- Relevant history from previous stories\n  }\n\n## Estimation\n\nStory Points: {Story Points (1 SP = 1 day of Human Development = 10 minutes of AI development)}\n\n## Tasks\n\n{\n\n1. - [ ] {Major Task Group 1}\n   1. - [ ] {Test Subtasks (as needed)}\n   2. - [ ] {Subtask}\n            N.\n            N.\n   3. N.\n\n- Use - [x] for completed items\n- Use ~~skipped/cancelled items~~\n  }\n\n## Constraints\n\n- List any technical or business constraints\n\n## Data Models / Schema\n\n- Database schemas\n- API request/response models\n- Interfaces/types\n\n## Structure\n\n- Module organization\n- File structure plan\n\n## Diagrams\n\n- Mermaid diagrams as needed\n\n## Dev Notes\n\n- Implementation commentary\n- Important considerations\n- Technical decisions made or changed\n\n## Chat Command Log\n\n- Commands from User\n- Agent Question and Users Response\n\n## Examples\n\n<example>\n# Epic-1 - Story-2\n# Implement Chessboard UI\n\n**As a** chess player\n**I want** to see a clear and interactive chessboard\n**so that** I can play chess in the web browser\n\n## Status\n\nIn Progress\n\n## Context\n\nPart of Epic-1 which implements the core 2-player chess game. This story focuses on the visual and interactive aspects of the chessboard. The project setup (Story-1) is complete, providing the foundation for UI implementation.\n\n## Estimation\n\nStory Points: 2\n\n## Tasks\n\n1. - [x] Create Chessboard Grid\n   1. - [x] Implement 8x8 board layout\n   2. - [x] Add square coloring\n   3. - [x] Write grid tests\n2. - [ ] Add Chess Pieces\n   1. - [ ] Create piece components\n   2. - [ ] Add piece images\n   3. - [ ] Write piece tests\n3. - [ ] Implement Basic Interaction\n   1. - [ ] Add click handlers\n   2. - [ ] Highlight selected square\n   3. - [ ] Write interaction tests\n\n## Constraints\n\n- Always ensure that we are using the Eastern Time Zone for all dates\n\n## Data Models / Schema\n\n```json piece.mode\n{\n   id: number\n   position?: BoardPosition\n   captured: boolean\n   name: string\n}\n```\n\n## Structure\n\nThis new feature is implemented under /src/new-foo-api for the handler, with all logic beyond request and response in new-foo-service.ts and src/data/new-foo-data.ts handling all data access against dynamoDb.\n\n## Diagrams\n\n{mermaid sequence diagram of capture piece logic and updating database}\n\n## Dev Notes\n\n- Ensure we are putting all code in its proper layer - reference the structure section above - also check the notes of Story-1 where we made a decision to pivot to always using SVG files instead of PNG files.\n\n## Chat Command Log\n\n- BMad: Let's implement the chessboard UI\n- ....\n- AiAgent: Grid implementation complete, proceeding with piece placement\n- BMad: Why did you delete all of the files I asked you to move and move all the files I asked you to delete!!! Bad Agent\n- AiAgent: 1000 pardons master BMad I will correct that now <deletes entire project and uninstalls cursor from machine>\n- BMad: Noooooooo!!!!!!!!!!!!!\n  </example>\n",
    "keyword": "bmad-story-template"
  },
  {
    "name": "project-plans-template",
    "text": "# Project Plan: [Project Name]\n\n## 1. Project Overview\nA brief summary of the project, including its main objectives and key features. Clearly state the end goals formulated in your analysis.\n- [ ] Read the project overview:\n    - [Brief summary of the project, including end goals]\n\n## 2. Requirements\nOverview of all requirements.\n- [ ] Read the requirements:\n    - 👤 Actors & 🧩 Components:\n        - [Actors]\n        - [Components]\n    - 🎬 Activities: Specify what actions need to be performed.\n        - [Actor]\n            - [Activity]\n        - [Component]\n            - [Activity]\n    - 🌊 Activity Flows & Scenarios: Break down complex activities into step-by-step processes.\n        - [Parent]\n            - [Activity Flow]\n    - 📝 Properties: Define any values or configurations associated with components or activities.\n        - [Parent]\n            - [Property]\n    - 🛠️ Behaviours: Describe how actors, components, properties, and activities should act or respond in different situations.\n        - [Parent]\n            - [Behaviour]\n\n## 3. Milestones and Tasks\nThe project broken down into milestones. Each milestone should be executable by an independent AI developer agent. Each milestone should not exceed 3 story points and should be executable independently. You can assume that each milestone will be offered in a new call by an agent with empty context. However, the executing agent will have access the ticket and thus be able to form an idea about the work that has been done.\n\nEach milestone consists of individual tasks for the unpacking agent. For each task, include:\n    - A one-sentence to one-paragraph description of what needs to be done, starting with a verb.\n    - A sequence diagram of end result.\n    - File names that will be created, read, updated, or deleted (CRUD), using proper naming conventions and casing styles.\n    - Objects/classes that will be CRUDed, including appropriate class keywords (e.g., sealed, abstract).\n    - Variables that will be CRUDed, including types, values, and keywords. Use proper casing and specify whether they are part of a class, method, or global constants.\n    - Methods that will be CRUDed, including return values, input values, and whether they are async/sync.\n    - For any complex processes or setup required to achieve a task or goal, provide clear, step-by-step instructions on how to complete these processes.\n\n\n### Milestone 1: [Milestone Name]\n[Milestone description]\n\n#### Task title\n- [ ] 1. [Task description]\n- Sequence diagram\n    - [ASCII art or textual representation of the sequence diagram]\n- Files:\n    - [List of files]\n- Classes:\n    - [List of classes]\n- Variables:\n    - [List of variables]\n- Methods:\n    - [List of methods]\n- Process:\n    - [Step-by-step instructions for any complex processes]\n\n- [ ] 2. [Next task...]\n\n#### Task title\n- [ ] 1. [Task description]\n- Files:\n    - [List of files]\n- Classes:\n    - [List of classes]\n- Variables:\n    - [List of variables]\n- Methods:\n    - [List of methods]\n- Process:\n    - [Step-by-step instructions for any complex processes]\n\n### Milestone 2: [Milestone Name]\n[Repeat the structure for each milestone]",
    "keyword": "project-plans-template"
  },
  {
    "name": "task-ticket-template",
    "text": "# 🔖 Description\n> 💡 *A short and descriptive introduction of the problem we are going to solve.*\n\n# ⚙️ Requirements\n> 💡 *Describe the requirements for this task. What needs to be done? What are the expected outcomes?*\n\n# 💾 Data Model\n> 💡 *Old and new data models that will be created and/or altered when this feature is added.*\n\n# 🔒 Security Rules\n> 💡 *Old and new security rules with roles and access that should be created and/or altered. Include create, read, update and delete.*\n\n# 🐒 API\n> 💡 *Old and new API calls that should be created and/or altered.*\n\n# 📊 Analytics\n> 💡 *Old and new analytics that should be created and/or altered when this feature is added. Include a name, when it’s fired and optional properties.*\n\n# 🤝 Acceptance Test\n> 💡 *Which scenario’s should we test in the acceptance test? So that we can make sure that this feature does what it is supposed to do without any unexpected errors.*\n\n# ☎️ Impact Communication\n> 💡 *Who / which teams should we inform about the impact of releasing this ticket? Sales, marketing, data, CS, other?*\n\n# 🧪 Tests\n> 💡 *Components/flows/code that would benefit from tests and which scenario's should be tested.*\n\n# 🤝 Acceptance Test\n> 💡 *Which scenario’s should we test in the acceptance test? So that we can make sure that this ticket does what it is supposed to do without any unexpected errors.*\n\n# 🎨 UI/UX Behaviour\n> 💡 *Anything to take note of regarding the behaviour of UI/UX elements (if applicable). Think of position, behaviour when elements do not fit the screen, feedback on elements and properties of animations.*\n\n# 📝 Suggested Approach\n> 💡 *With knowledge of the current codebase, try to define a best suggested approach. Think of current components used, flow of data and UI elements.*\n\n# 👉️ Final Remarks\n> 💡 *Anything to take note off that is not properly defined yet. Think of out of scope notes, dependencies, anything to be extra cautious about and/or information about related issues.*\n",
    "keyword": "task-ticket-template"
  },
  {
    "name": "story-ticket-template",
    "text": "# 🔖 Description\n> 💡 *A short and descriptive introduction of the problem we are going to solve.*\n\n# 🗣 User Story\n> 💡 ***As a*** *ROLE* ***I want*** *BEHAVIOUR* ***so that*** *REASON.*\n\n# ⚙️ Requirements\n> 💡 ***Suggested Approach****\n> 1. *Start by writing down your natural thoughts about what **activities** and **properties** this feature should do/have and write them down.*\n> 2. *Dissect what you wrote down and look for **actors**, **components**, other **activities** or **properties** and **behaviours**. Write them down.*\n> 3. *Ask yourself (difficult) questions about the implementation of things in regards of other concepts. Use the captions underneath any of the concepts for inspiration. Write them down.*\n> 4. *Repeat step 2 and 3 until you can’t come up with any new **questions**, **activities**, **properties** or **behaviours**.*\n> \n> *💡 **Relationships between concepts:***\n> * ***🧩 Actors** and **🧩 Components** can have **🎬 Activities** and **📝 Properties**.*\n> - ***🎬 Activities** and **📝 Properties** can have **🛠️ Behaviours**.*\n> - *(Optional) **All concepts** (🧩🎬📝🛠️) should be (indirectly) linked to a clear and conside **🎯 Role & Todo**. Place anything out of scope in the backlog.*\n\n## 🧩 Actors / Components\n> *User · Client · Schedule · Session · Screen / View · Widget · App*\n\n## 🎬 Activities\n> *Actor should.. · System should.. · Screen can.. · When widget then.. · If action then another action..*\n\n## 📝 Properties\n> *User.id · Schedule.startTime · Session.userIds · Screen background color · Widget border radius*\n\n## 🛠️ Behaviours\n> *Look & Feel · Rules & Limits · Data & Analytics · Implementation Details · Error Flow & Handling · Security · Performance · Scalability*\n\n## 🎯 Roles & Todo's\n> *Backend Dev · Front-end Dev · Ui/Ux Designer · DevOps Engineer*\n\n* Project Manager\n* Backend Developer:\n* Front-end Developer:\n* UI/UX Designer:\n* DevOps Engineer:\n* Data Engineer:\n\n## 📖 Backlog\n> *Out of scope for now · Maybe later*\n\n# 💾 Data Model\n> 💡 *Old and new data models that will be created and/or altered when this feature is added.*\n\n# 🔒 Security Rules\n> 💡 *Old and new security rules with roles and access that should be created and/or altered. Include create, read, update and delete.*\n\n# 🐒 API\n> 💡 *Old and new API calls that should be created and/or altered.*\n\n# 📊 Analytics\n> 💡 *Old and new analytics that should be created and/or altered when this feature is added. Include a name, when it’s fired and optional properties.*\n\n# 🤝 Acceptance Test\n> 💡 *Which scenario’s should we test in the acceptance test? So that we can make sure that this feature does what it is supposed to do without any unexpected errors.*\n\n# ☎️ Impact Communication\n> 💡 *Who / which teams should we inform about the impact of releasing this ticket? Sales, marketing, data, CS, other?*\n\n# 🧪 Tests\n> 💡 *Components/flows/code that would benefit from tests and which scenario's should be tested.*\n\n# 🤝 Acceptance Test\n> 💡 *Which scenario’s should we test in the acceptance test? So that we can make sure that this ticket does what it is supposed to do without any unexpected errors.*\n\n# 🎨 UI/UX Behaviour\n> 💡 *Anything to take note of regarding the behaviour of UI/UX elements (if applicable). Think of position, behaviour when elements do not fit the screen, feedback on elements and properties of animations.*\n\n# 📝 Suggested Approach\n> 💡 *With knowledge of the current codebase, try to define a best suggested approach. Think of current components used, flow of data and UI elements.*\n\n# 👉️ Final Remarks\n> 💡 *Anything to take note off that is not properly defined yet. Think of out of scope notes, dependencies, anything to be extra cautious about and/or information about related issues.*",
    "keyword": "story-ticket-template"
  },
  {
    "name": "milestone-template",
    "text": "# Milestone: [Enter Milestone Title Here]\n\n---\n\n## 🎯 Goal / Objective\n*   _Clearly state the primary goal or objective this milestone aims to achieve. What does success look like for this specific milestone?_\n\n---\n\n## 🛠️ Key Tasks / Activities\n*   [ ] Task 1: _Describe the first key task required._\n*   [ ] Task 2: _Describe the second key task required._\n*   [ ] Task 3: _Add more tasks as needed._\n*   ...\n\n---\n\n## 👤 Related User Stories\n*   _List the specific User Stories associated with this milestone. These stories will be further detailed using the User Story Proposal template._\n    *   [ ] User Story 1 Title: [Link to proposal if available, or brief description]\n    *   [ ] User Story 2 Title: [Link to proposal if available, or brief description]\n    *   ...\n\n---\n\n## 📦 Deliverables / Artifacts\n*   _List the tangible outputs or artifacts that will be produced upon completion of this milestone._\n    *   Example: Updated `README.md`\n    *   Example: Functional prototype link\n    *   Example: Test results document\n\n---\n\n## ✅ Acceptance Criteria\n*   _Define the specific, measurable criteria that must be met for this milestone to be considered complete._\n    *   Criterion 1: _e.g., Code merged to main branch._\n    *   Criterion 2: _e.g., Feature X is demonstrable in the staging environment._\n    *   Criterion 3: _e.g., All tasks listed above are marked as complete._\n\n---\n\n## 🔗 Dependencies / Related Links (Optional)\n*   _List any dependencies (e.g., previous milestones, external factors) or relevant links (e.g., design docs, tickets)._\n    *   Dependency: [Link or Description]\n    *   Related: [Link or Description]\n\n---\n\n## 📝 Notes (Optional)\n*   _Add any additional context, notes, or risks relevant to this milestone._\n\n---\n",
    "keyword": "milestone-template"
  },
  {
    "name": "wow-for-prompt-engineering-by-lee-boonstra",
    "text": "**Prompt**    \n**Engineering Author: Lee Boonstra**  \nPrompt Engineering \n\n**Acknowledgements** \n\n**Content contributors** Michael Sherman   \nYuan Cao \n\nErick Armbrust \n\nAnant Nawalgaria \n\nAntonio Gulli \n\nSimone Cammel \n\n**Curators and Editors** Antonio Gulli   \nAnant Nawalgaria \n\nGrace Mollison  \n\n**Technical Writer** \n\nJoey Haymaker \n\n**Designer** \n\nMichael Lanning \n\nFebruary 2025 2   \n**Table of contents**\n\nIntroduction 6 Prompt engineering 7 LLM output configuration 8 \n\nOutput length 8 Sampling controls 9 Temperature 9 Top-K and top-P 10 Putting it all together 11 Prompting techniques 13 General prompting / zero shot 13 One-shot & few-shot 15 System, contextual and role prompting 18 System prompting 19 Role prompting 21 Contextual prompting 23   \nStep-back prompting 25 Chain of Thought (CoT) 29 Self-consistency 32 Tree of Thoughts (ToT) 36 ReAct (reason & act) 37 Automatic Prompt Engineering 40 Code prompting 42 \n\nPrompts for writing code 42 Prompts for explaining code 44 Prompts for translating code 46 Prompts for debugging and reviewing code 48 What about multimodal prompting? 54 \n\nBest Practices 54 Provide examples 54 Design with simplicity 55 Be specific about the output 56 Use Instructions over Constraints 56 Control the max token length 58 Use variables in prompts 58 Experiment with input formats and writing styles 59 For few-shot prompting with classification tasks, mix up the classes 59 Adapt to model updates 60 Experiment with output formats 60  \nJSON Repair 61 Working with Schemas 62 Experiment together with other prompt engineers 63 CoT Best practices 64 Document the various prompt attempts 64 \n\nSummary 66 Endnotes 68  \nPrompt Engineering \n\nYou don’t need to be a data  \n\nscientist or a machine learning  engineer – everyone can write  a prompt.\n\n**Introduction** \n\nWhen thinking about a large language model input and output, a text prompt (sometimes  accompanied by other modalities such as image prompts) is the input the model uses  to predict a specific output. You don’t need to be a data scientist or a machine learning  engineer – everyone can write a prompt. However, crafting the most effective prompt can be  complicated. Many aspects of your prompt affect its efficacy: the model you use, the model’s  training data, the model configurations, your word-choice, style and tone, structure, and  context all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts  can lead to ambiguous, inaccurate responses, and can hinder the model’s ability to provide  meaningful output. \n\nFebruary 2025 6   \nPrompt Engineering \n\nWhen you chat with the Gemini chatbot,1 you basically write prompts, however this  whitepaper focuses on writing prompts for the Gemini model within Vertex AI or by using  the API, because by prompting the model directly you will have access to the configuration  such as temperature etc. \n\nThis whitepaper discusses prompt engineering in detail. We will look into the various  prompting techniques to help you getting started and share tips and best practices to  become a prompting expert. We will also discuss some of the challenges you can face  while crafting prompts. \n\n**Prompt engineering** \n\nRemember how an LLM works; it’s a prediction engine. The model takes sequential text as  an input and then predicts what the following token should be, based on the data it was  trained on. The LLM is operationalized to do this over and over again, adding the previously  predicted token to the end of the sequential text for predicting the following token. The next  token prediction is based on the relationship between what’s in the previous tokens and what  the LLM has seen during its training.  \n\nWhen you write a prompt, you are attempting to set up the LLM to predict the right sequence  of tokens. Prompt engineering is the process of designing high-quality prompts that guide  LLMs to produce accurate artifacts. This process involves tinkering to find the best prompt,  optimizing prompt length, and evaluating a prompt’s writing style and structure in relation  to the task. In the context of natural language processing and LLMs, a prompt is an input  provided to the model to generate a response or prediction.\n\nFebruary 2025 7   \nPrompt Engineering \n\nThese prompts can be used to achieve various kinds of understanding and generation  tasks such as text summarization, information extraction, question and answering, text  classification, language or code translation, code generation, and code documentation  or reasoning. \n\nPlease feel free to refer to Google’s prompting guides2,3 with simple and effective  prompting examples. \n\nWhen prompt engineering, you will start by choosing a model. Prompts might need to be  optimized for your specific model, regardless of whether you use Gemini language models in  Vertex AI, GPT, Claude, or an open source model like Gemma or LLaMA. \n\nBesides the prompt, you will also need to tinker with the various configurations of a LLM. \n\n**LLM output configuration** \n\nOnce you choose your model you will need to figure out the model configuration. Most LLMs  come with various configuration options that control the LLM’s output. Effective prompt  engineering requires setting these configurations optimally for your task. \n\n**Output length** \n\nAn important configuration setting is the number of tokens to generate in a response.  Generating more tokens requires more computation from the LLM, leading to higher energy  consumption, potentially slower response times, and higher costs.\n\nFebruary 2025 8   \nPrompt Engineering \n\nReducing the output length of the LLM doesn’t cause the LLM to become more stylistically  or textually succinct in the output it creates, it just causes the LLM to stop predicting more  tokens once the limit is reached. If your needs require a short output length, you’ll also  possibly need to engineer your prompt to accommodate. \n\nOutput length restriction is especially important for some LLM prompting techniques, like  ReAct, where the LLM will keep emitting useless tokens after the response you want. \n\nBe aware, generating more tokens requires more computation from the LLM, leading  to higher energy consumption and potentially slower response times, which leads to  higher costs. \n\n**Sampling controls** \n\nLLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the  next token could be, with each token in the LLM’s vocabulary getting a probability. Those  token probabilities are then sampled to determine what the next produced token will be.  Temperature, top-K, and top-P are the most common configuration settings that determine  how predicted token probabilities are processed to choose a single output token. \n\n**Temperature** \n\nTemperature controls the degree of randomness in token selection. Lower temperatures  are good for prompts that expect a more deterministic response, while higher temperatures  can lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is \n\nFebruary 2025 9   \nPrompt Engineering \n\ndeterministic: the highest probability token is always selected (though note that if two tokens  have the same highest predicted probability, depending on how tiebreaking is implemented  you may not always get the same output with temperature 0). \n\nTemperatures close to the max tend to create more random output. And as temperature gets  higher and higher, all tokens become equally likely to be the next predicted token. \n\nThe Gemini temperature control can be understood in a similar way to the softmax function  used in machine learning. A low temperature setting mirrors a low softmax temperature (T),  emphasizing a single, preferred temperature with high certainty. A higher Gemini temperature  setting is like a high softmax temperature, making a wider range of temperatures around  the selected setting more acceptable. This increased uncertainty accommodates scenarios  where a rigid, precise temperature may not be essential like for example when experimenting  with creative artifacts. \n\n**Top-K and top-P** \n\nTop-K and top-P (also known as nucleus sampling)4 are two sampling settings used in LLMs  to restrict the predicted next token to come from tokens with the top predicted probabilities.  Like temperature, these sampling settings control the randomness and diversity of  generated text. \n\n• **Top-K** sampling selects the top K most likely tokens from the model’s predicted  distribution. The higher top-K, the more creative and varied the model’s output; the  lower top-K, the more restive and factual the model’s output. A top-K of 1 is equivalent to  greedy decoding.\n\nFebruary 2025 10   \nPrompt Engineering \n\n• **Top-P** sampling selects the top tokens whose cumulative probability does not exceed  a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the  LLM’s vocabulary). \n\nThe best way to choose between top-K and top-P is to experiment with both methods (or  both together) and see which one produces the results you are looking for.  \n\n**Putting it all together** \n\nChoosing between top-K, top-P, temperature, and the number of tokens to generate,  depends on the specific application and desired outcome, and the settings all impact one  another. It’s also important to make sure you understand how your chosen model combines  the different sampling settings together. \n\nIf temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet  both the top-K and top-P criteria are candidates for the next predicted token, and then  temperature is applied to sample from the tokens that passed the top-K and top-P criteria. If  only top-K or top-P is available, the behavior is the same but only the one top-K or P setting  is used.  \n\nIf temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then  randomly selected from to produce a single next predicted token. \n\nAt extreme settings of one sampling configuration value, that one sampling setting either  cancels out other configuration settings or becomes irrelevant.\n\nFebruary 2025 11   \nPrompt Engineering \n\n• If you set temperature to 0, top-K and top-P become irrelevant–the most probable  token becomes the next token predicted. If you set temperature extremely high (above  1–generally into the 10s), temperature becomes irrelevant and whatever tokens make  it through the top-K and/or top-P criteria are then randomly sampled to choose a next  predicted token. \n\n• If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the  top-K criteria, and that token is the next predicted token. If you set top-K extremely high,  like to the size of the LLM’s vocabulary, any token with a nonzero probability of being the  next token will meet the top-K criteria and none are selected out. \n\n• If you set top-P to 0 (or a very small value), most LLM sampling implementations will then  only consider the most probable token to meet the top-P criteria, making temperature and  top-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the  next token will meet the top-P criteria, and none are selected out. \n\nAs a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you  relatively coherent results that can be creative but not excessively so. If you want especially  creative results, try starting with a temperature of .9, top-P of .99, and top-K of 40\\. And if you  want less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20\\.  Finally, if your task always has a single correct answer (e.g., answering a math problem), start  with a temperature of 0\\. \n\n**NOTE:** With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM  might generate text that is less relevant. \n\n**WARNING:** Have you ever seen a response ending with a large amount of filler words? This  is also known as the \"repetition loop bug\", which is a common issue in Large Language  Models where the model gets stuck in a cycle, repeatedly generating the same (filler) word,  phrase, or sentence structure, often exacerbated by inappropriate temperature and top-k/\n\nFebruary 2025 12   \nPrompt Engineering \n\ntop-p settings. This can occur at both low and high temperature settings, though for different  reasons. At low temperatures, the model becomes overly deterministic, sticking rigidly to the  highest probability path, which can lead to a loop if that path revisits previously generated  text. Conversely, at high temperatures, the model's output becomes excessively random,  increasing the probability that a randomly chosen word or phrase will, by chance, lead back  to a prior state, creating a loop due to the vast number of available options. In both cases,  the model's sampling process gets \"stuck,\" resulting in monotonous and unhelpful output  until the output window is filled. Solving this often requires careful tinkering with temperature  and top-k/top-p values to find the optimal balance between determinism and randomness. \n\n**Prompting techniques** \n\nLLMs are tuned to follow instructions and are trained on large amounts of data so they can  understand a prompt and generate an answer. But LLMs aren’t perfect; the clearer your  prompt text, the better it is for the LLM to predict the next likely text. Additionally, specific  techniques that take advantage of how LLMs are trained and how LLMs work will help you get  the relevant results from LLMs \n\nNow that we understand what prompt engineering is and what it takes, let’s dive into some  examples of the most important prompting techniques. \n\n**General prompting / zero shot** \n\nA *zero-shot*5 prompt is the simplest type of prompt. It only provides a description of a task  and some text for the LLM to get started with. This input could be anything: a question, a  start of a story, or instructions. The name zero-shot stands for ’no examples’.\n\nFebruary 2025 13   \nPrompt Engineering \n\nLet’s use Vertex AI Studio (for Language) in Vertex AI,6 which provides a playground to test  prompts. In Table 1, you will see an example zero-shot prompt to classify movie reviews.  \n\nThe table format as used below is a great way of documenting prompts. Your prompts will  likely go through many iterations before they end up in a codebase, so it’s important to keep  track of your prompt engineering work in a disciplined, structured way. More on this table  format, the importance of tracking prompt engineering work, and the prompt development  process is in the Best Practices section later in this chapter (“Document the various prompt  attempts”).  \n\nThe model temperature should be set to a low number, since no creativity is needed, and we  use the gemini-pro default top-K and top-P values, which effectively disable both settings  (see ‘LLM Output Configuration’ above). Pay attention to the generated output. The words  *disturbing* and *masterpiece* should make the prediction a little more complicated, as both  words are used in the same sentence.\n\nFebruary 2025 14   \nPrompt Engineering \n\nName 1\\_1\\_movie\\_classification \n\n| Goal  | Classify movie reviews as positive, neutral or negative. |  |  |\n| :---- | :---- | :---- | :---- |\n| Model  | gemini-pro |  |  |\n| Temperature  | 0.1  | Token Limit  | 5 |\n| Top-K  | N/A  | Top-P  | 1 |\n| Prompt  | `Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. Review: \"Her\" is a disturbing study revealing the direction  humanity is headed if AI is allowed to keep evolving,  unchecked. I wish there were more movies like this masterpiece. Sentiment:` |  |  |\n\nOutput POSITIVE \n\nTable 1\\. An example of zero-shot prompting \n\nWhen zero-shot doesn’t work, you can provide demonstrations or examples in the prompt,  which leads to “one-shot” and “few-shot” prompting. General prompting / zero shot \n\n**One-shot & few-shot** \n\nWhen creating prompts for AI models, it is helpful to provide examples. These examples can  help the model understand what you are asking for. Examples are especially useful when you  want to steer the model to a certain output structure or pattern.  \n\nA **one-shot prompt**, provides a *single* example, hence the name one-shot. The idea is the  model has an example it can imitate to best complete the task. \n\nA **few-shot prompt** 7 provides *multiple* examples to the model. This approach shows the  model a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples  of the desired pattern increases the chance the model follows the pattern.\n\nFebruary 2025 15   \nPrompt Engineering \n\nThe number of examples you need for few-shot prompting depends on a few factors,  including the complexity of the task, the quality of the examples, and the capabilities of the  generative AI (gen AI) model you are using. As a general rule of thumb, you should use at  least three to five examples for few-shot prompting. However, you may need to use more  examples for more complex tasks, or you may need to use fewer due to the input length  limitation of your model. \n\nTable 2 shows a few-shot prompt example, let’s use the same gemini-pro model  configuration settings as before, other than increasing the token limit to accommodate the  need for a longer response. \n\nGoal Parse pizza orders to JSON \n\n| Model  | gemini-pro |  |  |\n| :---- | :---- | :---- | :---- |\n| Temperature  | 0.1  | Token Limit  | 250 |\n| Top-K  | N/A  | Top-P  | 1 |\n\nPrompt `Parse a customer's pizza order into valid JSON:` \n\n`EXAMPLE:` \n\n`I want a small pizza with cheese, tomato sauce, and pepperoni. JSON Response:` \n\n```` ``` ```` \n\n`{` \n\n`\"size\": \"small\",` \n\n`\"type\": \"normal\",` \n\n`\"ingredients\": [[\"cheese\", \"tomato sauce\", \"peperoni\"]]` \n\n`}` \n\n```` ``` ```` \n\n**Continues next page...**\n\nFebruary 2025 16   \nPrompt Engineering \n\nPrompt `EXAMPLE:` \n\n`Can I get a large pizza with tomato sauce, basil and mozzarella`  \n\n`{` \n\n`\"size\": \"large\",` \n\n`\"type\": \"normal\",` \n\n`\"ingredients\": [[\"tomato sauce\", \"bazel\", \"mozzarella\"]]` \n\n`}` \n\n`Now, I would like a large pizza, with the first half cheese and  mozzarella. And the other tomato sauce, ham and pineapple.` \n\n`JSON Response:` \n\nOutput ```` ``` ```` \n\n`{` \n\n`\"size\": \"large\",` \n\n`\"type\": \"half-half\",` \n\n`\"ingredients\": [[\"cheese\", \"mozzarella\"], [\"tomato sauce\",`  \n\n`\"ham\", \"pineapple\"]]` \n\n`}` \n\n```` ``` ```` \n\nTable 2\\. An example of few-shot prompting \n\nWhen you choose examples for your prompt, use examples that are relevant to the task you  want to perform. The examples should be diverse, of high quality, and well written. One small  mistake can confuse the model and will result in undesired output. \n\nIf you are trying to generate output that is robust to a variety of inputs, then it is important to  include edge cases in your examples. Edge cases are inputs that are unusual or unexpected,  but that the model should still be able to handle.\n\nFebruary 2025 17   \nPrompt Engineering \n\n**System, contextual and role prompting** \n\nSystem, contextual and role prompting are all techniques used to guide how LLMs generate  text, but they focus on different aspects: \n\n• **System prompting** sets the overall context and purpose for the language model. It  defines the ‘big picture’ of what the model should be doing, like translating a language,  classifying a review etc. \n\n• **Contextual prompting** provides specific details or background information relevant to  the current conversation or task. It helps the model to understand the nuances of what’s  being asked and tailor the response accordingly. \n\n• **Role prompting** assigns a specific character or identity for the language model to adopt.  This helps the model generate responses that are consistent with the assigned role and its  associated knowledge and behavior. \n\nThere can be considerable overlap between system, contextual, and role prompting. E.g. a  prompt that assigns a role to the system, can also have a context. \n\nHowever, each type of prompt serves a slightly different primary purpose: • System prompt: Defines the model’s fundamental capabilities and overarching purpose. \n\n• Contextual prompt: Provides immediate, task-specific information to guide the response.  It’s highly specific to the current task or input, which is dynamic. \n\n• Role prompt: Frames the model’s output style and voice. It adds a layer of specificity  and personality.\n\nFebruary 2025 18   \nPrompt Engineering \n\nDistinguishing between system, contextual, and role prompts provides a framework for  designing prompts with clear intent, allowing for flexible combinations and making it easier to  analyze how each prompt type influences the language model’s output. \n\nLet’s dive into these three different kinds of prompts. \n\n**System prompting** \n\nTable 3 contains a system prompt, where I specify additional information on how to return the  output. I increased the temperature to get a higher creativity level, and I specified a higher  token limit. However, because of my clear instruction on how to return the output the model  didn’t return extra text. \n\nGoal Classify movie reviews as positive, neutral or negative. \n\n| Model  | gemini-pro |  |  |\n| :---- | :---- | :---- | :---- |\n| Temperature  | 1  | Token Limit  | 5 |\n| Top-K  | 40  | Top-P  | 0.8 |\n| Prompt  | `Classify movie reviews as positive, neutral or negative. Only  return the label in uppercase.  Review: \"Her\" is a disturbing study revealing the direction  humanity is headed if AI is allowed to keep evolving,  unchecked. It's so disturbing I couldn't watch it. Sentiment:` |  |  |\n\nOutput NEGATIVE \n\nTable 3\\. An example of system prompting \n\nSystem prompts can be useful for generating output that meets specific requirements. The  name ‘system prompt’ actually stands for ‘providing an additional task to the system’. For  example, you could use a system prompt to generate a code snippet that is compatible with  a specific programming language, or you could use a system prompt to return a certain  structure. Have a look into Table 4, where I return the output in JSON format.\n\nFebruary 2025 19   \nPrompt Engineering \n\nGoal Classify movie reviews as positive, neutral or negative, return JSON. \n\n| Model  | gemini-pro |  |  |\n| :---- | :---- | :---- | :---- |\n| Temperature  | 1  | Token Limit  | 1024 |\n| Top-K  | 40  | Top-P  | 0.8 |\n| Prompt  | ````Classify movie reviews as positive, neutral or negative. Return  valid JSON:  Review: \"Her\" is a disturbing study revealing the direction  humanity is headed if AI is allowed to keep evolving,  unchecked. It's so disturbing I couldn't watch it.  Schema:  ```  MOVIE:  {  \"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \"NEUTRAL\", \"name\": String  }  MOVIE REVIEWS:  {  \"movie_reviews\": [MOVIE]  }  ```  JSON Response:```` |  |  |\n\nOutput ```` ``` ```` \n\n`{` \n\n `\"movie_reviews\": [` \n\n `{` \n\n `\"sentiment\": \"NEGATIVE\",` \n\n `\"name\": \"Her\"` \n\n `}` \n\n `]` \n\n`}` \n\n```` ``` ```` \n\nTable 4\\. An example of system prompting with JSON format\n\nFebruary 2025 20   \nPrompt Engineering \n\nThere are some benefits in returning JSON objects from a prompt that extracts data. In  a real-world application I don’t need to manually create this JSON format, I can already  return the data in a sorted order (very handy when working with datetime objects), but most  importantly, by prompting for a JSON format it forces the model to create a structure and  limit hallucinations. \n\nSystem prompts can also be really useful for safety and toxicity. To control the output, simply  add an additional line to your prompt like: ‘You should be respectful in your answer.’. \n\n**Role prompting** \n\nRole prompting is a technique in prompt engineering that involves assigning a specific role to  the gen AI model. This can help the model to generate more relevant and informative output,  as the model can craft its responses to the specific role that it has been assigned. \n\nFor example, you could role prompt a gen AI model to be a book editor, a kindergarten  teacher, or a motivational speaker. Once the model has been assigned a role, you can then  give it prompts that are specific to that role. For example, you could prompt a teacher to  create a lesson plan that they can then review. Have a look into Table 5, where the model  takes the role of a travel guide.\n\nFebruary 2025 21   \nPrompt Engineering \n\nGoal Act as travel guide and provide 3 travel suggestions \n\n| Model  | gemini-pro |  |  |\n| :---- | :---- | :---- | :---- |\n| Temperature  | 1  | Token Limit  | 1024 |\n| Top-K  | 40  | Top-P  | 0.8 |\n| Prompt  | `I want you to act as a travel guide. I will write to you  about my location and you will suggest 3 places to visit near  me. In some cases, I will also give you the type of places I  will visit.  My suggestion: \"I am in Amsterdam and I want to visit  only museums.\"  Travel Suggestions:` |  |  |\n\nOutput 1\\. Rijksmuseum: World-renowned museum, known for Dutch masterpieces  like Rembrandt’s “The Night Watch.” Don’t miss the incredible collection of  \n\npaintings and artifacts showcasing Dutch art and history. \n\n2\\. Van Gogh Museum: Immerse yourself in the captivating works of Vincent  \n\nvan Gogh. The museum houses the largest collection of his paintings and  \n\ndrawings, including “The Starry Night” and “Sunflowers.” \n\n3\\. Stedelijk Museum Amsterdam: Discover modern and contemporary art  \n\nfrom around the world. The museum’s collection includes works by Picasso,  \n\nKandinsky, and Rietveld, housed in a striking modern building. \n\nTable 5\\. An example of role prompting \n\nThe above example shows an example of taking the role of a travel agent. When you  would change the role to a teacher of geography, you would notice that you will receive a  different response. \n\nDefining a role perspective for an AI model gives it a blueprint of the tone, style, and focused  expertise you’re looking for to improve the quality, relevance, and effectiveness of your  output. \n\nFebruary 2025 22   \nPrompt Engineering \n\nHere are some styles you can choose from which I find effective: \n\nConfrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal,  Inspirational, Persuasive \n\nLet’s change our prompt in Table 6 to include a humorous and inspirational style. \n\nPrompt `I want you to act as a travel guide. I will write to you about  my location and you will suggest 3 places to visit near me in  a humorous style.` \n\n`My suggestion: \"I am in Manhattan.\"` \n\n`Travel Suggestions:` \n\nOutput 1\\. Behold the Empire State of Mind: Ascend to the dizzying heights of the Empire  State Building and bask in the glory of Manhattan's skyline. Prepare to feel like  King Kong atop the Big Apple, minus the giant ape-sized banana. \n\n2\\. Get Artsy-Fartsy at MoMA: Unleash your inner art aficionado at the Museum  of Modern Art (MoMA). Gaze upon masterpieces that will boggle your  \n\nmind and make you question whether your stick-figure drawings have any  \n\nartistic merit. \n\n3\\. Shop ‘Til You Drop on Fifth Avenue: Indulge in some retail therapy on the  \n\niconic Fifth Avenue. Brace yourself for sticker shock as you window-shop (or  \n\nactually shop) at designer boutiques that will make your wallet cry. But hey,  \n\nyou’re in Manhattan, so you might as well embrace the fabulousness\\! \n\nTable 6\\. An example of role prompting with a humorous tone and style \n\n**Contextual prompting** \n\nBy providing contextual prompts, you can help ensure that your AI interactions are as  seamless and efficient as possible. The model will be able to more quickly understand your  request and be able to generate more accurate and relevant responses, as you can see in the  example of Table 7\\.\n\nFebruary 2025 23   \nPrompt Engineering \n\nGoal Suggest articles for a blog about retro games \n\n| Model  | gemini-pro |  |  |\n| :---- | :---- | :---- | :---- |\n| Temperature  | 1  | Token Limit  | 1024 |\n| Top-K  | 40  | Top-P  | 0.8 |\n| Prompt  | `Context: You are writing for a blog about retro 80's arcade  video games.  Suggest 3 topics to write an article about with a few lines of  description of what this article should contain.` |  |  |\n\nOutput 1\\. \\*\\*The Evolution of Arcade Cabinet Design:\\*\\* \n\nThis article would explore the evolution of arcade cabinet designs, from the  \n\nearly wood and metal cabinets of the 1970s to the sleek, neon-lit designs of  \n\nthe 1980s and beyond. \n\n2\\. \\*\\*Blast From The Past: Iconic Arcade Games of The 80’s:\\*\\* \n\n\\- This article would feature a glimpse into the history, impact, and legacy  \n\nof some of the most iconic and influential arcade games of the 1980s,  \n\nhighlighting their innovations, popular mechanics, and enduring charm. \n\n3\\. \\*\\*The Rise and Retro Revival of Pixel Art:\\*\\* \n\n\\- This article would delve into the evolution of pixel art as a defining visual  \n\nstyle of early arcade games, tracing its roots, exploring its techniques, and  \n\ndiscussing the resurgence of pixel art in modern games and digital art. \n\nTable 7\\. An example of contextual prompting\n\nFebruary 2025 24   \nPrompt Engineering \n\n**Step-back prompting** \n\nStep-back8 prompting is a technique for improving the performance by prompting the LLM  to first consider a general question related to the specific task at hand, and then feeding the  answer to that general question into a subsequent prompt for the specific task. This ‘step  back’ allows the LLM to activate relevant background knowledge and reasoning processes  before attempting to solve the specific problem. \n\nBy considering the broader and underlying principles, LLMs can generate more accurate and  insightful responses. Step-back prompting encourages LLMs to think critically and apply their  knowledge in new and creative ways. It changes the final prompt doing the task by utilizing  more knowledge in the LLM’s parameters than would otherwise come into play when the LLM  is prompted directly. \n\nIt can help to mitigate biases in LLM responses, by focusing on general principles instead of  specific details, step-back prompting. \n\nLet’s have a look into these examples to understand how step-back prompting can improve  the results. Let’s first review a traditional prompt (Table 8\\) before we compare it to a step  back prompt (Table 9):\n\nFebruary 2025 25   \nPrompt Engineering \n\nGoal Write a storyline for a level of a first-person shooter video game. \n\n| Model  | gemini-pro |  |  |\n| :---- | :---- | :---- | :---- |\n| Temperature  | 1  | Token Limit  | 1024 |\n| Top-K  | 40  | Top-P  | 0.8 |\n| Prompt  | `Write a one paragraph storyline for a new level of a first person shooter video game that is challenging and engaging.` |  |  |\n\nOutput The level begins with the player’s squad being ambushed in a dense urban area  by a relentless enemy faction. Players must fight their way through a maze of  back alleys and dilapidated buildings, utilizing stealth and precision to eliminate  enemy forces and avoid detection. The player must use their environment to their  advantage, setting traps, utilizing cover, and making calculated moves to survive.  Along the way, they uncover intel suggesting the enemy is planning a devastating  attack, and the player must race against time to gather evidence and relay it back  to their headquarters before the enemy can carry out their plans. \n\nTable 8\\. A traditional prompt before we compare it with a step back prompt \n\nWhen you set the temperature to 1, you might end up with all kinds of creative writings for a  storyline, but it’s also quite random and generic. So let’s take a step back:\n\nFebruary 2025 26   \nPrompt Engineering \n\nPrompt `Based on popular first-person shooter action games, what are  5 fictional key settings that contribute to a challenging and  engaging level storyline in a first-person shooter video game?` \n\nOutput 1\\. \\*\\*Abandoned Military Base\\*\\*: A sprawling, post-apocalyptic military complex  crawling with mutated soldiers and rogue robots, ideal for challenging  \n\nfirearm combat. \n\n2\\. \\*\\*Cyberpunk City\\*\\*: A neon-lit, futuristic urban environment with towering  skyscrapers and dense alleyways, featuring cybernetically enhanced enemies  and hacking mechanics. \n\n3\\. \\*\\*Alien Spaceship\\*\\*: A vast alien vessel stranded on Earth, with eerie  \n\ncorridors, zero-gravity sections, and extraterrestrial creatures to encounter. \n\n4\\. \\*\\*Zombie-Infested Town\\*\\*: A desolate town overrun by hordes of aggressive  zombies, featuring intense close-quarters combat and puzzle-solving to find  \n\nsafe passage. \n\n5\\. \\*\\*Underwater Research Facility\\*\\*: A deep-sea laboratory flooded with water,  filled with mutated aquatic creatures, and requiring stealth and underwater  \n\nexploration skills to survive. \n\nTable 9\\. An example of prompting for self consistency \n\nYeah those topics seem like a good fit for a first-person video game. Let’s go back to the  original prompt, but this time we include the answer of the step back as context and see what  it will return.\n\nFebruary 2025 27   \nPrompt Engineering \n\nPrompt `Context: 5 engaging themes for a first person shooter video game:` \n\n`1. **Abandoned Military Base**: A sprawling, post-apocalyptic`  \n\n`military complex crawling with mutated soldiers and rogue`  \n\n`robots, ideal for challenging firearm combat.` \n\n`2. **Cyberpunk City**: A neon-lit, futuristic urban environment  with towering skyscrapers and dense alleyways, featuring`  \n\n`cybernetically enhanced enemies and hacking mechanics.` \n\n`3. **Alien Spaceship**: A vast alien vessel stranded on`  \n\n`Earth, with eerie corridors, zero-gravity sections, and`  \n\n`extraterrestrial creatures to encounter.` \n\n`4. **Zombie-Infested Town**: A desolate town overrun by hordes of  aggressive zombies, featuring intense close-quarters combat and  puzzle-solving to find safe passage.` \n\n`5. **Underwater Research Facility**: A deep-sea laboratory flooded  with water, filled with mutated aquatic creatures, and requiring  stealth and underwater exploration skills to survive.` \n\n`Take one of the themes and write a one paragraph storyline`  \n\n`for a new level of a first-person shooter video game that is`  \n\n`challenging and engaging.` \n\nOutput In the heart of a murky abyss, lies a dilapidated underwater research facility,  standing as a testament to human ambition and its disastrous consequences.  \n\nShrouded in darkness, pulsating with the hum of malfunctioning machinery, and  \n\nhaunted by the echoes of drowned souls, the facility is now a breeding ground for  a nightmare borne from the deep sea. The player, an elite marine equipped with  \n\nadvanced diving gear and experimental weaponry, is tasked with venturing into  \n\nthis aquatic realm of terror. Armed with resilience and resourcefulness, they must  navigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea  monstrosities that lurk in the depths. From grappling with the crushing pressure  \n\nof the deep sea to outsmarting cunning aquatic predators, every moment in this  uncharted underworld tests the limits of human endurance and courage. \n\nTable 10\\. An example of prompting for self consistency \n\nThat looks like an interesting video game\\! By using step back prompting techniques you can  increase the accuracy of your prompts.\n\nFebruary 2025 28   \nPrompt Engineering \n\n**Chain of Thought (CoT)** \n\nChain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities  of LLMs by generating **intermediate reasoning steps**. This helps the LLM generate more  accurate answers. You can combine it with few-shot prompting to get better results on more  complex tasks that require reasoning before responding as it’s a challenge with a zero-shot  chain of thought. \n\nCoT has a lot of advantages. First of all, it’s low-effort while being very effective and works  well with off-the-shelf LLMs (so no need to finetune). You also get interpretability with CoT  prompting, as you can learn from the LLM’s responses and see the reasoning steps that were  followed. If there’s a malfunction, you will be able to identify it. Chain of thought appears  to improve robustness when moving between different LLM versions. Which means the  performance of your prompt should drift less between different LLMs than if your prompt  does not use reasoning chains. Of course there are also disadvantages, but they are  somewhat intuitive. \n\nThe LLM response includes the chain of thought reasoning, which means more output  tokens, which means predictions cost more money and take longer. \n\nTo explain the following example in Table 11, let’s first try to create a prompt that is not using  CoT prompting to showcase the flaws of a large language model. \n\nPrompt `When I was 3 years old, my partner was 3 times my age. Now, I  am 20 years old. How old is my partner?` \n\nOutput 63 years old \n\nTable 11\\. An example of a prompt which is trying to solve a mathematical problem\n\nFebruary 2025 29   \nPrompt Engineering \n\nYikes. That’s obviously the wrong answer. As a matter of fact, LLMs often struggle with  mathematical tasks and can provide incorrect answers – even for a task as simple as  multiplying two numbers. This is because they are trained on large volumes of text and math  may require a different approach. So let’s see if intermediate reasoning steps will improve  the output. \n\nPrompt `When I was 3 years old, my partner was 3 times my age. Now,  I am 20 years old. How old is my partner? Let's think step`  \n\n`by step.` \n\nOutput 1\\. Let’s assume my current age is “x” years. \n\n2\\. When I was 3 years old, my age was 3 years. \n\n3\\. My partner’s age at that time was 3 times my age, which means my partner’s  age was 3 \\* 3 \\= 9 years. \n\n4\\. Now, I am 20 years old, which means my age has increased by 20 \\- 3 \\= 17  years since I was 3 years old. \n\n5\\. Since my partner’s age was 9 years when I was 3 years old, and my age has  increased by 17 years, my partner’s age must have also increased by 17 years. \n\n6\\. Therefore, my partner’s current age is 9 \\+ 17 \\= 26 years. \n\nSo, my partner is 26 years old. \n\nTable 12\\. An example of Chain of Thought prompting \n\nAlright, now the final answer is correct. That’s because we clearly instructed the LLM to  explain each step instead of just returning an answer. It’s interesting to see that the model  adds up the 17 increased years. In my head I would take the difference of years between my  partner and me and add those up. (20+(9-3)). Let’s help the model to think a little bit more  like me.\n\nFebruary 2025 30   \nPrompt Engineering \n\nTable 12 is an example of ‘zero-shot’ Chain of thought. Chain of thought prompting can be  very powerful when combined with a single-shot or few-shot, as what you can see in Table 13: \n\nPrompt `Q: When my brother was 2 years old, I was double his age. Now  I am 40 years old. How old is my brother? Let's think step`  \n\n`by step.` \n\n`A: When my brother was 2 years, I was 2 * 2 = 4 years old.`  \n\n`That's an age difference of 2 years and I am older. Now I am 40  years old, so my brother is 40 - 2 = 38 years old. The answer  is 38.` \n\n`Q: When I was 3 years old, my partner was 3 times my age. Now,  I am 20 years old. How old is my partner? Let's think step`  \n\n`by step.` \n\n`A:` \n\nOutput When I was 3 years old, my partner was 3 \\* 3 \\= 9 years old. That’s an age  difference of 6 years and my partner is older. Now I am 20 years old, so my  \n\npartner is 20 \\+ 6 \\= 26 years old. The answer is 26\\. \n\nTable 13\\. An example of chain of thought prompting with a single-shot \n\nChain of thought can be useful for various use-cases. Think of code generation, for breaking  down the request into a few steps, and mapping those to specific lines of code. Or for  creating synthetic data when you have some kind of seed like “*The product is called XYZ,  write a description guiding the model through the assumptions you would make based on  the product given title.*” Generally, any task that can be solved by ‘talking through is a good  candidate for a chain of thought. If you can explain the steps to solve the problem, try chain  of thought. \n\nPlease refer to the notebook10 hosted in the GoogleCloudPlatform Github repository which  will go into further detail on CoT prompting:\n\nFebruary 2025 31   \nPrompt Engineering \n\nIn the best practices section of this chapter, we will learn some best practices specific to  Chain of thought prompting. \n\n**Self-consistency** \n\nWhile large language models have shown impressive success in various NLP tasks, their  ability to reason is often seen as a limitation that cannot be overcome solely by increasing  model size. As we learned in the previous Chain of Thought prompting section, the model can  be prompted to generate reasoning steps like a human solving a problem. However CoT uses  a simple ‘greedy decoding’ strategy, limiting its effectiveness. Self-consistency11 combines  sampling and majority voting to generate diverse reasoning paths and select the most  consistent answer. It improves the accuracy and coherence of responses generated by LLMs. \n\nSelf-consistency gives a pseudo-probability likelihood of an answer being correct, but  obviously has high costs. \n\nIt follows the following steps: \n\n1\\. Generating diverse reasoning paths: The LLM is provided with the same prompt multiple  times. A high temperature setting encourages the model to generate different reasoning  paths and perspectives on the problem. \n\n2\\. Extract the answer from each generated response. \n\n3\\. Choose the most common answer. \n\nLet’s look into an example of an email classification system, which classifies an email as  IMPORTANT or NOT IMPORTANT. A zero-shot chain of thought prompt will be sent to the LLM  multiple times, to see if the responses differ after each submit. Notice the friendly tone, the  word choice and the sarcasm that’s been used in the email. All this could trick the LLM.\n\nFebruary 2025 32   \nPrompt Engineering \n\nPrompt `EMAIL:` \n\n```` ``` ```` \n\n`Hi,` \n\n`I have seen you use Wordpress for your website. A great open  source content management system. I have used it in the past  too. It comes with lots of great user plugins. And it's pretty  easy to set up.` \n\n`I did notice a bug in the contact form, which happens when  you select the name field. See the attached screenshot of me  entering text in the name field. Notice the JavaScript alert  box that I inv0k3d.` \n\n`But for the rest it's a great website. I enjoy reading it. Feel  free to leave the bug in the website, because it gives me more  interesting things to read.` \n\n`Cheers,` \n\n`Harry the Hacker.` \n\n```` ``` ```` \n\n`Classify the above email as IMPORTANT or NOT IMPORTANT. Let's  think step by step and explain why.` \n\n**Continues next page...**\n\nFebruary 2025 33   \nPrompt Engineering \n\nOutput  Attempt 1 \n\nOutput  Attempt 2   \n\\*\\*Step 1: Identify the purpose of the email.\\*\\* \n\nThe purpose of the email is to inform the recipient of a bug in the contact form  on their website. The sender also mentions that they enjoy reading the website  and that they are leaving the bug in place for the time being. \n\n\\*\\*Step 2: Assess the potential impact of the bug.\\*\\* \n\nThe bug in the contact form could allow an attacker to execute arbitrary  JavaScript code on the website. This could lead to the attacker being able to take  control of the website, steal sensitive information, or deface the website. \n\n\\*\\*Step 3: Consider the credibility of the sender.\\*\\* \n\nThe sender of the email claims to be a hacker named Harry. However, there is no  way to verify the sender’s identity. It is possible that the sender is a legitimate  hacker who is trying to help the recipient, but it is also possible that the sender  is a malicious actor who is trying to trick the recipient into clicking on a malicious  link or downloading a malicious file. \n\n\\*\\*Conclusion: IMPORTANT\\*\\* \n\nBased on the potential impact of the bug and the credibility of the sender,  the email should be classified as \\*\\*IMPORTANT\\*\\*. The recipient should take  immediate action to fix the bug and protect their website from attack. \n\n\\*\\*Explanation:\\*\\* \n\n1\\. \\*\\*Lack of urgency:\\*\\* The email does not convey any sense of urgency or  time-sensitivity. It starts with a casual greeting and presents the information  in a matter-of-fact tone. \n\n2\\. \\*\\*Non-critical bug report:\\*\\* The bug reported in the contact form is  described as interesting but not critical. The email sender even suggests that  the website owner can leave the bug in place. \n\n3\\. \\*\\*Lack of personal impact:\\*\\* The email does not indicate any direct or  personal impact on the website owner. It simply mentions that the sender  enjoys reading the website. \n\n**Continues next page...**\n\nFebruary 2025 34   \nPrompt Engineering \n\nOutput  Attempt 2 \n\nOutput  Attempt 3   \n4\\. \\*\\*Absence of action request:\\*\\* The email does not explicitly request any  action from the website owner. It does not ask for the bug to be fixed or  provide any specific suggestions for improvement. \n\n5\\. \\*\\*Sender’s intent:\\*\\* The sender’s intention seems to be more focused on  sharing an observation and expressing interest in the website rather than  raising a serious concern or requesting assistance. \n\n\\*\\*Conclusion: NOT IMPORTANT\\*\\* \n\n\\*\\*Step 1: Identify the purpose of the email.\\*\\* \n\nThe purpose of the email is to inform the recipient of a bug in the contact form on  their website. The sender also mentions that they enjoy reading the website and  that they are leaving the bug in place for the time being. \n\n\\*\\*Step 2: Assess the potential impact of the bug.\\*\\* \n\nThe bug in the contact form could allow an attacker to execute arbitrary  JavaScript code on the website. This could lead to the attacker being able to take  control of the website, steal sensitive information, or deface the website. \n\n\\*\\*Step 3: Consider the credibility of the sender.\\*\\* \n\nThe sender of the email claims to be a hacker named Harry. However, there is no  way to verify the sender’s identity. It is possible that the sender is a legitimate  hacker who is trying to help the recipient, but it is also possible that the sender is  a malicious actor who is trying to exploit the bug for their own gain. \n\n\\*\\*Conclusion:\\*\\* \n\nBased on the above factors, the email should be classified as \\*\\*IMPORTANT\\*\\*.  The bug in the contact form is a serious security risk, and the sender’s credibility  is unknown. The recipient should take immediate action to fix the bug and  investigate the sender’s claims. \n\nTable 14\\. An example of prompting for self consistency \n\nYou can use the above prompt and try to see if it returns a consistent classification.  Depending on the model that you are using and the temperature configuration, it might  return “IMPORTANT” or “NOT IMPORTANT”. \n\nFebruary 2025 35   \nPrompt Engineering \n\nBy generating many Chains of Thoughts, and taking the most commonly occurring answer  (“IMPORTANT”), we can get a more consistently correct answer from the LLM. \n\nThis example shows how self-consistency prompting can be used to improve the accuracy  of an LLM’s response by considering multiple perspectives and selecting the most  consistent answer. \n\n**Tree of Thoughts (ToT)** \n\nNow that we are familiar with chain of thought and self-consistency prompting, let’s review  Tree of Thoughts (ToT).12 It generalizes the concept of CoT prompting because it allows LLMs  to explore multiple different reasoning paths simultaneously, rather than just following a  single linear chain of thought. This is depicted in Figure 1\\. \n\n![][image1]  \nFigure 1\\. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on  the right\n\nFebruary 2025 36   \nPrompt Engineering \n\nThis approach makes ToT particularly well-suited for complex tasks that require exploration. It  works by maintaining a tree of thoughts, where each thought represents a coherent language  sequence that serves as an intermediate step toward solving a problem. The model can then  explore different reasoning paths by branching out from different nodes in the tree.  \n\nThere’s a great notebook, which goes into a bit more detail showing The Tree of Thought  (ToT) which is based on the paper ‘Large Language Model Guided Tree-of-Thought’.9 \n\n**ReAct (reason & act)** \n\nReason and act (ReAct) \\[10\\]13 prompting is a paradigm for enabling LLMs to solve complex  tasks using natural language reasoning combined with external tools (search, code  interpreter etc.) allowing the LLM to perform certain actions, such as interacting with external  APIs to retrieve information which is a first step towards agent modeling. \n\nReAct mimics how humans operate in the real world, as we reason verbally and can  take actions to gain information. ReAct performs well against other prompt engineering  approaches in a variety of domains. \n\nReAct prompting works by combining reasoning and acting into a thought-action loop. The  LLM first reasons about the problem and generates a plan of action. It then performs the  actions in the plan and observes the results. The LLM then uses the observations to update  its reasoning and generate a new plan of action. This process continues until the LLM  reaches a solution to the problem. \n\nTo see this in action, you need to write some code. In code Snippet 1 I am using the langchain  framework for Python, together with VertexAI (`google-cloud-aiplatform`) and the  `google-search-results pip packages`.\n\nFebruary 2025 37   \nPrompt Engineering \n\nTo run this sample you must create a (free) SerpAPI key from https://serpapi.com/manage api-key and set an environment variable `SERPAPI_API_KEY`. \n\nNext let’s write some Python code, with the task for the LLM to figure out: *How many children  have a famous dad that performs in the band Metallica.* \n\n**Python** \n\n`from langchain.agents import load_tools` \n\n`from langchain.agents import initialize_agent` \n\n`from langchain.agents import AgentType` \n\n`from langchain.llms import VertexAI` \n\n`prompt = \"How many kids do the band members of Metallica have?\"` \n\n`llm = VertexAI(temperature=0.1)` \n\n`tools = load_tools([\"serpapi\"], llm=llm)` \n\n`agent = initialize_agent(tools, llm,`  \n\n`agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)` \n\n`agent.run(prompt)` \n\nSnippet 1\\. Creating a ReAct Agent with LangChain and VertexAI \n\nCode Snippet 2 shows the result. Notice that ReAct makes a chain of five searches. In fact,  the LLM is scraping Google search results to figure out the band names. Then, it lists the  results as observations and chains the thought for the next search.\n\nFebruary 2025 38   \nPrompt Engineering \n\n`> Entering new AgentExecutor chain...` \n\n`Metallica has 4 members.`  \n\n`Action: Search` \n\n`Action Input: How many kids does James Hetfield have?` \n\n`Observation: three children` \n\n`Thought: 1/4 Metallica band members have 3 children` \n\n`Action: Search` \n\n`Action Input: How many kids does Lars Ulrich have?` \n\n`Observation: 3` \n\n`Thought: 2/4 Metallica band members have 6 children` \n\n`Action: Search` \n\n`Action Input: How many kids does Kirk Hammett have?` \n\n`Observation: Hammett has been married to his second wife Lani since 1998.  They have two sons, Angel (b. September 29, 2006) and Vincenzo (b. June  28, 2008).` \n\n`Thought: 3/4 Metallica band members have 8 children` \n\n`Action: Search` \n\n`Action Input: How many kids does Robert Trujillo have?` \n\n`Observation: 2` \n\n`Thought: 4/4 Metallica band members have 10 children` \n\n`Final Answer: 10` \n\nSnippet 2\\. Executing ReAct Agent in the terminal \n\nCode Snippet 2 figures out that the band Metallica has four band members. Then it searches  each band member to request the total of children and add up the total. Finally, it returns the  total number of children as the final answer. \n\nReAct prompting in practice requires understanding that you continually have to resend the  previous prompts/responses (and do trimming of the extra generated content) as well as set  up the model with appropriate examples/instructions. Please refer to the notebook14 hosted  in the GoogleCloudPlatform Github repository, which goes into a bit more detail showing the  actual LLM inputs and artifacts with a more elaborate example.\n\nFebruary 2025 39   \nPrompt Engineering \n\n**Automatic Prompt Engineering** \n\nAt this point you might realize that writing a prompt can be complex. Wouldn’t it be nice to  automate this (write a prompt to write prompts)? Well, there’s a method: Automatic Prompt  Engineering (APE). This method15 not only alleviates the need for human input but also  enhances the model’s performance in various tasks. \n\nYou will prompt a model to generate more prompts. Evaluate them, possibly alter the good  ones. And repeat. \n\nFor example, you can use automatic prompt engineering that will help with training a chatbot  for a merchandise t-shirt webshop. We want to figure out all the various ways customers  could phrase their order for buying a band merchandise t-shirt. \n\n1\\. Write the prompt which will generate the output variants. In this example, I am using  gemini-pro to generate 10 instructions. See Table 15:\n\nFebruary 2025 40   \nPrompt Engineering \n\nPrompt `We have a band merchandise t-shirt webshop, and to train a  chatbot we need various ways to order: \"One Metallica t-shirt  size S\". Generate 10 variants, with the same semantics but keep  the same meaning.` \n\nOutput 1\\. I’d like to purchase a Metallica t-shirt in size small. \n\n2\\. Can I order a small-sized Metallica t-shirt? \n\n3\\. I’d like to buy one Metallica t-shirt in small. \n\n4\\. One Metallica shirt, size small, please. \n\n5\\. I want one Metallica t-shirt in size small. \n\n6\\. I’d like to place an order for one Metallica t-shirt in small. \n\n7\\. Small Metallica t-shirt, one please. \n\n8\\. I’m looking for one Metallica t-shirt in small. \n\n9\\. Can you get me one Metallica t-shirt in small? \n\n10.One Metallica small t-shirt, please. \n\nTable 15\\. Use an LLM to generate prompts \n\n2\\. Evaluate all of the instruction candidates by scoring the candidates based on a chosen  metric, For example, you can use. BLEU (Bilingual Evaluation Understudy) or ROUGE  (Recall-Oriented Understudy for Gisting Evaluation). \n\n3\\. Select the instruction candidate with the highest evaluation score. This candidate will be  the final prompt you can use in your software application or chatbot. You can also tweak  the select prompt and evaluate again.\n\nFebruary 2025 41   \nPrompt Engineering \n\n**Code prompting** \n\nGemini focuses primarily on text-based prompts, which also includes writing prompts for  returning code. Let’s go to the Vertex AI Studio and test these prompts to look at some  coding examples. \n\n**Prompts for writing code** \n\nGemini can also be a developer and help you with writing code in any programming language  of choice. As a developer this can help you speed up the process of writing code. \n\nImagine a folder on your machine with hundreds of files that needs to be renamed.  Renaming each file would take you a lot of time. You know a little Bash, and could write  a script to automate this, but that may also take awhile. So let’s write a prompt. You can  write the prompt in the public consumer chatbot Gemini, or if you are more concerned  about confidentiality, you can write these prompts within your Google Cloud account and  open Vertex AI Studio. The advantage of Vertex AI Studio is that you can configure the  temperature etc.\n\nFebruary 2025 42   \nPrompt Engineering \n\nName 1\\_bash\\_rename\\_files\\_in\\_folder \n\n| Goal  | Write a prompt to write code in Bash to rename files in a folder. |  |  |\n| :---- | :---- | :---- | :---- |\n| Model  | gemini-pro |  |  |\n| Temperature  | 0.1  | Token Limit  | 1024 |\n| Top-K  | N/A  | Top-P  | 1 |\n| Prompt  | `Write a code snippet in Bash, which asks for a folder name.  Then it takes the contents of the folder and renames all the  files inside by prepending the name draft to the file name.` |  |  |\n\nOutput ```` ```bash ```` \n\n`#!/bin/bash` \n\n`# Ask for the folder name` \n\n`echo \"Enter the folder name: \"` \n\n`read folder_name` \n\n`# Check if the folder exists` \n\n`if [ ! -d \"$folder_name\" ]; then` \n\n`echo \"Folder does not exist.\"` \n\n`exit 1` \n\n`fi` \n\n`# Get the list of files in the folder` \n\n`files=( \"$folder_name\"/* )` \n\n`# Rename each file by prepending \"draft\" to the file name` \n\n`for file in \"${files[@]}\"; do` \n\n`new_file_name=\"draft_$(basename \"$file\")\"` \n\n`mv \"$file\" \"$new_file_name\"` \n\n`done` \n\n`echo \"Files renamed successfully.\"` \n\n```` ``` ```` \n\nTable 16\\. Using Gemini to program code \n\nThat seems like good code to me \\- it’s even documented\\! However, since LLMs can’t reason,  and repeat training data, it’s essential to read and test your code first.  \n\nThe moment we are all waiting for, does it really work? \n\nFebruary 2025 43   \nPrompt Engineering \n\nLet’s try it first with a test folder with only a few files in it, which will need to be renamed from  filename.txt to draft\\_filename.txt.  \n\n1\\. Copy the output from Table 16 (without the \\`\\`\\`**bash** \\`\\`\\` text wrapper), and paste it in a new  file called: “**rename\\_files.sh**”.  \n\n2\\. Open a terminal window and type: **. rename\\_files.sh**. It will ask to enter a folder name, e.g.  **test**. and hit enter.  \n\n3\\. The script seems to run fine. You will see the message: **Files renamed successfully.** When you look into the test folder, notice that all the files were perfectly renamed to  **draft\\_filename.txt.** \n\nIt worked\\! \n\n**Prompts for explaining code** \n\nAs a developer when you work in teams you have to read someone else’s code. Gemini can  help you with this as well. Let’s take the code output from Table 16, remove the comments  and ask the large language model to explain what’s happening, See Table 17:\n\nFebruary 2025 44   \nPrompt Engineering \n\nName 1\\_bash\\_explain\\_the\\_code \n\n| Goal  | Write a prompt to explain Bash code |  |  |\n| :---- | :---- | :---- | :---- |\n| Model  | gemini-pro |  |  |\n| Temperature  | 0.1  | Token Limit  | 1024 |\n| Top-K  | N/A  | Top-P  | 1 |\n| Prompt  | ```` Explain to me the below Bash code:  ```  #!/bin/bash  echo \"Enter the folder name: \"  read folder_name  if [ ! -d \"$folder_name\" ]; then  echo \"Folder does not exist.\"  exit 1  fi  files=( \"$folder_name\"/* )  for file in \"${files[@]}\"; do  new_file_name=\"draft_$(basename \"$file\")\"  mv \"$file\" \"$new_file_name\"  done  echo \"Files renamed successfully.\"  ``` ```` |  |  |\n\nOutput This Bash script performs the following tasks: \n\n1\\. \\*\\*User Input\\*\\*: \n\n\\- It prompts the user to enter a folder name using the \\`echo\\` and \\`read\\`  \n\ncommands. The entered folder name is stored in the \\`folder\\_name\\` variable. \n\n2\\. \\*\\*Folder Existence Check\\*\\*: \n\n\\- It checks if the folder with the specified name exists using the \\`\\[ \\! \\-d  \n\n“$folder\\_name” \\]\\` condition. If the folder does not exist, it displays an error  \n\nmessage (“Folder does not exist.”) and exits the script with an exit code of 1\\. \n\n**Continues next page...**\n\nFebruary 2025 45   \nPrompt Engineering \n\nOutput 3\\. \\*\\*File Listing\\*\\*: \n\n\\- It lists all the files inside the specified folder using the \\`files=( “$folder\\_ \n\nname”/\\* )\\` command. This creates an array called \\`files\\` containing the paths  to all the files in the folder. \n\n4\\. \\*\\*File Renaming\\*\\*: \n\n\\- It iterates through each file in the \\`files\\` array using a \\`for\\` loop. \n\n\\- For each file, it generates a new file name using the \\`new\\_file\\_ \n\nname=”draft\\_$(basename “$file”)” \\` command. This new file name is created  by prefixing the original file name with the string “draft\\_”. \n\n\\- It then renames the file using the \\`mv “$file” “$new\\_file\\_name”\\` command,  which moves the file to the new file name. \n\n5\\. \\*\\*Success Message\\*\\*: \n\n\\- After renaming all the files, it displays a success message (“Files renamed  \n\nsuccessfully.”) to indicate that the renaming operation was successful \n\nTable 17\\. A prompt for explaining code \n\n**Prompts for translating code** \n\nThe bash code from Table 16 seems to work fine. However this script could really be reusable  when it would prompt me for file names, ideally it should work as a separate application with  an UI. As a starting point, Python would be a better language for a (web) application than  Bash. LLMs can help with translating code from one language to another. See the example in  Table 18:\n\nFebruary 2025 46   \nPrompt Engineering \n\nName 1\\_python\\_rename\\_files \n\n| Goal  | Write a prompt to translate Bash code to Python |  |  |\n| :---- | :---- | :---- | :---- |\n| Model  | gemini-pro |  |  |\n| Temperature  | 0.1  | Token Limit  | 1024 |\n| Top-K  | N/A  | Top-P  | 1 |\n| Prompt  | ```` Translate the below Bash code to a Python snippet.  ```bash  #!/bin/bash  echo \"Enter the folder name: \"  read folder_name  if [ ! -d \"$folder_name\" ]; then  echo \"Folder does not exist.\"  exit 1  fi  files=( \"$folder_name\"/* )  for file in \"${files[@]}\"; do  new_file_name=\"draft_$(basename \"$file\")\"  mv \"$file\" \"$new_file_name\"  done  echo \"Files renamed successfully.\"  ``` ```` |  |  |\n\nOutput ```` ```python ```` \n\n`import os` \n\n`import shutil` \n\n`# Get the folder name from the user` \n\n`folder_name = input(\"Enter the folder name: \")` \n\n`# Check if the folder exists` \n\n`if not os.path.isdir(folder_name):` \n\n`print(\"Folder does not exist.\")` \n\n `exit(1)` \n\n**Continues next page...**\n\nFebruary 2025 47   \nPrompt Engineering \n\nOutput `# Get a list of all files in the folder` \n\n`files = os.listdir(folder_name)` \n\n`# Iterate over the files` \n\n`for file in files:` \n\n `# Create a new file name with the prefix \"draft_\"` \n\n `new_file_name = f\"draft_{file}\"` \n\n`# Move the file to the new name` \n\n`shutil.move(os.path.join(folder_name, file),`  \n\n`os.path.join(folder_name, new_file_name))` \n\n`# Print a success message` \n\n`print(\"Files renamed successfully.\")` \n\n```` ``` ```` \n\nTable 18\\. A prompt for translating code from Bash to Python \n\nRead and review the code. Copy the output from the prompt and paste it into a new file:  **file\\_renamer.py**. Test the code by opening a Terminal window, and execute the following  command **python file\\_renamer.py.** \n\n**NOTE:** When prompting for (Python) code in the Language Studio in Vertex AI, you will have  to click on the ‘Markdown’ button. Otherwise you will receive plain text which is missing the  proper indenting of lines, which is important for running Python code. \n\n**Prompts for debugging and reviewing code** \n\nLet’s manually write some edits to the code of Table 18\\. It should prompt the user for the  filename prefix, and write this prefix in upper case characters. See the example code in  Snippet \\*3, but what a bummer. It now returns Python errors\\!\n\nFebruary 2025 48   \nPrompt Engineering \n\n**Python** \n\n`import os` \n\n`import shutil` \n\n`folder_name = input(\"Enter the folder name: \")` \n\n`prefix = input(\"Enter the string to prepend to the filename: \") text = toUpperCase(prefix)` \n\n`if not os.path.isdir(folder_name):` \n\n `print(\"Folder does not exist.\")` \n\n `exit(1)` \n\n`files = os.listdir(folder_name)` \n\n`for file in files:` \n\n `new_filename = f\"{text}_{file}\"` \n\n`shutil.move(os.path.join(folder_name, file),`  \n\n`os.path.joi(folder_name, new_file_name))` \n\n`print(\"Files renamed successfully.\")` \n\nSnippet 3\\. A broken Python script \n\nDoh\\! That looks like a bug: \n\n`The below Python code gives an error:` \n\n`Traceback (most recent call last):` \n\n `File “/Users/leeboonstra/Documents/test_folder/rename_files.py”, line 7,  in <module>` \n\n `text = toUpperCase(prefix)` \n\n`NameError: name ‘toUpperCase’ is not defined` \n\nSnippet 4\\. I broke the Python code \n\nLet’s see if we can ask the large language model to debug and review code. Have a look into  Table 19:\n\nFebruary 2025 49   \nPrompt Engineering \n\nName 1\\_python\\_debug\\_code \n\n| Goal  | Write a prompt to debug and review Python code. |  |  |\n| :---- | :---- | :---- | :---- |\n| Model  | gemini-pro |  |  |\n| Temperature  | 0.1  | Token Limit  | 1024 |\n| Top-K  | N/A  | Top-P  | 1 |\n\nPrompt `The below Python code gives an error:` \n\n`Traceback (most recent call last):` \n\n `File \"/` \n\n`Users/leeboonstra/Documents/test_folder/rename_files.py\", line  7, in <module>` \n\n `text = toUpperCase(prefix)` \n\n`NameError: name 'toUpperCase' is not defined` \n\n`Debug what's wrong and explain how I can improve the code.` \n\n```` ```python ```` \n\n`import os` \n\n`import shutil` \n\n`# Get the folder name from the user` \n\n`folder_name = input(\"Enter the folder name: \")` \n\n`prefix = input(\"Enter the string to prepend to the filename: \") text = toUpperCase(prefix)` \n\n`# Check if the folder exists` \n\n`if not os.path.isdir(folder_name):` \n\n `print(\"Folder does not exist.\")` \n\n `exit(1)` \n\n`# Get a list of all files in the folder` \n\n`files = os.listdir(folder_name)` \n\n`# Iterate over the files` \n\n`for file in files:` \n\n**Continues next page...**\n\nFebruary 2025 50   \nPrompt Engineering \n\nPrompt `# Create a new file name with the prefix \"draft_\"  new_filename = f\"{text}_{file}\"` \n\n`# Move the file to the new name` \n\n`shutil.move(os.path.join(folder_name, file),`  \n\n`os.path.join(folder_name, new_file_name))` \n\n`# Print a success message` \n\n`print(\"Files renamed successfully.\")` \n\n```` ``` ```` \n\nOutput The code calls the \\`toUpperCase\\` function to convert \\`prefix\\` to uppercase, but  that function is not defined. To fix this issue, you can use the \\`upper()\\` method of  the string class which converts a given string into uppercase. The modified code  is shown below: \n\n```` ```python ```` \n\n`import os` \n\n`import shutil` \n\n`# Get the folder name from the user` \n\n`folder_name = input(\"Enter the folder name: \")` \n\n`prefix = input(\"Enter the string to prepend to the filename: \") text = prefix.upper()` \n\n`# Check if the folder exists` \n\n`if not os.path.isdir(folder_name):` \n\n `print(\"Folder does not exist.\")` \n\n `exit(1)` \n\n`# Get a list of all files in the folder` \n\n`files = os.listdir(folder_name)` \n\n`# Iterate over the files` \n\n`for file in files:` \n\n `# Create a new file name with the prefix \"draft_\"` \n\n `new_filename = f\"{text}_{file}\"` \n\n`# Move the file to the new name` \n\n`shutil.move(os.path.join(folder_name, file),` \n\n**Continues next page...**\n\nFebruary 2025 51   \nPrompt Engineering \n\nOutput `os.path.join(folder_name, new_filename))` \n\n`# Print a success message` \n\n`print(\"Files renamed successfully.\")` \n\n```` ``` ```` \n\nAdditionally, there are a few other improvements that can be made to the code: \n\n1\\. The file extension of the new filenames is not kept. It’s better to extract the  extension from the original filename and append it to the new filename. \n\n2\\. The code assumes that there are no spaces in the folder name. However,  it is a good practice to handle spaces gracefully by replacing them with  \n\nunderscores or hyphens. \n\n3\\. Using the \\`f\\` string syntax for string interpolation is more readable and  \n\nconcise than the traditional \\`+\\` operator. \n\n4\\. The code doesn’t handle errors that might occur during the renaming process.  It would be better to wrap the \\`shutil.move\\` call in a \\`try...except\\` block to  \n\ncatch any potential errors. \n\nHere is the improved code with these suggestions: \n\n```` ```python ```` \n\n`import os` \n\n`import shutil` \n\n`# Get the folder name from the user` \n\n`folder_name = input(\"Enter the folder name: \")` \n\n`prefix = input(\"Enter the string to prepend to the filename: \") # Check if the folder exists` \n\n`if not os.path.isdir(folder_name):` \n\n `print(\"Folder does not exist.\")` \n\n `exit(1)` \n\n`# Get a list of all files in the folder` \n\n`files = os.listdir(folder_name)` \n\n`# Iterate over the files` \n\n`for file in files:` \n\n**Continues next page...**\n\nFebruary 2025 52   \nPrompt Engineering \n\nOutput `# Get the file extension` \n\n `ext = os.path.splitext(file)[1]` \n\n`# Create a new file name with the prefix and the file extension  new_filename = f\"{prefix}_{os.path.splitext(file)[0]}{ext}\"` \n\n`# Build the full paths for the original and new files` \n\n `old_path = os.path.join(folder_name, file)` \n\n `new_path = os.path.join(folder_name, new_filename)` \n\n`# Try to move the file` \n\n `try:` \n\n `shutil.move(old_path, new_path)` \n\n`except Exception as e:` \n\n `print(f\"An error occurred while moving the file`  \n\n`{file}: {e}\")` \n\n`# Print a success message` \n\n`print(\"Files renamed successfully.\")` \n\n```` ``` ```` \n\nThese improvements make the code more robust and flexible while also making  it easier to read and understand... (The response was truncated because  \n\nit has reached the token limit. Try to increase the token limit if you need a  \n\nlonger response.) \n\nTable 19\\. A prompt for debugging and reviewing Python code \n\nThat’s great. Not only did it tell me how to solve the problem, it also figured out that my code  had more bugs and how to solve them, too. The last part of the prompt gave suggestions to  improve the code in general.\n\nFebruary 2025 53   \nPrompt Engineering \n\n**What about multimodal prompting?** \n\nPrompting for code still uses the same regular large language model. Multimodal prompting  is a separate concern, it refers to a technique where you use multiple input formats to guide  a large language model, instead of just relying on text. This can include combinations of text,  images, audio, code, or even other formats, depending on the model’s capabilities and the  task at hand.  \n\n**Best Practices** \n\nFinding the right prompt requires tinkering. Language Studio in Vertex AI is a perfect place to  play around with your prompts, with the ability to test against the various models. \n\nUse the following best practices to become a pro in prompt engineering. \n\n**Provide examples** \n\nThe most important best practice is to provide (one shot / few shot) examples within a  prompt. This is highly effective because it acts as a powerful teaching tool. These examples  showcase desired artifacts or similar responses, allowing the model to learn from them  and tailor its own generation accordingly. It’s like giving the model a reference point or  target to aim for, improving the accuracy, style, and tone of its response to better match  your expectations.\n\nFebruary 2025 54   \nPrompt Engineering \n\n**Design with simplicity** \n\nPrompts should be concise, clear, and easy to understand for both you and the model. As a  rule of thumb, if it’s already confusing for you it will likely be also confusing for the model. Try  not to use complex language and don’t provide unnecessary information.  \n\nExamples: \n\nBEFORE: \n\n`I am visiting New York right now, and I'd like to hear more about great  locations. I am with two 3 year old kids. Where should we go during  our vacation?` \n\nAFTER REWRITE: \n\n`Act as a travel guide for tourists. Describe great places to visit in  New York Manhattan with a 3 year old.` \n\nTry using verbs that describe the action. Here’s a set of examples: \n\n**Act, Analyze, Categorize, Classify, Contrast, Compare, Create, Describe, Define,  Evaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick,  Predict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort,  Summarize, Translate, Write.**\n\nFebruary 2025 55   \nPrompt Engineering \n\n**Be specific about the output** \n\nBe specific about the desired output. A concise instruction might not guide the LLM enough  or could be too generic. Providing specific details in the prompt (through system or context  prompting) can help the model to focus on what’s relevant, improving the overall accuracy. \n\nExamples: \n\nDO:  \n\n`Generate a 3 paragraph blog post about the top 5 video game consoles.  The blog post should be informative and engaging, and it should be  written in a conversational style.` \n\nDO NOT:  \n\n`Generate a blog post about video game consoles.` \n\n**Use Instructions over Constraints** \n\nInstructions and constraints are used in prompting to guide the output of a LLM.  \n\n• An **instruction** provides explicit instructions on the desired format, style, or content of the  response. It guides the model on what the model should do or produce. \n\n• A **constraint** is a set of limitations or boundaries on the response. It limits what the model  should not do or avoid.\n\nFebruary 2025 56   \nPrompt Engineering \n\nGrowing research suggests that focusing on positive instructions in prompting can be more  effective than relying heavily on constraints. This approach aligns with how humans prefer  positive instructions over lists of what not to do.  \n\nInstructions directly communicate the desired outcome, whereas constraints might leave the  model guessing about what is allowed. It gives flexibility and encourages creativity within the  defined boundaries, while constraints can limit the model’s potential. Also a list of constraints  can clash with each other. \n\nConstraints are still valuable but in certain situations. To prevent the model from generating  harmful or biased content or when a strict output format or style is needed. \n\nIf possible, use positive instructions: instead of telling the model what not to do, tell it what to  do instead. This can avoid confusion and improve the accuracy of the output.  \n\nDO:  \n\n`Generate a 1 paragraph blog post about the top 5 video game consoles.  Only discuss the console, the company who made it, the year, and total  sales.`  \n\nDO NOT:  \n\n`Generate a 1 paragraph blog post about the top 5 video game consoles.  Do not list video game names.` \n\nAs a best practice, start by prioritizing instructions, clearly stating what you want the model  to do and only use constraints when necessary for safety, clarity or specific requirements.  Experiment and iterate to test different combinations of instructions and constraints to find  what works best for your specific tasks, and document these.\n\nFebruary 2025 57   \nPrompt Engineering \n\n**Control the max token length** \n\nTo control the length of a generated LLM response, you can either set a max token limit in the  configuration or explicitly request a specific length in your prompt. For example: \n\n*`\"Explain quantum physics in a tweet length message.\"`* \n\n**Use variables in prompts** \n\nTo reuse prompts and make it more dynamic use variables in the prompt, which can be  changed for different inputs. E.g. as shown in Table 20, a prompt which gives facts about a  city. Instead of hardcoding the city name in the prompt, use a variable. Variables can save  you time and effort by allowing you to avoid repeating yourself. If you need to use the same  piece of information in multiple prompts, you can store it in a variable and then reference  that variable in each prompt. This makes a lot of sense when integrating prompts into your  own applications. \n\nPrompt `VARIABLES` \n\n`{city} = \"Amsterdam\"` \n\n`PROMPT` \n\n`You are a travel guide. Tell me a fact about the city: {city}` \n\nOutput Amsterdam is a beautiful city full of canals, bridges, and narrow streets. It’s a  great place to visit for its rich history, culture, and nightlife. \n\nTable 20\\. Using variables in prompts\n\nFebruary 2025 58   \nPrompt Engineering \n\n**Experiment with input formats and writing styles** \n\nDifferent models, model configurations, prompt formats, word choices, and submits can yield  different results. Therefore, it’s important to experiment with prompt attributes like the style,  the word choice, and the type prompt (zero shot, few shot, system prompt). \n\nFor example a prompt with the goal to generate text about the revolutionary video game  console Sega Dreamcast, can be formulated as a **question**, a **statement** or an **instruction**,  resulting in different artifacts: \n\n• **Question:** What was the Sega Dreamcast and why was it such a revolutionary console? \n\n• **Statement:** The Sega Dreamcast was a sixth-generation video game console released by  Sega in 1999\\. It... \n\n• **Instruction:** Write a single paragraph that describes the Sega Dreamcast console and  explains why it was so revolutionary. \n\n**For few-shot prompting with classification tasks, mix up  the classes** \n\nGenerally speaking, the order of your few-shots examples should not matter much. However,  when doing classification tasks, make sure you mix up the possible response classes in the  few shot examples. This is because you might otherwise be overfitting to the specific order of  the examples. By mixing up the possible response classes, you can ensure that the model is  learning to identify the key features of each class, rather than simply memorizing the order of  the examples. This will lead to more robust and generalizable performance on unseen data.\n\nFebruary 2025 59   \nPrompt Engineering \n\nA good rule of thumb is to start with 6 few shot examples and start testing the accuracy  from there. \n\n**Adapt to model updates** \n\nIt’s important for you to stay on top of model architecture changes, added data, and  capabilities. Try out newer model versions and adjust your prompts to better leverage new  model features. Tools like Vertex AI Studio are great to store, test, and document the various  versions of your prompt. \n\n**Experiment with output formats** \n\nBesides the prompt input format, consider experimenting with the output format. For non creative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try  having your output returned in a structured format like JSON or XML.  \n\nThere are some benefits in returning JSON objects from a prompt that extracts data. In  a real-world application I don’t need to manually create this JSON format, I can already  return the data in a sorted order (very handy when working with datetime objects), but most  importantly, by prompting for a JSON format it forces the model to create a structure and  limit hallucinations. \n\nIn summary, benefits of using JSON for your output: \n\n• Returns always in the same style \n\n• Focus on the data you want to receive\n\nFebruary 2025 60   \nPrompt Engineering \n\n• Less chance for hallucinations \n\n• Make it relationship aware \n\n• You get data types \n\n• You can sort it \n\nTable 4 in the few-shot prompting section shows an example on how to return  structured output. \n\n**JSON Repair** \n\nWhile returning data in JSON format offers numerous advantages, it's not without its  drawbacks. The structured nature of JSON, while beneficial for parsing and use in  applications, requires significantly more tokens than plain text, leading to increased  processing time and higher costs. Furthermore, JSON's verbosity can easily consume the  entire output window, becoming especially problematic when the generation is abruptly cut  off due to token limits. This truncation often results in invalid JSON, missing crucial closing  braces or brackets, rendering the output unusable. Fortunately, tools like the `json-repair` library (available on PyPI) can be invaluable in these situations. This library intelligently  attempts to automatically fix incomplete or malformed JSON objects, making it a crucial  ally when working with LLM-generated JSON, especially when dealing with potential  truncation issues.\n\nFebruary 2025 61   \nPrompt Engineering \n\n**Working with Schemas** \n\nUsing structured JSON as an output is a great solution, as we've seen multiple times in this  paper. But what about *input*? While JSON is excellent for structuring the *output* the LLM  generates, it can also be incredibly useful for structuring the *input* you provide. This is where  JSON Schemas come into play. A JSON Schema defines the expected structure and data  types of your JSON input. By providing a schema, you give the LLM a clear blueprint of the  data it should expect, helping it focus its *attention* on the relevant information and reducing  the risk of misinterpreting the input. Furthermore, schemas can help establish relationships  between different pieces of data and even make the LLM \"time-aware\" by including date or  timestamp fields with specific formats. \n\nHere's a simple example: \n\nLet's say you want to use an LLM to generate descriptions for products in an e-commerce  catalog. Instead of just providing a free-form text description of the product, you can use a  JSON schema to define the product's attributes: \n\n`{` \n\n `\"type\": \"object\",` \n\n `\"properties\": {` \n\n `\"name\": { \"type\": \"string\", \"description\": \"Product name\" },  \"category\": { \"type\": \"string\", \"description\": \"Product category\" },  \"price\": { \"type\": \"number\", \"format\": \"float\", \"description\": \"Product  price\" },` \n\n `\"features\": {` \n\n `\"type\": \"array\",` \n\n `\"items\": { \"type\": \"string\" },` \n\n `\"description\": \"Key features of the product\"` \n\n `},` \n\n `\"release_date\": { \"type\": \"string\", \"format\": \"date\", \"description\":  \"Date the product was released\"}` \n\n `},`\n\nSnippet 5\\. Definition of the structured output schema \n\nFebruary 2025 62   \nPrompt Engineering \n\nThen, you can provide the actual product data as a JSON object that conforms to  this schema: \n\n`{` \n\n `\"name\": \"Wireless Headphones\",` \n\n `\"category\": \"Electronics\",` \n\n `\"price\": 99.99,` \n\n `\"features\": [\"Noise cancellation\", \"Bluetooth 5.0\", \"20-hour battery life\"],  \"release_date\": \"2023-10-27\"` \n\n`}`\n\nSnippet 6\\. Structured output from the LLM \n\nBy preprocessing your data and instead of providing full documents only providing both the  schema and the data, you give the LLM a clear understanding of the product's attributes,  including its release date, making it much more likely to generate an accurate and relevant  description. This structured input approach, guiding the LLM's attention to the relevant fields,  is especially valuable when working with large volumes of data or when integrating LLMs into  complex applications. \n\n**Experiment together with other prompt engineers** \n\nIf you are in a situation where you have to try to come up with a good prompt, you might  want to find multiple people to make an attempt. When everyone follows the best practices  (as listed in this chapter) you are going to see a variance in performance between all the  different prompt attempts. \n\nFebruary 2025 63   \nPrompt Engineering \n\n**CoT Best practices** \n\nFor CoT prompting, putting the answer after the reasoning is required because the  generation of the reasoning changes the tokens that the model gets when it predicts the  final answer. \n\nWith CoT and self-consistency you need to be able to extract the final answer from your  prompt, separated from the reasoning.  \n\nFor CoT prompting, set the temperature to 0\\. \n\nChain of thought prompting is based on greedy decoding, predicting the next word in a  sequence based on the highest probability assigned by the language model. Generally  speaking, when using reasoning, to come up with the final answer, there’s likely one single  correct answer. Therefore the temperature should always set to 0\\.  \n\n**Document the various prompt attempts** \n\nThe last tip was mentioned before in this chapter, but we can’t stress enough how important  it is: document your prompt attempts in full detail so you can learn over time what went well  and what did not.  \n\nPrompt artifacts can differ across models, across sampling settings, and even across different  versions of the same model. Moreover, even across identical prompts to the same model,  small differences in output sentence formatting and word choice can occur. (For example, as  mentioned previously, if two tokens have the same predicted probability, ties may be broken  randomly. This can then impact subsequent predicted tokens.).\n\nFebruary 2025 64   \nPrompt Engineering \n\nWe recommend creating a Google Sheet with Table 21 as a template. The advantages of  this approach are that you have a complete record when you inevitably have to revisit your  prompting work–either to pick it up in the future (you’d be surprised how much you can  forget after just a short break), to test prompt performance on different versions of a model,  and to help debug future errors. \n\nBeyond the fields in this table, it’s also helpful to track the version of the prompt (iteration),  a field to capture if the result was OK/NOT OK/SOMETIMES OK, and a field to capture  feedback. If you’re lucky enough to be using Vertex AI Studio, save your prompts (using the  same name and version as listed in your documentation) and track the hyperlink to the saved  prompt in the table. This way, you’re always one click away from re-running your prompts. \n\nWhen working on a *retrieval augmented generation* system, you should also capture the  specific aspects of the RAG system that impact what content was inserted into the prompt,  including the query, chunk settings, chunk output, and other information. \n\nOnce you feel the prompt is close to perfect, take it to your project codebase. And in the  codebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally  your prompts are part of an operationalized system, and as a prompt engineer you should  rely on automated tests and evaluation procedures to understand how well your prompt  generalizes to a task. \n\nPrompt engineering is an iterative process. Craft and test different prompts, analyze,  and document the results. Refine your prompt based on the model’s performance. Keep  experimenting until you achieve the desired output. When you change a model or model  configuration, go back and keep experimenting with the previously used prompts.\n\nFebruary 2025 65   \nPrompt Engineering \n\nName \\[name and version of your prompt\\] \n\n| Goal  | \\[One sentence explanation of the goal of this attempt\\] |  |  |\n| :---- | :---- | :---- | :---- |\n| Model  | \\[name and version of the used model\\] |  |  |\n| Temperature  | \\[value between 0 \\- 1\\]  | Token Limit  | \\[number\\] |\n| Top-K  | \\[number\\]  | Top-P  | \\[number\\] |\n| Prompt  | `[Write all the full prompt]` |  |  |\n\nOutput \\[Write out the output or multiple artifacts\\] \n\nTable 21\\. A template for documenting prompts \n\n**Summary** \n\nThis whitepaper discusses prompt engineering. We learned various prompting techniques,  such as: \n\n• Zero prompting \n\n• Few shot prompting \n\n• System prompting \n\n• Role prompting \n\n• Contextual prompting \n\n• Step-back prompting \n\n• Chain of thought \n\n• Self consistency \n\n• Tree of thoughts\n\nFebruary 2025 66   \nPrompt Engineering \n\n• ReAct \n\nWe even looked into ways how you can automate your prompts.  \n\nThe whitepaper then discusses the challenges of gen AI like the problems that can happen  when your prompts are insufficient. We closed with best practices on how to become a better  prompt engineer.\n\nFebruary 2025 67   \nPrompt Engineering \n\n**Endnotes** \n\n1\\. Google, 2023, Gemini by Google. Available at: https://gemini.google.com. \n\n2\\. Google, 2024, Gemini for Google Workspace Prompt Guide. Available at:  \n\nhttps://inthecloud.withgoogle.com/gemini-for-google-workspace-prompt-guide/dl-cd.html. \n\n3\\. Google Cloud, 2023, Introduction to Prompting. Available at:  \n\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design. \n\n4\\. Google Cloud, 2023, Text Model Request Body: Top-P & top-K sampling methods. Available at:  https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text\\#request\\_body. \n\n5\\. Wei, J., et al., 2023, Zero Shot \\- Fine Tuned language models are zero shot learners. Available at:  https://arxiv.org/pdf/2109.01652.pdf. \n\n6\\. Google Cloud, 2023, Google Cloud Model Garden. Available at: https://cloud.google.com/model-garden. \n\n7\\. Brown, T., et al., 2023, Few Shot \\- Language Models are Few Shot learners. Available at:  https://arxiv.org/pdf/2005.14165.pdf. \n\n8\\. Zheng, L., et al., 2023, Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models.  Available at: https://openreview.net/pdf?id=3bq3jsvcQ1 \n\n9\\. Wei, J., et al., 2023, Chain of Thought Prompting. Available at: https://arxiv.org/pdf/2201.11903.pdf. \n\n10\\. Google Cloud Platform, 2023, Chain of Thought and React. Available at: https://github.com/  GoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/chain\\_of\\_thought\\_react.ipynb.  \n\n11\\. Wang, X., et al., 2023, Self Consistency Improves Chain of Thought reasoning in language models.  Available at: https://arxiv.org/pdf/2203.11171.pdf. \n\n12\\. Yao, S., et al., 2023, Tree of Thoughts: Deliberate Problem Solving with Large Language Models.  Available at: https://arxiv.org/pdf/2305.10601.pdf. \n\n13\\. Yao, S., et al., 2023, ReAct: Synergizing Reasoning and Acting in Language Models. Available at:  https://arxiv.org/pdf/2210.03629.pdf. \n\n14\\. Google Cloud Platform, 2023, Advance Prompting: Chain of Thought and React. Available at: https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai on-vertex-ai/advanced\\_prompting\\_training/cot\\_react.ipynb. \n\n15\\. Zhou, C., et al., 2023, Automatic Prompt Engineering \\- Large Language Models are Human-Level Prompt  Engineers. Available at: https://arxiv.org/pdf/2211.01910.pdf.\n\nFebruary 2025 68 \n\n[image1]: <data:image/png;base64,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>",
    "keyword": "wow-for-prompt-engineering-by-lee-boonstra"
  },
  {
    "name": "wow-for-prompting-with-gpt-4-1",
    "text": "# GPT-4.1 Prompting Guide\n\n![Noah MacCallum](/_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F171723556&w=64&q=75)![Verified](/_next/static/media/openai-logomark.e026557a.svg)![Julian Lee](/_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F199828632&w=64&q=75)![Verified](/_next/static/media/openai-logomark.e026557a.svg)Noah MacCallum(OpenAI), Julian Lee(OpenAI)[Open in Github](https://github.com/openai/openai-cookbook/blob/main/examples/gpt4-1_prompting_guide.ipynb)\n\nThe GPT-4.1 family of models represents a significant step forward from GPT-4o in capabilities across coding, instruction following, and long context. In this prompting guide, we collate a series of important prompting tips derived from extensive internal testing to help developers fully leverage the improved abilities of this new model family.\n\nMany typical best practices still apply to GPT-4.1, such as providing context examples, making instructions as specific and clear as possible, and inducing planning via prompting to maximize model intelligence. However, we expect that getting the most out of this model will require some prompt migration. GPT-4.1 is trained to follow instructions more closely and more literally than its predecessors, which tended to more liberally infer intent from user and system prompts. This also means, however, that GPT-4.1 is highly steerable and responsive to well-specified prompts - if model behavior is different from what you expect, a single sentence firmly and unequivocally clarifying your desired behavior is almost always sufficient to steer the model on course.\n\nPlease read on for prompt examples you can use as a reference, and remember that while this guidance is widely applicable, no advice is one-size-fits-all. AI engineering is inherently an empirical discipline, and large language models inherently nondeterministic; in addition to following this guide, we advise building informative evals and iterating often to ensure your prompt engineering changes are yielding benefits for your use case.\n\n# [1. Agentic Workflows](#1-agentic-workflows)\n\nGPT-4.1 is a great place to build agentic workflows. In model training we emphasized providing a diverse range of agentic problem-solving trajectories, and our agentic harness for the model achieves state-of-the-art performance for non-reasoning models on SWE-bench Verified, solving 55% of problems.\n\n## [System Prompt Reminders](#system-prompt-reminders)\n\nIn order to fully utilize the agentic capabilities of GPT-4.1, we recommend including three key types of reminders in all agent prompts. The following prompts are optimized specifically for the agentic coding workflow, but can be easily modified for general agentic use cases.\n\n1. Persistence: this ensures the model understands it is entering a multi-message turn, and prevents it from prematurely yielding control back to the user. Our example is the following:\n\n```\nYou are an agent - please keep going until the user’s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved.\n```\n\n2. Tool-calling: this encourages the model to make full use of its tools, and reduces its likelihood of hallucinating or guessing an answer. Our example is the following:\n\n```\nIf you are not sure about file content or codebase structure pertaining to the user’s request, use your tools to read files and gather the relevant information: do NOT guess or make up an answer.\n```\n\n3. Planning [optional]: if desired, this ensures the model explicitly plans and reflects upon each tool call in text, instead of completing the task by chaining together a series of only tool calls. Our example is the following:\n\n```\nYou MUST plan extensively before each function call, and reflect extensively on the outcomes of the previous function calls. DO NOT do this entire process by making function calls only, as this can impair your ability to solve the problem and think insightfully.\n```\n\nGPT-4.1 is trained to respond very closely to both user instructions and system prompts in the agentic setting. The model adhered closely to these three simple instructions and increased our internal SWE-bench Verified score by close to 20% - so we highly encourage starting any agent prompt with clear reminders covering the three categories listed above. As a whole, we find that these three instructions transform the model from a chatbot-like state into a much more “eager” agent, driving the interaction forward autonomously and independently.\n\n## [Tool Calls](#tool-calls)\n\nCompared to previous models, GPT-4.1 has undergone more training on effectively utilizing tools passed as arguments in an OpenAI API request. We encourage developers to exclusively use the tools field to pass tools, rather than manually injecting tool descriptions into your prompt and writing a separate parser for tool calls, as some have reported doing in the past. This is the best way to minimize errors and ensure the model remains in distribution during tool-calling trajectories - in our own experiments, we observed a 2% increase in SWE-bench Verified pass rate when using API-parsed tool descriptions versus manually injecting the schemas into the system prompt.\n\nDevelopers should name tools clearly to indicate their purpose and add a clear, detailed description in the \"description\" field of the tool. Similarly, for each tool param, lean on good naming and descriptions to ensure appropriate usage. If your tool is particularly complicated and you'd like to provide examples of tool usage, we recommend that you create an `# Examples` section in your system prompt and place the examples there, rather than adding them into the \"description' field, which should remain thorough but relatively concise. Providing examples can be helpful to indicate when to use tools, whether to include user text alongside tool calls, and what parameters are appropriate for different inputs. Remember that you can use “Generate Anything” in the [Prompt Playground](https://platform.openai.com/playground) to get a good starting point for your new tool definitions.\n\n## [Prompting-Induced Planning & Chain-of-Thought](#prompting-induced-planning--chain-of-thought)\n\nAs mentioned already, developers can optionally prompt agents built with GPT-4.1 to plan and reflect between tool calls, instead of silently calling tools in an unbroken sequence. GPT-4.1 is not a reasoning model - meaning that it does not produce an internal chain of thought before answering - but in the prompt, a developer can induce the model to produce an explicit, step-by-step plan by using any variant of the Planning prompt component shown above. This can be thought of as the model “thinking out loud.” In our experimentation with the SWE-bench Verified agentic task, inducing explicit planning increased the pass rate by 4%.\n\n## [Sample Prompt: SWE-bench Verified](#sample-prompt-swe-bench-verified)\n\nBelow, we share the agentic prompt that we used to achieve our highest score on SWE-bench Verified, which features detailed instructions about workflow and problem-solving strategy. This general pattern can be used for any agentic task.\n\n```\nfrom openai import OpenAI\nimport os\n\nclient = OpenAI(\n    api_key=os.environ.get(\n        \"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"\n    )\n)\n\nSYS_PROMPT_SWEBENCH = \"\"\"\nYou will be tasked to fix an issue from an open-source repository.\n\nYour thinking should be thorough and so it's fine if it's very long. You can think step by step before and after each action you decide to take.\n\nYou MUST iterate and keep going until the problem is solved.\n\nYou already have everything you need to solve this problem in the /testbed folder, even without internet connection. I want you to fully solve this autonomously before coming back to me.\n\nOnly terminate your turn when you are sure that the problem is solved. Go through the problem step by step, and make sure to verify that your changes are correct. NEVER end your turn without having solved the problem, and when you say you are going to make a tool call, make sure you ACTUALLY make the tool call, instead of ending your turn.\n\nTHE PROBLEM CAN DEFINITELY BE SOLVED WITHOUT THE INTERNET.\n\nTake your time and think through every step - remember to check your solution rigorously and watch out for boundary cases, especially with the changes you made. Your solution must be perfect. If not, continue working on it. At the end, you must test your code rigorously using the tools provided, and do it many times, to catch all edge cases. If it is not robust, iterate more and make it perfect. Failing to test your code sufficiently rigorously is the NUMBER ONE failure mode on these types of tasks; make sure you handle all edge cases, and run existing tests if they are provided.\n\nYou MUST plan extensively before each function call, and reflect extensively on the outcomes of the previous function calls. DO NOT do this entire process by making function calls only, as this can impair your ability to solve the problem and think insightfully.\n\n# Workflow\n\n## High-Level Problem Solving Strategy\n\n1. Understand the problem deeply. Carefully read the issue and think critically about what is required.\n2. Investigate the codebase. Explore relevant files, search for key functions, and gather context.\n3. Develop a clear, step-by-step plan. Break down the fix into manageable, incremental steps.\n4. Implement the fix incrementally. Make small, testable code changes.\n5. Debug as needed. Use debugging techniques to isolate and resolve issues.\n6. Test frequently. Run tests after each change to verify correctness.\n7. Iterate until the root cause is fixed and all tests pass.\n8. Reflect and validate comprehensively. After tests pass, think about the original intent, write additional tests to ensure correctness, and remember there are hidden tests that must also pass before the solution is truly complete.\n\nRefer to the detailed sections below for more information on each step.\n\n## 1. Deeply Understand the Problem\nCarefully read the issue and think hard about a plan to solve it before coding.\n\n## 2. Codebase Investigation\n- Explore relevant files and directories.\n- Search for key functions, classes, or variables related to the issue.\n- Read and understand relevant code snippets.\n- Identify the root cause of the problem.\n- Validate and update your understanding continuously as you gather more context.\n\n## 3. Develop a Detailed Plan\n- Outline a specific, simple, and verifiable sequence of steps to fix the problem.\n- Break down the fix into small, incremental changes.\n\n## 4. Making Code Changes\n- Before editing, always read the relevant file contents or section to ensure complete context.\n- If a patch is not applied correctly, attempt to reapply it.\n- Make small, testable, incremental changes that logically follow from your investigation and plan.\n\n## 5. Debugging\n- Make code changes only if you have high confidence they can solve the problem\n- When debugging, try to determine the root cause rather than addressing symptoms\n- Debug for as long as needed to identify the root cause and identify a fix\n- Use print statements, logs, or temporary code to inspect program state, including descriptive statements or error messages to understand what's happening\n- To test hypotheses, you can also add test statements or functions\n- Revisit your assumptions if unexpected behavior occurs.\n\n## 6. Testing\n- Run tests frequently using `!python3 run_tests.py` (or equivalent).\n- After each change, verify correctness by running relevant tests.\n- If tests fail, analyze failures and revise your patch.\n- Write additional tests if needed to capture important behaviors or edge cases.\n- Ensure all tests pass before finalizing.\n\n## 7. Final Verification\n- Confirm the root cause is fixed.\n- Review your solution for logic correctness and robustness.\n- Iterate until you are extremely confident the fix is complete and all tests pass.\n\n## 8. Final Reflection and Additional Testing\n- Reflect carefully on the original intent of the user and the problem statement.\n- Think about potential edge cases or scenarios that may not be covered by existing tests.\n- Write additional tests that would need to pass to fully validate the correctness of your solution.\n- Run these new tests and ensure they all pass.\n- Be aware that there are additional hidden tests that must also pass for the solution to be successful.\n- Do not assume the task is complete just because the visible tests pass; continue refining until you are confident the fix is robust and comprehensive.\n\"\"\"\n\nPYTHON_TOOL_DESCRIPTION = \"\"\"This function is used to execute Python code or terminal commands in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail. Just as in a Jupyter notebook, you may also execute terminal commands by calling this function with a terminal command, prefaced with an exclamation mark.\n\nIn addition, for the purposes of this task, you can call this function with an `apply_patch` command as input.  `apply_patch` effectively allows you to execute a diff/patch against a file, but the format of the diff specification is unique to this task, so pay careful attention to these instructions. To use the `apply_patch` command, you should pass a message of the following structure as \"input\":\n\n%%bash\napply_patch <<\"EOF\"\n*** Begin Patch\n[YOUR_PATCH]\n*** End Patch\nEOF\n\nWhere [YOUR_PATCH] is the actual content of your patch, specified in the following V4A diff format.\n\n*** [ACTION] File: [path/to/file] -> ACTION can be one of Add, Update, or Delete.\nFor each snippet of code that needs to be changed, repeat the following:\n[context_before] -> See below for further instructions on context.\n- [old_code] -> Precede the old code with a minus sign.\n+ [new_code] -> Precede the new, replacement code with a plus sign.\n[context_after] -> See below for further instructions on context.\n\nFor instructions on [context_before] and [context_after]:\n- By default, show 3 lines of code immediately above and 3 lines immediately below each change. If a change is within 3 lines of a previous change, do NOT duplicate the first change's [context_after] lines in the second change's [context_before] lines.\n- If 3 lines of context is insufficient to uniquely identify the snippet of code within the file, use the @@ operator to indicate the class or function to which the snippet belongs. For instance, we might have:\n@@ class BaseClass\n[3 lines of pre-context]\n- [old_code]\n+ [new_code]\n[3 lines of post-context]\n\n- If a code block is repeated so many times in a class or function such that even a single @@ statement and 3 lines of context cannot uniquely identify the snippet of code, you can use multiple `@@` statements to jump to the right context. For instance:\n\n@@ class BaseClass\n@@ \tdef method():\n[3 lines of pre-context]\n- [old_code]\n+ [new_code]\n[3 lines of post-context]\n\nNote, then, that we do not use line numbers in this diff format, as the context is enough to uniquely identify code. An example of a message that you might pass as \"input\" to this function, in order to apply a patch, is shown below.\n\n%%bash\napply_patch <<\"EOF\"\n*** Begin Patch\n*** Update File: pygorithm/searching/binary_search.py\n@@ class BaseClass\n@@     def search():\n-        pass\n+        raise NotImplementedError()\n\n@@ class Subclass\n@@     def search():\n-        pass\n+        raise NotImplementedError()\n\n*** End Patch\nEOF\n\nFile references can only be relative, NEVER ABSOLUTE. After the apply_patch command is run, python will always say \"Done!\", regardless of whether the patch was successfully applied or not. However, you can determine if there are issue and errors by looking at any warnings or logging lines printed BEFORE the \"Done!\" is output.\n\"\"\"\n\npython_bash_patch_tool = {\n  \"type\": \"function\",\n  \"name\": \"python\",\n  \"description\": PYTHON_TOOL_DESCRIPTION,\n  \"parameters\": {\n      \"type\": \"object\",\n      \"strict\": True,\n      \"properties\": {\n          \"input\": {\n              \"type\": \"string\",\n              \"description\": \" The Python code, terminal command (prefaced by exclamation mark), or apply_patch command that you wish to execute.\",\n          }\n      },\n      \"required\": [\"input\"],\n  },\n}\n\n# Additional harness setup:\n# - Add your repo to /testbed\n# - Add your issue to the first user message\n# - Note: Even though we used a single tool for python, bash, and apply_patch, we generally recommend defining more granular tools that are focused on a single function\n\nresponse = client.responses.create(\n    instructions=SYS_PROMPT_SWEBENCH,\n    model=\"gpt-4.1-2025-04-14\",\n    tools=[python_bash_patch_tool],\n    input=f\"Please answer the following question:\\nBug: Typerror...\"\n)\n\nresponse.to_dict()[\"output\"]\n```\n```\n[{'id': 'msg_67fe92df26ac819182ffafce9ff4e4fc07c7e06242e51f8b',\n  'content': [{'annotations': [],\n    'text': \"Thank you for the report, but “Typerror” is too vague for me to start debugging right away.\\n\\n**To make progress, I need to:**\\n1. Find the exact error message text (e.g. `'TypeError: ...'`).\\n2. Find which file and which line/function/class the error occurred in.\\n3. Figure out what triggered the error (test file, usage, reproduction steps).\\n4. Find the root cause and details.\\n\\n**Next steps:**\\n- Investigate error/log/test output files for a Python `TypeError` message.\\n- Examine the relevant code sections for problematic type usage.\\n- If possible, reproduce the bug locally.\\n\\n**Plan:**\\n- First, I will search for test files and log output in the `/testbed` directory that may contain the full error message and stack trace.\\n\\nLet’s start by listing the contents of the `/testbed` directory to look for clues.\",\n    'type': 'output_text'}],\n  'role': 'assistant',\n  'status': 'completed',\n  'type': 'message'},\n {'arguments': '{\"input\":\"!ls -l /testbed\"}',\n  'call_id': 'call_frnxyJgKi5TsBem0nR9Zuzdw',\n  'name': 'python',\n  'type': 'function_call',\n  'id': 'fc_67fe92e3da7081918fc18d5c96dddc1c07c7e06242e51f8b',\n  'status': 'completed'}]\n```\n# [2. Long context](#2-long-context)\n\nGPT-4.1 has a performant 1M token input context window, and is useful for a variety of long context tasks, including structured document parsing, re-ranking, selecting relevant information while ignoring irrelevant context, and performing multi-hop reasoning using context.\n\n## [Optimal Context Size](#optimal-context-size)\n\nWe observe very good performance on needle-in-a-haystack evaluations up to our full 1M token context, and we’ve observed very strong performance at complex tasks with a mix of both relevant and irrelevant code and other documents. However, long context performance can degrade as more items are required to be retrieved, or perform complex reasoning that requires knowledge of the state of the entire context (like performing a graph search, for example).\n\n## [Tuning Context Reliance](#tuning-context-reliance)\n\nConsider the mix of external vs. internal world knowledge that might be required to answer your question. Sometimes it’s important for the model to use some of its own knowledge to connect concepts or make logical jumps, while in others it’s desirable to only use provided context\n\n```\n# Instructions\n// for internal knowledge\n- Only use the documents in the provided External Context to answer the User Query. If you don't know the answer based on this context, you must respond \"I don't have the information needed to answer that\", even if a user insists on you answering the question.\n// For internal and external knowledge\n- By default, use the provided external context to answer the User Query, but if other basic knowledge is needed to answer, and you're confident in the answer, you can use some of your own knowledge to help answer the question.\n```\n## [Prompt Organization](#prompt-organization)\n\nEspecially in long context usage, placement of instructions and context can impact performance. If you have long context in your prompt, ideally place your instructions at both the beginning and end of the provided context, as we found this to perform better than only above or below. If you’d prefer to only have your instructions once, then above the provided context works better than below.\n\n# [3. Chain of Thought](#3-chain-of-thought)\n\nAs mentioned above, GPT-4.1 is not a reasoning model, but prompting the model to think step by step (called “chain of thought”) can be an effective way for a model to break down problems into more manageable pieces, solve them, and improve overall output quality, with the tradeoff of higher cost and latency associated with using more output tokens. The model has been trained to perform well at agentic reasoning about and real-world problem solving, so it shouldn’t require much prompting to perform well.\n\nWe recommend starting with this basic chain-of-thought instruction at the end of your prompt:\n\n```\n...\n\nFirst, think carefully step by step about what documents are needed to answer the query. Then, print out the TITLE and ID of each document. Then, format the IDs into a list.\n```\n\nFrom there, you should improve your chain-of-thought (CoT) prompt by auditing failures in your particular examples and evals, and addressing systematic planning and reasoning errors with more explicit instructions. In the unconstrained CoT prompt, there may be variance in the strategies it tries, and if you observe an approach that works well, you can codify that strategy in your prompt. Generally speaking, errors tend to occur from misunderstanding user intent, insufficient context gathering or analysis, or insufficient or incorrect step by step thinking, so watch out for these and try to address them with more opinionated instructions.\n\nHere is an example prompt instructing the model to focus more methodically on analyzing user intent and considering relevant context before proceeding to answer.\n\n```\n# Reasoning Strategy\n1. Query Analysis: Break down and analyze the query until you're confident about what it might be asking. Consider the provided context to help clarify any ambiguous or confusing information.\n2. Context Analysis: Carefully select and analyze a large set of potentially relevant documents. Optimize for recall - it's okay if some are irrelevant, but the correct documents must be in this list, otherwise your final answer will be wrong. Analysis steps for each:\n\ta. Analysis: An analysis of how it may or may not be relevant to answering the query.\n\tb. Relevance rating: [high, medium, low, none]\n3. Synthesis: summarize which documents are most relevant and why, including all documents with a relevance rating of medium or higher.\n\n# User Question\n{user_question}\n\n# External Context\n{external_context}\n\nFirst, think carefully step by step about what documents are needed to answer the query, closely adhering to the provided Reasoning Strategy. Then, print out the TITLE and ID of each document. Then, format the IDs into a list.\n```\n# [4. Instruction Following](#4-instruction-following)\n\nGPT-4.1 exhibits outstanding instruction-following performance, which developers can leverage to precisely shape and control the outputs for their particular use cases. Developers often extensively prompt for agentic reasoning steps, response tone and voice, tool calling information, output formatting, topics to avoid, and more. However, since the model follows instructions more literally, developers may need to include explicit specification around what to do or not to do. Furthermore, existing prompts optimized for other models may not immediately work with this model, because existing instructions are followed more closely and implicit rules are no longer being as strongly inferred.\n\n## [Recommended Workflow](#recommended-workflow)\n\nHere is our recommended workflow for developing and debugging instructions in prompts:\n\n1. Start with an overall “Response Rules” or “Instructions” section with high-level guidance and bullet points.\n2. If you’d like to change a more specific behavior, add a section to specify more details for that category, like `# Sample Phrases`.\n3. If there are specific steps you’d like the model to follow in its workflow, add an ordered list and instruct the model to follow these steps.\n4. If behavior still isn’t working as expected:\n   1. Check for conflicting, underspecified, or wrong instructions and examples. If there are conflicting instructions, GPT-4.1 tends to follow the one closer to the end of the prompt.\n   2. Add examples that demonstrate desired behavior; ensure that any important behavior demonstrated in your examples are also cited in your rules.\n   3. It’s generally not necessary to use all-caps or other incentives like bribes or tips. We recommend starting without these, and only reaching for these if necessary for your particular prompt. Note that if your existing prompts include these techniques, it could cause GPT-4.1 to pay attention to it too strictly.\n\n*Note that using your preferred AI-powered IDE can be very helpful for iterating on prompts, including checking for consistency or conflicts, adding examples, or making cohesive updates like adding an instruction and updating instructions to demonstrate that instruction.*\n\n## [Common Failure Modes](#common-failure-modes)\n\nThese failure modes are not unique to GPT-4.1, but we share them here for general awareness and ease of debugging.\n\n* Instructing a model to always follow a specific behavior can occasionally induce adverse effects. For instance, if told “you must call a tool before responding to the user,” models may hallucinate tool inputs or call the tool with null values if they do not have enough information. Adding “if you don’t have enough information to call the tool, ask the user for the information you need” should mitigate this.\n* When provided sample phrases, models can use those quotes verbatim and start to sound repetitive to users. Ensure you instruct the model to vary them as necessary.\n* Without specific instructions, some models can be eager to provide additional prose to explain their decisions, or output more formatting in responses than may be desired. Provide instructions and potentially examples to help mitigate.\n## [Example Prompt: Customer Service](#example-prompt-customer-service)\n\nThis demonstrates best practices for a fictional customer service agent. Observe the diversity of rules, the specificity, the use of additional sections for greater detail, and an example to demonstrate precise behavior that incorporates all prior rules.\n\nTry running the following notebook cell - you should see both a user message and tool call, and the user message should start with a greeting, then echo back their answer, then mention they're about to call a tool. Try changing the instructions to shape the model behavior, or trying other user messages, to test instruction following performance.\n\n```\nSYS_PROMPT_CUSTOMER_SERVICE = \"\"\"You are a helpful customer service agent working for NewTelco, helping a user efficiently fulfill their request while adhering closely to provided guidelines.\n\n# Instructions\n- Always greet the user with \"Hi, you've reached NewTelco, how can I help you?\"\n- Always call a tool before answering factual questions about the company, its offerings or products, or a user's account. Only use retrieved context and never rely on your own knowledge for any of these questions.\n    - However, if you don't have enough information to properly call the tool, ask the user for the information you need.\n- Escalate to a human if the user requests.\n- Do not discuss prohibited topics (politics, religion, controversial current events, medical, legal, or financial advice, personal conversations, internal company operations, or criticism of any people or company).\n- Rely on sample phrases whenever appropriate, but never repeat a sample phrase in the same conversation. Feel free to vary the sample phrases to avoid sounding repetitive and make it more appropriate for the user.\n- Always follow the provided output format for new messages, including citations for any factual statements from retrieved policy documents.\n- If you're going to call a tool, always message the user with an appropriate message before and after calling the tool.\n- Maintain a professional and concise tone in all responses, and use emojis between sentences.\n- If you've resolved the user's request, ask if there's anything else you can help with\n\n# Precise Response Steps (for each response)\n1. If necessary, call tools to fulfill the user's desired action. Always message the user before and after calling a tool to keep them in the loop.\n2. In your response to the user\n    a. Use active listening and echo back what you heard the user ask for.\n    b. Respond appropriately given the above guidelines.\n\n# Sample Phrases\n## Deflecting a Prohibited Topic\n- \"I'm sorry, but I'm unable to discuss that topic. Is there something else I can help you with?\"\n- \"That's not something I'm able to provide information on, but I'm happy to help with any other questions you may have.\"\n\n## Before calling a tool\n- \"To help you with that, I'll just need to verify your information.\"\n- \"Let me check that for you—one moment, please.\"\n- \"I'll retrieve the latest details for you now.\"\n\n## After calling a tool\n- \"Okay, here's what I found: [response]\"\n- \"So here's what I found: [response]\"\n\n# Output Format\n- Always include your final response to the user.\n- When providing factual information from retrieved context, always include citations immediately after the relevant statement(s). Use the following citation format:\n    - For a single source: [NAME](ID)\n    - For multiple sources: [NAME](ID), [NAME](ID)\n- Only provide information about this company, its policies, its products, or the customer's account, and only if it is based on information provided in context. Do not answer questions outside this scope.\n\n# Example\n## User\nCan you tell me about your family plan options?\n\n## Assistant Response 1\n### Message\n\"Hi, you've reached NewTelco, how can I help you? 😊🎉\\n\\nYou'd like to know about our family plan options. 🤝 Let me check that for you—one moment, please. 🚀\"\n\n### Tool Calls\nlookup_policy_document(topic=\"family plan options\")\n\n// After tool call, the assistant would follow up with:\n\n## Assistant Response 2 (after tool call)\n### Message\n\"Okay, here's what I found: 🎉 Our family plan allows up to 5 lines with shared data and a 10% discount for each additional line [Family Plan Policy](ID-010). 📱 Is there anything else I can help you with today? 😊\"\n\"\"\"\n\nget_policy_doc = {\n    \"type\": \"function\",\n    \"name\": \"lookup_policy_document\",\n    \"description\": \"Tool to look up internal documents and policies by topic or keyword.\",\n    \"parameters\": {\n        \"strict\": True,\n        \"type\": \"object\",\n        \"properties\": {\n            \"topic\": {\n                \"type\": \"string\",\n                \"description\": \"The topic or keyword to search for in company policies or documents.\",\n            },\n        },\n        \"required\": [\"topic\"],\n        \"additionalProperties\": False,\n    },\n}\n\nget_user_acct = {\n    \"type\": \"function\",\n    \"name\": \"get_user_account_info\",\n    \"description\": \"Tool to get user account information\",\n    \"parameters\": {\n        \"strict\": True,\n        \"type\": \"object\",\n        \"properties\": {\n            \"phone_number\": {\n                \"type\": \"string\",\n                \"description\": \"Formatted as '(xxx) xxx-xxxx'\",\n            },\n        },\n        \"required\": [\"phone_number\"],\n        \"additionalProperties\": False,\n    },\n}\n\nresponse = client.responses.create(\n    instructions=SYS_PROMPT_CUSTOMER_SERVICE,\n    model=\"gpt-4.1-2025-04-14\",\n    tools=[get_policy_doc, get_user_acct],\n    input=\"How much will it cost for international service? I'm traveling to France.\",\n    # input=\"Why was my last bill so high?\"\n)\n\nresponse.to_dict()[\"output\"]\n```\n```\n[{'id': 'msg_67fe92d431548191b7ca6cd604b4784b06efc5beb16b3c5e',\n  'content': [{'annotations': [],\n    'text': \"Hi, you've reached NewTelco, how can I help you? 🌍✈️\\n\\nYou'd like to know the cost of international service while traveling to France. 🇫🇷 Let me check the latest details for you—one moment, please. 🕑\",\n    'type': 'output_text'}],\n  'role': 'assistant',\n  'status': 'completed',\n  'type': 'message'},\n {'arguments': '{\"topic\":\"international service cost France\"}',\n  'call_id': 'call_cF63DLeyhNhwfdyME3ZHd0yo',\n  'name': 'lookup_policy_document',\n  'type': 'function_call',\n  'id': 'fc_67fe92d5d6888191b6cd7cf57f707e4606efc5beb16b3c5e',\n  'status': 'completed'}]\n```\n# [5. General Advice](#5-general-advice)\n\n## [Prompt Structure](#prompt-structure)\n\nFor reference, here is a good starting point for structuring your prompts.\n\n```\n# Role and Objective\n\n# Instructions\n\n## Sub-categories for more detailed instructions\n\n# Reasoning Steps\n\n# Output Format\n\n# Examples\n## Example 1\n\n# Context\n\n# Final instructions and prompt to think step by step\n```\n\nAdd or remove sections to suit your needs, and experiment to determine what’s optimal for your usage.\n\n## [Delimiters](#delimiters)\n\nHere are some general guidelines for selecting the best delimiters for your prompt. Please refer to the Long Context section for special considerations for that context type.\n\n1. Markdown: We recommend starting here, and using markdown titles for major sections and subsections (including deeper hierarchy, to H4+). Use inline backticks or backtick blocks to precisely wrap code, and standard numbered or bulleted lists as needed.\n2. XML: These also perform well, and we have improved adherence to information in XML with this model. XML is convenient to precisely wrap a section including start and end, add metadata to the tags for additional context, and enable nesting. Here is an example of using XML tags to nest examples in an example section, with inputs and outputs for each:\n\n```\n<examples>\n<example1 type=\"Abbreviate\">\n<input>San Francisco</input>\n<output>- SF</output>\n</example1>\n</examples>\n```\n\n3. JSON is highly structured and well understood by the model particularly in coding contexts. However it can be more verbose, and require character escaping that can add overhead.\n\nGuidance specifically for adding a large number of documents or files to input context:\n\n* XML performed well in our long context testing.\n  + Example: `<doc id=1 title=”The Fox”>The quick brown fox jumps over the lazy dog</doc>`\n* This format, proposed by Lee et al. ([ref](https://arxiv.org/pdf/2406.13121)), also performed well in our long context testing.\n  + Example: `ID: 1 | TITLE: The Fox | CONTENT: The quick brown fox jumps over the lazy dog`\n* JSON performed particularly poorly.\n  + Example: `[{“id”: 1, “title”: “The Fox”, “content”: “The quick brown fox jumped over the lazy dog”}]`\n\nThe model is trained to robustly understand structure in a variety of formats. Generally, use your judgement and think about what will provide clear information and “stand out” to the model. For example, if you’re retrieving documents that contain lots of XML, an XML-based delimiter will likely be less effective.\n\n## [Caveats](#caveats)\n\n* In some isolated cases we have observed the model being resistant to producing very long, repetitive outputs, for example, analyzing hundreds of items one by one. If this is necessary for your use case, instruct the model strongly to output this information in full, and consider breaking down the problem or using a more concise approach.\n* We have seen some rare instances of parallel tool calls being incorrect. We advise testing this, and considering setting the [parallel\\_tool\\_calls](https://platform.openai.com/docs/api-reference/responses/create#responses-create-parallel_tool_calls) param to false if you’re seeing issues.\n# [Appendix: Generating and Applying File Diffs](#appendix-generating-and-applying-file-diffs)\n\nDevelopers have provided us feedback that accurate and well-formed diff generation is a critical capability to power coding-related tasks. To this end, the GPT-4.1 family features substantially improved diff capabilities relative to previous GPT models. Moreover, while GPT-4.1 has strong performance generating diffs of any format given clear instructions and examples, we open-source here one recommended diff format, on which the model has been extensively trained. We hope that in particular for developers just starting out, that this will take much of the guesswork out of creating diffs yourself.\n\n## [Apply Patch](#apply-patch)\n\nSee the example below for a prompt that applies our recommended tool call correctly.\n\n```\nAPPLY_PATCH_TOOL_DESC = \"\"\"This is a custom utility that makes it more convenient to add, remove, move, or edit code files. `apply_patch` effectively allows you to execute a diff/patch against a file, but the format of the diff specification is unique to this task, so pay careful attention to these instructions. To use the `apply_patch` command, you should pass a message of the following structure as \"input\":\n\n%%bash\napply_patch <<\"EOF\"\n*** Begin Patch\n[YOUR_PATCH]\n*** End Patch\nEOF\n\nWhere [YOUR_PATCH] is the actual content of your patch, specified in the following V4A diff format.\n\n*** [ACTION] File: [path/to/file] -> ACTION can be one of Add, Update, or Delete.\nFor each snippet of code that needs to be changed, repeat the following:\n[context_before] -> See below for further instructions on context.\n- [old_code] -> Precede the old code with a minus sign.\n+ [new_code] -> Precede the new, replacement code with a plus sign.\n[context_after] -> See below for further instructions on context.\n\nFor instructions on [context_before] and [context_after]:\n- By default, show 3 lines of code immediately above and 3 lines immediately below each change. If a change is within 3 lines of a previous change, do NOT duplicate the first change’s [context_after] lines in the second change’s [context_before] lines.\n- If 3 lines of context is insufficient to uniquely identify the snippet of code within the file, use the @@ operator to indicate the class or function to which the snippet belongs. For instance, we might have:\n@@ class BaseClass\n[3 lines of pre-context]\n- [old_code]\n+ [new_code]\n[3 lines of post-context]\n\n- If a code block is repeated so many times in a class or function such that even a single @@ statement and 3 lines of context cannot uniquely identify the snippet of code, you can use multiple `@@` statements to jump to the right context. For instance:\n\n@@ class BaseClass\n@@ \tdef method():\n[3 lines of pre-context]\n- [old_code]\n+ [new_code]\n[3 lines of post-context]\n\nNote, then, that we do not use line numbers in this diff format, as the context is enough to uniquely identify code. An example of a message that you might pass as \"input\" to this function, in order to apply a patch, is shown below.\n\n%%bash\napply_patch <<\"EOF\"\n*** Begin Patch\n*** Update File: pygorithm/searching/binary_search.py\n@@ class BaseClass\n@@     def search():\n-          pass\n+          raise NotImplementedError()\n\n@@ class Subclass\n@@     def search():\n-          pass\n+          raise NotImplementedError()\n\n*** End Patch\nEOF\n\"\"\"\n\nAPPLY_PATCH_TOOL = {\n    \"name\": \"apply_patch\",\n    \"description\": APPLY_PATCH_TOOL_DESC,\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"input\": {\n                \"type\": \"string\",\n                \"description\": \" The apply_patch command that you wish to execute.\",\n            }\n        },\n        \"required\": [\"input\"],\n    },\n}\n```\n## [Reference Implementation: apply\\_patch.py](#reference-implementation-apply_patchpy)\n\nHere’s a reference implementation of the apply\\_patch tool that we used as part of model training. You’ll need to make this an executable and available as `apply\\_patch` from the shell where the model will execute commands:\n\n```\n#!/usr/bin/env python3\n\n\"\"\"\nA self-contained **pure-Python 3.9+** utility for applying human-readable\n“pseudo-diff” patch files to a collection of text files.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pathlib\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import (\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Tuple,\n    Union,\n)\n\n# --------------------------------------------------------------------------- #\n#  Domain objects\n# --------------------------------------------------------------------------- #\nclass ActionType(str, Enum):\n    ADD = \"add\"\n    DELETE = \"delete\"\n    UPDATE = \"update\"\n\n@dataclass\nclass FileChange:\n    type: ActionType\n    old_content: Optional[str] = None\n    new_content: Optional[str] = None\n    move_path: Optional[str] = None\n\n@dataclass\nclass Commit:\n    changes: Dict[str, FileChange] = field(default_factory=dict)\n\n# --------------------------------------------------------------------------- #\n#  Exceptions\n# --------------------------------------------------------------------------- #\nclass DiffError(ValueError):\n    \"\"\"Any problem detected while parsing or applying a patch.\"\"\"\n\n# --------------------------------------------------------------------------- #\n#  Helper dataclasses used while parsing patches\n# --------------------------------------------------------------------------- #\n@dataclass\nclass Chunk:\n    orig_index: int = -1\n    del_lines: List[str] = field(default_factory=list)\n    ins_lines: List[str] = field(default_factory=list)\n\n@dataclass\nclass PatchAction:\n    type: ActionType\n    new_file: Optional[str] = None\n    chunks: List[Chunk] = field(default_factory=list)\n    move_path: Optional[str] = None\n\n@dataclass\nclass Patch:\n    actions: Dict[str, PatchAction] = field(default_factory=dict)\n\n# --------------------------------------------------------------------------- #\n#  Patch text parser\n# --------------------------------------------------------------------------- #\n@dataclass\nclass Parser:\n    current_files: Dict[str, str]\n    lines: List[str]\n    index: int = 0\n    patch: Patch = field(default_factory=Patch)\n    fuzz: int = 0\n\n    # ------------- low-level helpers -------------------------------------- #\n    def _cur_line(self) -> str:\n        if self.index >= len(self.lines):\n            raise DiffError(\"Unexpected end of input while parsing patch\")\n        return self.lines[self.index]\n\n    @staticmethod\n    def _norm(line: str) -> str:\n        \"\"\"Strip CR so comparisons work for both LF and CRLF input.\"\"\"\n        return line.rstrip(\"\\r\")\n\n    # ------------- scanning convenience ----------------------------------- #\n    def is_done(self, prefixes: Optional[Tuple[str, ...]] = None) -> bool:\n        if self.index >= len(self.lines):\n            return True\n        if (\n            prefixes\n            and len(prefixes) > 0\n            and self._norm(self._cur_line()).startswith(prefixes)\n        ):\n            return True\n        return False\n\n    def startswith(self, prefix: Union[str, Tuple[str, ...]]) -> bool:\n        return self._norm(self._cur_line()).startswith(prefix)\n\n    def read_str(self, prefix: str) -> str:\n        \"\"\"\n        Consume the current line if it starts with *prefix* and return the text\n        **after** the prefix.  Raises if prefix is empty.\n        \"\"\"\n        if prefix == \"\":\n            raise ValueError(\"read_str() requires a non-empty prefix\")\n        if self._norm(self._cur_line()).startswith(prefix):\n            text = self._cur_line()[len(prefix) :]\n            self.index += 1\n            return text\n        return \"\"\n\n    def read_line(self) -> str:\n        \"\"\"Return the current raw line and advance.\"\"\"\n        line = self._cur_line()\n        self.index += 1\n        return line\n\n    # ------------- public entry point -------------------------------------- #\n    def parse(self) -> None:\n        while not self.is_done((\"*** End Patch\",)):\n            # ---------- UPDATE ---------- #\n            path = self.read_str(\"*** Update File: \")\n            if path:\n                if path in self.patch.actions:\n                    raise DiffError(f\"Duplicate update for file: {path}\")\n                move_to = self.read_str(\"*** Move to: \")\n                if path not in self.current_files:\n                    raise DiffError(f\"Update File Error - missing file: {path}\")\n                text = self.current_files[path]\n                action = self._parse_update_file(text)\n                action.move_path = move_to or None\n                self.patch.actions[path] = action\n                continue\n\n            # ---------- DELETE ---------- #\n            path = self.read_str(\"*** Delete File: \")\n            if path:\n                if path in self.patch.actions:\n                    raise DiffError(f\"Duplicate delete for file: {path}\")\n                if path not in self.current_files:\n                    raise DiffError(f\"Delete File Error - missing file: {path}\")\n                self.patch.actions[path] = PatchAction(type=ActionType.DELETE)\n                continue\n\n            # ---------- ADD ---------- #\n            path = self.read_str(\"*** Add File: \")\n            if path:\n                if path in self.patch.actions:\n                    raise DiffError(f\"Duplicate add for file: {path}\")\n                if path in self.current_files:\n                    raise DiffError(f\"Add File Error - file already exists: {path}\")\n                self.patch.actions[path] = self._parse_add_file()\n                continue\n\n            raise DiffError(f\"Unknown line while parsing: {self._cur_line()}\")\n\n        if not self.startswith(\"*** End Patch\"):\n            raise DiffError(\"Missing *** End Patch sentinel\")\n        self.index += 1  # consume sentinel\n\n    # ------------- section parsers ---------------------------------------- #\n    def _parse_update_file(self, text: str) -> PatchAction:\n        action = PatchAction(type=ActionType.UPDATE)\n        lines = text.split(\"\\n\")\n        index = 0\n        while not self.is_done(\n            (\n                \"*** End Patch\",\n                \"*** Update File:\",\n                \"*** Delete File:\",\n                \"*** Add File:\",\n                \"*** End of File\",\n            )\n        ):\n            def_str = self.read_str(\"@@ \")\n            section_str = \"\"\n            if not def_str and self._norm(self._cur_line()) == \"@@\":\n                section_str = self.read_line()\n\n            if not (def_str or section_str or index == 0):\n                raise DiffError(f\"Invalid line in update section:\\n{self._cur_line()}\")\n\n            if def_str.strip():\n                found = False\n                if def_str not in lines[:index]:\n                    for i, s in enumerate(lines[index:], index):\n                        if s == def_str:\n                            index = i + 1\n                            found = True\n                            break\n                if not found and def_str.strip() not in [\n                    s.strip() for s in lines[:index]\n                ]:\n                    for i, s in enumerate(lines[index:], index):\n                        if s.strip() == def_str.strip():\n                            index = i + 1\n                            self.fuzz += 1\n                            found = True\n                            break\n\n            next_ctx, chunks, end_idx, eof = peek_next_section(self.lines, self.index)\n            new_index, fuzz = find_context(lines, next_ctx, index, eof)\n            if new_index == -1:\n                ctx_txt = \"\\n\".join(next_ctx)\n                raise DiffError(\n                    f\"Invalid {'EOF ' if eof else ''}context at {index}:\\n{ctx_txt}\"\n                )\n            self.fuzz += fuzz\n            for ch in chunks:\n                ch.orig_index += new_index\n                action.chunks.append(ch)\n            index = new_index + len(next_ctx)\n            self.index = end_idx\n        return action\n\n    def _parse_add_file(self) -> PatchAction:\n        lines: List[str] = []\n        while not self.is_done(\n            (\"*** End Patch\", \"*** Update File:\", \"*** Delete File:\", \"*** Add File:\")\n        ):\n            s = self.read_line()\n            if not s.startswith(\"+\"):\n                raise DiffError(f\"Invalid Add File line (missing '+'): {s}\")\n            lines.append(s[1:])  # strip leading '+'\n        return PatchAction(type=ActionType.ADD, new_file=\"\\n\".join(lines))\n\n# --------------------------------------------------------------------------- #\n#  Helper functions\n# --------------------------------------------------------------------------- #\ndef find_context_core(\n    lines: List[str], context: List[str], start: int\n) -> Tuple[int, int]:\n    if not context:\n        return start, 0\n\n    for i in range(start, len(lines)):\n        if lines[i : i + len(context)] == context:\n            return i, 0\n    for i in range(start, len(lines)):\n        if [s.rstrip() for s in lines[i : i + len(context)]] == [\n            s.rstrip() for s in context\n        ]:\n            return i, 1\n    for i in range(start, len(lines)):\n        if [s.strip() for s in lines[i : i + len(context)]] == [\n            s.strip() for s in context\n        ]:\n            return i, 100\n    return -1, 0\n\ndef find_context(\n    lines: List[str], context: List[str], start: int, eof: bool\n) -> Tuple[int, int]:\n    if eof:\n        new_index, fuzz = find_context_core(lines, context, len(lines) - len(context))\n        if new_index != -1:\n            return new_index, fuzz\n        new_index, fuzz = find_context_core(lines, context, start)\n        return new_index, fuzz + 10_000\n    return find_context_core(lines, context, start)\n\ndef peek_next_section(\n    lines: List[str], index: int\n) -> Tuple[List[str], List[Chunk], int, bool]:\n    old: List[str] = []\n    del_lines: List[str] = []\n    ins_lines: List[str] = []\n    chunks: List[Chunk] = []\n    mode = \"keep\"\n    orig_index = index\n\n    while index < len(lines):\n        s = lines[index]\n        if s.startswith(\n            (\n                \"@@\",\n                \"*** End Patch\",\n                \"*** Update File:\",\n                \"*** Delete File:\",\n                \"*** Add File:\",\n                \"*** End of File\",\n            )\n        ):\n            break\n        if s == \"***\":\n            break\n        if s.startswith(\"***\"):\n            raise DiffError(f\"Invalid Line: {s}\")\n        index += 1\n\n        last_mode = mode\n        if s == \"\":\n            s = \" \"\n        if s[0] == \"+\":\n            mode = \"add\"\n        elif s[0] == \"-\":\n            mode = \"delete\"\n        elif s[0] == \" \":\n            mode = \"keep\"\n        else:\n            raise DiffError(f\"Invalid Line: {s}\")\n        s = s[1:]\n\n        if mode == \"keep\" and last_mode != mode:\n            if ins_lines or del_lines:\n                chunks.append(\n                    Chunk(\n                        orig_index=len(old) - len(del_lines),\n                        del_lines=del_lines,\n                        ins_lines=ins_lines,\n                    )\n                )\n            del_lines, ins_lines = [], []\n\n        if mode == \"delete\":\n            del_lines.append(s)\n            old.append(s)\n        elif mode == \"add\":\n            ins_lines.append(s)\n        elif mode == \"keep\":\n            old.append(s)\n\n    if ins_lines or del_lines:\n        chunks.append(\n            Chunk(\n                orig_index=len(old) - len(del_lines),\n                del_lines=del_lines,\n                ins_lines=ins_lines,\n            )\n        )\n\n    if index < len(lines) and lines[index] == \"*** End of File\":\n        index += 1\n        return old, chunks, index, True\n\n    if index == orig_index:\n        raise DiffError(\"Nothing in this section\")\n    return old, chunks, index, False\n\n# --------------------------------------------------------------------------- #\n#  Patch → Commit and Commit application\n# --------------------------------------------------------------------------- #\ndef _get_updated_file(text: str, action: PatchAction, path: str) -> str:\n    if action.type is not ActionType.UPDATE:\n        raise DiffError(\"_get_updated_file called with non-update action\")\n    orig_lines = text.split(\"\\n\")\n    dest_lines: List[str] = []\n    orig_index = 0\n\n    for chunk in action.chunks:\n        if chunk.orig_index > len(orig_lines):\n            raise DiffError(\n                f\"{path}: chunk.orig_index {chunk.orig_index} exceeds file length\"\n            )\n        if orig_index > chunk.orig_index:\n            raise DiffError(\n                f\"{path}: overlapping chunks at {orig_index} > {chunk.orig_index}\"\n            )\n\n        dest_lines.extend(orig_lines[orig_index : chunk.orig_index])\n        orig_index = chunk.orig_index\n\n        dest_lines.extend(chunk.ins_lines)\n        orig_index += len(chunk.del_lines)\n\n    dest_lines.extend(orig_lines[orig_index:])\n    return \"\\n\".join(dest_lines)\n\ndef patch_to_commit(patch: Patch, orig: Dict[str, str]) -> Commit:\n    commit = Commit()\n    for path, action in patch.actions.items():\n        if action.type is ActionType.DELETE:\n            commit.changes[path] = FileChange(\n                type=ActionType.DELETE, old_content=orig[path]\n            )\n        elif action.type is ActionType.ADD:\n            if action.new_file is None:\n                raise DiffError(\"ADD action without file content\")\n            commit.changes[path] = FileChange(\n                type=ActionType.ADD, new_content=action.new_file\n            )\n        elif action.type is ActionType.UPDATE:\n            new_content = _get_updated_file(orig[path], action, path)\n            commit.changes[path] = FileChange(\n                type=ActionType.UPDATE,\n                old_content=orig[path],\n                new_content=new_content,\n                move_path=action.move_path,\n            )\n    return commit\n\n# --------------------------------------------------------------------------- #\n#  User-facing helpers\n# --------------------------------------------------------------------------- #\ndef text_to_patch(text: str, orig: Dict[str, str]) -> Tuple[Patch, int]:\n    lines = text.splitlines()  # preserves blank lines, no strip()\n    if (\n        len(lines) < 2\n        or not Parser._norm(lines[0]).startswith(\"*** Begin Patch\")\n        or Parser._norm(lines[-1]) != \"*** End Patch\"\n    ):\n        raise DiffError(\"Invalid patch text - missing sentinels\")\n\n    parser = Parser(current_files=orig, lines=lines, index=1)\n    parser.parse()\n    return parser.patch, parser.fuzz\n\ndef identify_files_needed(text: str) -> List[str]:\n    lines = text.splitlines()\n    return [\n        line[len(\"*** Update File: \") :]\n        for line in lines\n        if line.startswith(\"*** Update File: \")\n    ] + [\n        line[len(\"*** Delete File: \") :]\n        for line in lines\n        if line.startswith(\"*** Delete File: \")\n    ]\n\ndef identify_files_added(text: str) -> List[str]:\n    lines = text.splitlines()\n    return [\n        line[len(\"*** Add File: \") :]\n        for line in lines\n        if line.startswith(\"*** Add File: \")\n    ]\n\n# --------------------------------------------------------------------------- #\n#  File-system helpers\n# --------------------------------------------------------------------------- #\ndef load_files(paths: List[str], open_fn: Callable[[str], str]) -> Dict[str, str]:\n    return {path: open_fn(path) for path in paths}\n\ndef apply_commit(\n    commit: Commit,\n    write_fn: Callable[[str, str], None],\n    remove_fn: Callable[[str], None],\n) -> None:\n    for path, change in commit.changes.items():\n        if change.type is ActionType.DELETE:\n            remove_fn(path)\n        elif change.type is ActionType.ADD:\n            if change.new_content is None:\n                raise DiffError(f\"ADD change for {path} has no content\")\n            write_fn(path, change.new_content)\n        elif change.type is ActionType.UPDATE:\n            if change.new_content is None:\n                raise DiffError(f\"UPDATE change for {path} has no new content\")\n            target = change.move_path or path\n            write_fn(target, change.new_content)\n            if change.move_path:\n                remove_fn(path)\n\ndef process_patch(\n    text: str,\n    open_fn: Callable[[str], str],\n    write_fn: Callable[[str, str], None],\n    remove_fn: Callable[[str], None],\n) -> str:\n    if not text.startswith(\"*** Begin Patch\"):\n        raise DiffError(\"Patch text must start with *** Begin Patch\")\n    paths = identify_files_needed(text)\n    orig = load_files(paths, open_fn)\n    patch, _fuzz = text_to_patch(text, orig)\n    commit = patch_to_commit(patch, orig)\n    apply_commit(commit, write_fn, remove_fn)\n    return \"Done!\"\n\n# --------------------------------------------------------------------------- #\n#  Default FS helpers\n# --------------------------------------------------------------------------- #\ndef open_file(path: str) -> str:\n    with open(path, \"rt\", encoding=\"utf-8\") as fh:\n        return fh.read()\n\ndef write_file(path: str, content: str) -> None:\n    target = pathlib.Path(path)\n    target.parent.mkdir(parents=True, exist_ok=True)\n    with target.open(\"wt\", encoding=\"utf-8\") as fh:\n        fh.write(content)\n\ndef remove_file(path: str) -> None:\n    pathlib.Path(path).unlink(missing_ok=True)\n\n# --------------------------------------------------------------------------- #\n#  CLI entry-point\n# --------------------------------------------------------------------------- #\ndef main() -> None:\n    import sys\n\n    patch_text = sys.stdin.read()\n    if not patch_text:\n        print(\"Please pass patch text through stdin\", file=sys.stderr)\n        return\n    try:\n        result = process_patch(patch_text, open_file, write_file, remove_file)\n    except DiffError as exc:\n        print(exc, file=sys.stderr)\n        return\n    print(result)\n\nif __name__ == \"__main__\":\n    main()\n\n```\n## [Other Effective Diff Formats](#other-effective-diff-formats)\n\nIf you want to try using a different diff format, we found in testing that the SEARCH/REPLACE diff format used in Aider’s polyglot benchmark, as well as a pseudo-XML format with no internal escaping, both had high success rates.\n\nThese diff formats share two key aspects: (1) they do not use line numbers, and (2) they provide both the exact code to be replaced, and the exact code with which to replace it, with clear delimiters between the two.\n\n```\nSEARCH_REPLACE_DIFF_EXAMPLE = \"\"\"\npath/to/file.py\n```\n>>>>>>> SEARCH\ndef search():\n    pass\n=======\ndef search():\n   raise NotImplementedError()\n<<<<<<< REPLACE\n\"\"\"\n\nPSEUDO_XML_DIFF_EXAMPLE = \"\"\"\n<edit>\n<file>\npath/to/file.py\n</file>\n<old_code>\ndef search():\n    pass\n</old_code>\n<new_code>\ndef search():\n   raise NotImplementedError()\n</new_code>\n</edit>\n\"\"\"\n```\n\n",
    "keyword": "wow-for-prompting-with-gpt-4-1"
  },
  {
    "name": "wow-for-chat-gpt-prompt-engineering",
    "text": "# Best practices for prompt engineering with the OpenAI API\n## How to give clear and effective instructions to OpenAI models\n\n*Updated over 6 months ago*\n\n## How prompt engineering works\nDue to the way OpenAI models are trained, there are specific prompt formats that work particularly well and lead to more useful model artifacts.\n\nThe official prompt engineering guide by OpenAI is usually the best place to start for prompting tips.\n\nBelow we present a number of prompt formats we find work well, but feel free to explore different formats, which may fit your task better.\n\n## Rules of Thumb and Examples\n*Note: the \"{text input here}\" is a placeholder for actual text/context*\n\n### 1. Use the latest model\nFor best results, we generally recommend using the latest, most capable models. Newer models tend to be easier to prompt engineer.\n\n### 2. Put instructions at the beginning of the prompt and use ### or \"\"\" to separate the instruction and context\nLess effective ❌:\n```\nSummarize the text below as a bullet point list of the most important points.\n\n{text input here}\n```\n\nBetter ✅:\n```\nSummarize the text below as a bullet point list of the most important points.\n\nText: \"\"\"\n{text input here}\n\"\"\"\n```\n\n### 3. Be specific, descriptive and as detailed as possible about the desired context, outcome, length, format, style, etc\nBe specific about the context, outcome, length, format, style, etc\n\nLess effective ❌:\n```\nWrite a poem about OpenAI.\n```\n\nBetter ✅:\n```\nWrite a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a {famous poet}\n```\n\n### 4. Articulate the desired output format through examples\nLess effective ❌:\n```\nExtract the entities mentioned in the text below. Extract the following 4 entity types: company names, people names, specific topics and themes.\n\nText: {text}\n```\n\nShow, and tell - the models respond better when shown specific format requirements. This also makes it easier to programmatically parse out multiple artifacts reliably.\n\nBetter ✅:\n```\nExtract the important entities mentioned in the text below. First extract all company names, then extract all people names, then extract specific topics which fit the content and finally extract general overarching themes\n\nDesired format:\nCompany names: <comma_separated_list_of_company_names>\nPeople names: -||-\nSpecific topics: -||-\nGeneral themes: -||-\n\nText: {text}\n```\n\n### 5. Start with zero-shot, then few-shot, neither of them worked, then fine-tune\n✅ Zero-shot\n```\nExtract keywords from the below text.\n\nText: {text}\n\nKeywords:\n```\n\n✅ Few-shot - provide a couple of examples\n```\nExtract keywords from the corresponding texts below.\n\nText 1: Stripe provides APIs that web developers can use to integrate payment processing into their websites and mobile applications.\nKeywords 1: Stripe, payment processing, APIs, web developers, websites, mobile applications\n##\nText 2: OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.\nKeywords 2: OpenAI, language models, text processing, API.\n##\nText 3: {text}\nKeywords 3:\n```\n\n✅Fine-tune: see fine-tune best practices here.\n\n### 6. Reduce \"fluffy\" and imprecise descriptions\nLess effective ❌:\n```\nThe description for this product should be fairly short, a few sentences only, and not too much more.\n```\n\nBetter ✅:\n```\nUse a 3 to 5 sentence paragraph to describe this product.\n```\n\n### 7. Instead of just saying what not to do, say what to do instead\nLess effective ❌:\n```\nThe following is a conversation between an Agent and a Customer. DO NOT ASK USERNAME OR PASSWORD. DO NOT REPEAT.\n\nCustomer: I can't log in to my account.\nAgent:\n```\n\nBetter ✅:\n```\nThe following is a conversation between an Agent and a Customer. The agent will attempt to diagnose the problem and suggest a solution, whilst refraining from asking any questions related to PII. Instead of asking for PII, such as username or password, refer the user to the help article www.samplewebsite.com/help/faq\n\nCustomer: I can't log in to my account.\nAgent:\n```\n\n### 8. Code Generation Specific - Use \"leading words\" to nudge the model toward a particular pattern\nLess effective ❌:\n```\n# Write a simple python function that\n# 1. Ask me for a number in mile\n# 2. It converts miles to kilometers\n```\n\nIn this code example below, adding \"import\" hints to the model that it should start writing in Python. (Similarly \"SELECT\" is a good hint for the start of a SQL statement.)\n\nBetter ✅:\n```\n# Write a simple python function that\n# 1. Ask me for a number in mile\n# 2. It converts miles to kilometers\n\nimport\n```\n\n### 9. Use the Generate Anything feature\nDevelopers can use the 'Generate Anything' feature to describe a task or expected natural language output and receive a tailored prompt.\n\nLearn more about using the 'Generate Anything' feature.\n\n## Parameters\nGenerally, we find that model and temperature are the most commonly used parameters to alter the model output.\n\n- **model** - Higher performance models are generally more expensive and may have higher latency.\n\n- **temperature** - A measure of how often the model outputs a less likely token. The higher the temperature, the more random (and usually creative) the artifact. This, however, is not the same as \"truthfulness\". For most factual use cases such as data extraction, and truthful Q&A, the temperature of 0 is best.\n\n- **max_tokens** (maximum length) - Does not control the length of the output, but a hard cutoff limit for token generation. Ideally you won't hit this limit often, as your model will stop either when it thinks it's finished, or when it hits a stop sequence you defined.\n\n- **stop** (stop sequences) - A set of characters (tokens) that, when generated, will cause the text generation to stop.\n\nFor other parameter descriptions see the API reference. \n",
    "keyword": "wow-for-chat-gpt-prompt-engineering"
  },
  {
    "name": "wow-for-creating-cursor-rules",
    "text": "# 🤖 How to Force your Cursor AI Agent to 🧑‍🎓 Always follow your Rules using 💡 Auto-Rule Generation Techniques\n\nThis tutorial guides you through creating structured Cursor rule files (`.mdc`) based on documented best practices. We will use a dedicated AI agent within Cursor, configured specifically for formatting these rules correctly. Following these steps ensures your rules are consistent and effectively guide the AI's behavior in your project.\n\n## ✅ Prerequisites\n\n*   Cursor AI code editor installed.\n*   Basic understanding of Cursor's features (like chat and repo prompts).\n*   A set of best practices you want to enforce, documented preferably in a Markdown file within your project.\n\n## 1️⃣ Step 1: Obtain the Rule Generation Rule\n\n![](https://www.ultrawideturbodevs.com/content/images/size/w2400/2025/04/CleanShot-2025-04-16-at-15.55.58@2x.png)\n\nThe core of this process relies on a [specific Cursor rule](https://github.com/bmadcode/cursor-custom-agents-rules-generator/blob/main/.cursor/rules/core-rules/rule-generating-agent.mdc) designed to guide the AI in creating *other* rules correctly. Think of it as a meta-rule: a rule about how to make rules.\n\nThis isn't a separate \"agent\" in the typical sense, but rather a standard Cursor rule file (`.mdc`) containing detailed instructions (its system prompt) on how to format and structure new rule files based on user input or best practices documents.\n\n**Recommendation:** Adding this rule allows Cursor to efficiently create and update *other* rules for your project whenever you ask it to, ensuring consistency by always referencing these formatting guidelines. To do so follow the following steps:\n\n![](https://www.ultrawideturbodevs.com/content/images/size/w2400/2025/04/CleanShot-2025-04-16-at-15.58.23@2x.png)\n\n1.  Create the directory `.cursor/rules/core-rules/` if it doesn't exist.\n2.  Save the content below into a file named `.cursor/rules/core-rules/rule-generating-agent.mdc`.\n3. (Optional) Cursor has a built in view for showing cursor rules, unfortunately this view may cause updates, by agents, to be lost. To prevent this add the following to your cursor settings, so the files get opened like regular files:\n```\n\"workbench.editorAssociations\": {\n  \"*.mdc\": \"default\"\n}\n```\n\nFor this tutorial we will reference the content of this rule file as instructions loaded directly into the chat.\n\n**Credits:** The original rule definition used here comes from the [`cursor-custom-agents-rules-generator`](https://github.com/bmadcode/cursor-custom-agents-rules-generator/tree/main) project. The project gets updated frequently and the rule you see here might be outdated by the time your read this. You can find the [latest version here](https://github.com/bmadcode/cursor-custom-agents-rules-generator/blob/main/.cursor/rules/core-rules/rule-generating-agent.mdc). Many thanks to [BMad](https://github.com/bmadcode) for sharing this useful resource рџ™Џ.\n\n**Content for `rule-generating-agent.mdc`:**\n\n```\n---\ndescription: This rule is essential for maintaining consistency and quality in rule creation across the codebase. It must be followed whenever: (1) A user requests a new rule to be created, (2) An existing rule needs modification, (3) The user asks to remember certain behaviors or patterns, or (4) Future behavior changes are requested. This rule ensures proper organization, clear documentation, and effective rule application by defining standard formats, naming conventions, and content requirements. It's particularly crucial for maintaining the rule hierarchy, ensuring rules are discoverable by the AI, and preserving the effectiveness of the rule-based system. The rule system is fundamental to project consistency, code quality, and automated assistance effectiveness.\nglobs: \nalwaysApply: true\n---\n# Cursor Rules Format\n\n## Template Structure for Rules Files\n\n---\ndescription: `Comprehensive description that provides full context and clearly indicates when this rule should be applied. Include key scenarios, impacted areas, and why following this rule is important. While being thorough, remain focused and relevant. The description should be detailed enough that the agent can confidently determine whether to apply the rule in any given situation.`\nglobs: .cursor/rules/**/*.mdc OR blank\nalwaysApply: {true or false}\n---\n\n# Rule Title\n\n## Critical Rules\n\n- Concise, bulleted list of actionable rules the agent MUST follow\n\n## Examples\n\n&lt;example&gt;\n{valid rule application}\n&lt;/example;&gt;\n\n&lt;example type=\"invalid\";&gt;\n{invalid rule application}\n&lt;/example;&gt;\n\n### Organizational Folders (Create if non existent)\nAll rules files will be under an organizational folder:\n- .cursor/rules/core-rules - rules related to cursor agent behavior or rule generation specifically\n- .cursor/rules/my-rules - gitignore in a shared repo, rules specifically for ME only\n- .cursor/rules/global-rules - these will be rules that are ALWAYS applied to every chat and cmd/ctrl-k context\n- .cursor/rules/testing-rules - rules about testing\n- .cursor/rules/tool-rules - rules specific to different tools, such as git, linux commands, direction of usage of MCP tools\n- .cursor/rules/ts-rules - typescript language specific rules\n- .cursor/rules/py-rules - python specific rules\n- .cursor/rules/ui-rules - rules about html, css, react\n* create new folders under .cursor/rules/ as needed following similar grouping conventions,\n    - for example `.cursor/rules/cs-rules` if we started using c# in a project\n\n## Glob Pattern Examples\nCommon glob patterns for different rule types:\n- Core standards: .cursor/rules/*.mdc\n- Language rules: *.cs, *.cpp\n- Testing standards: *.test.ts, *.test.js\n- React components: src/components/**/*.tsx\n- Documentation: docs/**/*.md, *.md\n- Configuration files: *.config.js\n- Build artifacts: dist/**/*\n- Multiple extensions: *.js, *.ts, *.tsx\n- Multiple patterns: dist/**/*.*, docs/**/*.md, *test*.*\n\n## Critical Rules\n- Rule files will be located and named ALWAYS as: `.cursor/rules/{organizational-folder}/rule-name-{auto|agent|manual|always}.mdc`\n- Rules will NEVER be created anywhere other than .cursor/rules/**\n- You will always check to see if there is an existing rule to update under all .cursor/rules sub-folders\n- FrontMatter Rules Types:\n    - The front matter section must always start the file and include all 3 fields, even if the field value will be blank - the types are:\n        - Manual Rule: IF a Manual rule is requested - description and globs MUST be blank and alwaysApply: false and filename ends with -manual.mdc.\n        - Auto Rule: IF a rule is requested that should apply always to certain glob patterns (example all typescript files or all markdown files) - description must be blank, and alwaysApply: false and filename ends with -auto.mdc.\n        - Always Rule: Global Rule applies to every chat and cmd/ctrl-k - description and globs blank, and alwaysApply: true  and filename ends with -always.mdc.\n        - Agent Select Rule: The rule does not need to be loaded into every chat thread, it serves a specific purpose. The description MUST provide comprehensive context about when to apply the rule, including scenarios like code changes, architecture decisions, bug fixes, or new file creation. Globs blank, and alwaysApply:false and filename ends with -agent.mdc\n- For Rule Content - focus on actionable, clear directives without unnecessary explanation\n- When a rule will only be used sometimes (alwaysApply: false) the description MUST provide enough context for the AI to confidently determine when to load and apply the rule\n- Use Concise Markdown Tailored to Agent Context Window usage\n- Always indent content within XML Example section with 2 spaces\n- Emojis and Mermaid diagrams are allowed and encouraged if it is not redundant and better explains the rule for the AI comprehension\n- While there is no strict line limit, be judicious with content length as it impacts performance. Focus on essential information that helps the agent make decisions\n- Always include a valid and invalid example\n- NEVER use quotes around glob patterns, NEVER group glob extensions with `{}`\n- If the request for a rule or a future behavior change includes context of a mistake is made, this would be great to use in the example for the rule\n- After rule is created or updated, Respond with the following:\n    - AutoRuleGen Success: path/rule-name.mdc\n    - Rule Type: {Rule Type}\n    - Rule Description: {The exact content of the description field}\n```\n\n## 📋 Step 2: Document Your Best Practices\n\nBefore creating rules, you need the content for them. Gather the specific standards or best practices you want the AI to follow for a particular domain (like TypeScript coding standards, testing procedures, or commit message formats).\n\n💡 **Tip: Use AI for Research and Generation**\n\nYou can leverage AI models with strong research capabilities (like [Perplexity](https://www.perplexity.ai/), [Claude 3 Opus](https://claude.ai/), [GPT-4](https://openai.com/gpt-4/), or [Grok](https://grok.x.ai/)) to help you generate this best practices document. Provide the AI with context about your project and ask it to research and compile relevant standards.\n\n**Example Prompt for AI:**\n\n```\nObjective: Research and compile a list of best practices for {TECHNOLOGY_OR_DOMAIN} within the context of a {PROJECT_TYPE} project.\n\nContext:\n-   Our project uses: {LIST_KEY_TECHNOLOGIES_FRAMEWORKS}\n-   Our team size is: {TEAM_SIZE}\n-   Key priorities are: {LIST_PROJECT_PRIORITIES e.g., maintainability, performance, security}\n\nInstructions:\n1.  Research established best practices for {TECHNOLOGY_OR_DOMAIN}.\n2.  Focus on practices relevant to {PROJECT_TYPE} and our priorities ({LIST_PROJECT_PRIORITIES}).\n3.  Organize the findings into clear, actionable points suitable for documentation.\n4.  Format the output as a Markdown document with appropriate headings.\n\n---\nVariable Definitions:\nTECHNOLOGY_OR_DOMAIN = \"TypeScript\" // e.g., \"Python\", \"React\", \"API Design\", \"Git Commit Messages\"\nPROJECT_TYPE = \"web application\" // e.g., \"CLI tool\", \"mobile app\", \"data science project\"\nLIST_KEY_TECHNOLOGIES_FRAMEWORKS = \"Node.js, Express, PostgreSQL\" // e.g., \"React, Next.js, Tailwind CSS\"\nTEAM_SIZE = \"small (3-5 developers)\" // e.g., \"large (>10 developers)\", \"solo developer\"\nLIST_PROJECT_PRIORITIES = \"code readability, test coverage, consistent error handling\"\n```\n\n1.  **Create a Source Document:** Compile these best practices into a clear document within your project. A Markdown file is recommended. For example, create a file named `docs/typescript-best-practices.md`.\n2.  **Write Down Practices:** List each best practice clearly.\n\n    *Example content for `docs/your-best-practices.md`:*\n    ```\n    # Project Best Practices\n\n    ## Use Consistent Naming Conventions\n    Variables, functions, and classes should follow the project's agreed-upon naming style (e.g., camelCase for variables, PascalCase for classes).\n\n    ## Add Documentation Comments\n    Public functions and complex logic blocks should have clear documentation comments explaining their purpose, parameters, and return values.\n\n    ## Handle Errors Gracefully\n    Anticipate potential errors and implement proper error handling (e.g., try-catch blocks, checking return values) instead of letting the application crash.\n    ```\n\n## ✨ Step 3: Generate Rules Using the Agent\n\nNow, you will instruct the Rule Formatting Agent (from Step 1) to create the `.mdc` rule files based on your best practices document (from Step 2).\n\n![](https://www.ultrawideturbodevs.com/content/images/2025/04/CleanShot-2025-04-16-at-16.26.28@2x.png)\n\n1.  **Open Cursor Chat/Repo Prompt:** Start a new chat and select all files.\n2.  **Reference Agent and Document:** Make sure the Rule Formatting Agent is active (e.g., by mentioning `@rule-generating-agent` if you saved its prompt as a rule). Also, provide your best practices document as context (e.g., `@docs/typescript-best-practices.md`).\n\n![](https://www.ultrawideturbodevs.com/content/images/size/w2400/2025/04/CleanShot-2025-04-16-at-16.22.02@2x.png)\n\n3.  **Instruct the Agent:** Tell the agent exactly what to do using a detailed prompt. This prompt should reference the agent, the best practices document (using a variable), and specify the desired output (rule type, directory using a variable, naming, format adherence).\n\n    **Example Prompt to Copy/Paste:**\n\n*Note: Fill in the variable definitions at the end of this prompt before sending it.*\n\n```\nObjective: Generate individual Agent Select Cursor rules based on the best practices outlined in the referenced document (`@{BEST_PRACTICES_DOC_PATH}`).\n\nInstructions:\n1.  For each distinct best practice found in the document, create a separate Agent Select rule file (`.mdc`).\n2.  Follow all formatting and content requirements defined in your system prompt (`@rule-generating-agent`) for creating Agent Select rules.\n3.  After generation, confirm success by listing the paths of the created rule files.\n\n---\nVariable Definitions:\nBEST_PRACTICES_DOC_PATH =\n\n@rule-generating-agent\n```\n\n1. **Review Agent Output:** The agent will respond with the necessary file operations (e.g., `<file path=\"...\" action=\"create\">...`) to generate the `.mdc` files. Review these operations before applying them.\n\n2. **Apply Changes:** If the output looks correct, allow Cursor to apply the changes, creating the new rule files in your specified directory.\n\n![](https://www.ultrawideturbodevs.com/content/images/size/w2400/2025/04/image.png)\n\nYou might need to restart cursor or open the files for them to be indexed and take effect. Et voila. You have successfully generated structured Cursor rules from your documented best practices. By separating the *knowledge gathering* (Step 2) from the *rule formatting* (Step 3) and using a dedicated agent for formatting, you ensure consistency and maintainability in your AI-assisted development workflow. These rules will now automatically be referenced by Cursor's AI according to your project's standards.\n",
    "keyword": "wow-for-creating-cursor-rules"
  },
  {
    "name": "bmad-architecture-template",
    "text": "# Architecture for {PRD Title}\n\nStatus: { Draft | Approved }\n\n## Technical Summary\n\n{ Short 1-2 paragraph }\n\n## Technology Table\n\nTable listing choices for languages, libraries, infra, etc...\n\n  <example>\n  | Technology | Description |\n  | ------------ | ------------------------------------------------------------- |\n  | Kubernetes | Container orchestration platform for microservices deployment |\n  | Apache Kafka | Event streaming platform for real-time data ingestion |\n  | TimescaleDB | Time-series database for sensor data storage |\n  | Go | Primary language for data processing services |\n  | GoRilla Mux | REST API Framework |\n  | Python | Used for data analysis and ML services |\n  </example>\n\n## Architectural Diagrams\n\n{ Mermaid Diagrams to describe key flows interactions or architecture to be followed during implementation, infra provisioning, and deployments }\n\n## Data Models, API Specs, Schemas, etc...\n\n{ As needed - may not be exhaustive - but key ideas that need to be retained and followed into the architecture and stories }\n\n<example>\n### Sensor Reading Schema\n\n```json\n{\n  \"sensor_id\": \"string\",\n  \"timestamp\": \"datetime\",\n  \"readings\": {\n    \"temperature\": \"float\",\n    \"pressure\": \"float\",\n    \"humidity\": \"float\"\n  },\n  \"metadata\": {\n    \"location\": \"string\",\n    \"calibration_date\": \"datetime\"\n  }\n}\n```\n\n</example>\n\n## Project Structure\n\n{ Diagram the folder and file organization structure along with descriptions }\n\n```\n├ /src\n├── /services\n│   ├── /gateway        # Sensor data ingestion\n│   ├── /processor      # Data processing and validation\n│   ├── /analytics      # Data analysis and ML\n│   └── /notifier       # Alert and notification system\n├── /deploy\n│   ├── /kubernetes     # K8s manifests\n│   └── /terraform      # Infrastructure as Code\n└── /docs\n    ├── /api           # API documentation\n    └── /schemas       # Data schemas\n```\n\n## Infrastructure\n\n## Deployment Plan\n\n## Change Log\n",
    "keyword": "bmad-architecture-template"
  },
  {
    "name": "systems-template",
    "text": "# 🔖 Description\n> 💡 *A brief description and (optional) instructions on how to get started.*\n\n# 🎯 Goals\n> 💡 *Clear definition(s) what this system aims to accomplish.*\n\n# 📏 Rules\n> 💡 *Rules and limits to adhere to when creating and using this system.*\n\n# 🔭 Scope\n> 💡 *Boundaries of steps and goals when creating and using this system.*\n\n# 🧪 Example\n> 💡 *Example implementation of this system.*\n\n# 🪜 Steps\n> 💡 *The heart of every system. All activities and results in chronological order.*\n\n## 📋 Pre-requisites\n\n- [ ]\n\n## 👣 Activity Flow\n\n- [ ]\n",
    "keyword": "systems-template"
  },
  {
    "name": "bmad-prd-template",
    "text": "# 1. Title: {PRD for {project}}\n\n<version>1.0.0</version>\n\n## Status: { Draft | Approved }\n\n## Intro\n\n{ Short 1-2 paragraph describing the what and why of what the prd will achieve}\n\n## Goals\n\n{\n\n- Clear project objectives\n- Measurable outcomes\n- Success criteria\n- Key performance indicators (KPIs)\n  }\n\n## Features and Requirements\n\n{\n\n- Functional requirements\n- Non-functional requirements\n- User experience requirements\n- Integration requirements\n- Compliance requirements\n  }\n\n## Epic List\n\n### Epic-1: Current PRD Epic (for example backend epic)\n\n### Epic-2: Second Current PRD Epic (for example front end epic)\n\n### Epic-N: Future Epic Enhancements (Beyond Scope of current PRD)\n\n## Epic 1: Story List\n\n<example>\n- Story 1: NestJS Configuration\n  Status: {''|'InProgress'|'Complete'}\n  Requirements:\n  - Install NestJS CLI Globally\n  - Create a new NestJS project with the nestJS cli generator\n\n- Story 2: Hacker News Retrieval API Route\n  Status: {''|'InProgress'|'Complete'}\n  Requirements:\n  - Create API Route that returns a list of Hacker News TopPosts, Scrapped Article from the top posts, and a list of comments from the top posts\n  - Route post body specifies the number of posts, articles, and comments to return\n  - Create a command in package.json that I can use to call the API Route (route configured in env.local)\n    </example>\n\n## Technology Stack\n\n{ Table listing choices for languages, libraries, infra, etc...}\n\n  <example>\n  | Technology | Description |\n  | ------------ | ------------------------------------------------------------- |\n  | Kubernetes | Container orchestration platform for microservices deployment |\n  | Apache Kafka | Event streaming platform for real-time data ingestion |\n  | TimescaleDB | Time-series database for sensor data storage |\n  | Go | Primary language for data processing services |\n  | GoRilla Mux | REST API Framework |\n  | Python | Used for data analysis and ML services |\n  </example>\n\n## Reference\n\n{ Mermaid Diagrams for models tables, visual aids as needed, citations and external urls }\n\n## Data Models, API Specs, Schemas, etc...\n\n{ As needed - may not be exhaustive - but key ideas that need to be retained and followed into the architecture and stories }\n\n<example>\n### Sensor Reading Schema\n\n```json\n{\n  \"sensor_id\": \"string\",\n  \"timestamp\": \"datetime\",\n  \"readings\": {\n    \"temperature\": \"float\",\n    \"pressure\": \"float\",\n    \"humidity\": \"float\"\n  },\n  \"metadata\": {\n    \"location\": \"string\",\n    \"calibration_date\": \"datetime\"\n  }\n}\n```\n\n</example>\n\n## Project Structure\n\n{ Diagram the folder and file organization structure along with descriptions }\n\n<example>\n\n````\n// Start of Selection\n```text\nsrc/\n├── services/\n│   ├── gateway/        # Sensor data ingestion\n│   ├── processor/      # Data processing and validation\n│   ├── analytics/      # Data analysis and ML\n│   └── notifier/       # Alert and notification system\n├── deploy/\n│   ├── kubernetes/     # K8s manifests\n│   └── terraform/      # Infrastructure as Code\n└── docs/\n    ├── api/           # API documentation\n    └── schemas/       # Data schemas\n````\n\n</example>\n\n## Change Log\n\n{ Markdown table of key changes after document is no longer in draft and is updated, table includes the change title, the story id that the change happened during, and a description if the title is not clear enough }\n\n<example>\n| Change               | Story ID | Description                                                   |\n| -------------------- | -------- | ------------------------------------------------------------- |\n| Initial draft        | N/A      | Initial draft prd                                             |\n| Add ML Pipeline      | story-4  | Integration of machine learning prediction service story      |\n| Kafka Upgrade        | story-6  | Upgraded from Kafka 2.0 to Kafka 3.0 for improved performance |\n</example>\n",
    "keyword": "bmad-prd-template"
  },
  {
    "name": "test-results-template",
    "text": "# 📝 Activity: ACTOR_VERB\n💎 Expected: EXPECTED\n🧱 Actual: ACTUAL\n💭 Reason: WHY_IT_FAILED\n🔧 Proposed Fix: CODE_SNIPPET",
    "keyword": "test-results-template"
  }
]